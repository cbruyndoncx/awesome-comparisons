# DeepSeek V3 - https://github.com/deepseek-ai/DeepSeek-V3

Advanced 671B parameter Mixture-of-Experts model excelling in mathematics and coding. V3.1 and V3.2 updates in 2025 added enhanced reasoning and sparse attention for long contexts.

## General Info

### Classification
- AIE/Model

### Version
- V3.2-Exp (2025-09)

### Repo
- https://github.com/deepseek-ai/DeepSeek-V3

### Rating
- -

### Short Description
- 671B parameter MoE model (37B activated) with exceptional math and coding performance, featuring V3.1 hybrid reasoning and V3.2 sparse attention for 128K contexts.

### Description
DeepSeek-V3 is a 671-billion parameter Mixture-of-Experts model that achieves state-of-the-art performance in mathematics and coding tasks. The model family includes specialized variants: V3 (December 2024 baseline), V3.1 (August 2025) which combines the strengths of V3 and R1 into a hybrid model with enhanced reasoning, and V3.2-Exp (September 2025) featuring DeepSeek Sparse Attention (DSA) for improved long-context handling up to 128K tokens. Available through GitHub Models and various deployment platforms, DeepSeek V3 is particularly suited for solving advanced math problems and generating complex code, making it a top choice for technical and scientific applications.

### Languages
- Any

### Notes
- 671B total parameters: Massive MoE architecture with 37B activated parameters.
- Exceptional math and coding: Best performance on most technical benchmarks.
- V3.1 hybrid model (August 2025): Combines V3 and R1 strengths with enhanced thinking mode for coding and math.
- V3.2-Exp (September 2025): Experimental version with DeepSeek Sparse Attention for long contexts.
- 128K context length: Supports extended context windows for complex tasks.
- GitHub Models integration: Now generally available in GitHub Models platform.
- DeepSeek-Coder series: Separate specialized line (1B to 33B) trained on 87% code, 13% natural language.
- 2T token training: DeepSeek-Coder models pre-trained on 2 trillion tokens.
- Active development: Continuous improvements with V3.1 and V3.2 releases in 2025.
- Open source: Available on GitHub with full model weights and documentation.

### Last Update
- 2025-09

## Licensing

### Opensource
- Yes

### License
- MIT

### Free Trial
- Yes
