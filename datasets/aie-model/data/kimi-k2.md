# Kimi K2 - https://github.com/MoonshotAI/Kimi-K2

State-of-the-art Mixture-of-Experts (MoE) language model from Moonshot AI with 1 trillion total parameters and exceptional coding performance. Released July 2025, outperforming Claude Opus 4 and GPT-4.1 on coding benchmarks.

## General Info

### Classification
- AIE/Model

### Version
- K2.5 (January 27, 2026)

### Repo
- https://github.com/MoonshotAI/Kimi-K2

### Rating
- SWE-bench Verified: 65.8% (K2), higher on SWE-bench Multilingual (K2.5)
- SWE-bench Full: 71.6%
- LiveCodeBench: 53.7%
- HLE-Full: Surpasses GPT-5.2 and Claude Opus 4.5 (with tools)

### Short Description
- Trillion-parameter open-weight MoE model from Alibaba-backed Moonshot AI, achieving state-of-the-art performance in coding, math, and agentic reasoning with 32B activated parameters.

### Description
Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model developed by Moonshot AI, a Beijing startup backed by Alibaba. Released in July 2025, it features 32 billion activated parameters and 1 trillion total parameters, making it one of the largest open-weight models available. The model achieves exceptional performance in frontier knowledge, mathematics, and coding, surpassing Claude Opus 4 on multiple benchmarks and demonstrating better overall performance than OpenAI's GPT-4.1. With 65.8% pass@1 on SWE-bench Verified and 71.6% on full SWE-bench, Kimi K2 excels at agentic reasoning and real-world coding tasks. The model is cost-effective at $0.15 per million input tokens and $2.50 per million output tokensâ€”significantly cheaper than competitors.

### Languages
- Any

### Notes
- Trillion-parameter MoE: 1 trillion total parameters with 32B activated, one of the largest open-weight models.
- Exceptional coding performance: 71.6% on SWE-bench, 65.8% on SWE-bench Verified, 53.7% on LiveCodeBench.
- Surpasses proprietary models: Outperforms Claude Opus 4 and GPT-4.1 on coding benchmarks.
- Long context support: Optimized for long-context understanding and processing.
- Agentic reasoning: Strong performance on tool use and multi-step reasoning tasks.
- Cost-effective: $0.15/M input tokens, $2.50/M output tokens (100x cheaper than Claude Opus 4 for input).
- Open weights: Available on GitHub and Hugging Face for community use.
- Moonshot AI: Developed by Alibaba-backed Beijing startup.
- Released July 2025 as a major advancement in open-weight models.
- 2025-2026 Update: K2 Thinking and K2 Thinking Turbo released November 6, 2025 for complex reasoning and agentic tasks. Major price cuts: 75% off cached input, 50% off uncached input. K2.5 released January 27, 2026 as native multimodal agentic model trained on 15T mixed visual/text tokens. K2.5 features 256K context, MoonViT 400M vision encoder, 384 experts (8 selected per token), Agent Swarm for multi-agent parallel execution. Benchmarks show K2.5 surpassing GPT-5.2 on SWE-bench Multilingual and VideoMMMU. Moonshot raised at $4.8B valuation.

### Last Update
- 2026-01-30

## Licensing

### Opensource
- Yes
  - Open weights available on GitHub and Hugging Face

### License
- Apache-2.0

### Free Trial
- Yes
