# Qwen3 - https://qwenlm.github.io/blog/qwen3/

Alibaba's flagship trillion-parameter large language model with MoE architecture. Qwen3-Max achieves 69.6% on SWE-Bench Verified and perfect scores on AIME 2025 and HMMT math benchmarks, competing with GPT-5 and Claude Opus 4.

## General Info

### Classification
- AIE/Model

### Version
- Qwen3-Max (2025)

### Repo
- https://github.com/QwenLM/Qwen3

### Rating
- SWE-bench Verified: 69.6%
- AIME 2025: 100%

### Short Description
- Alibaba's trillion-parameter MoE model excelling at reasoning, coding, and multilingual tasks with adaptive thinking modes and 262K token context.

### Description
Qwen3-Max is Alibaba's flagship large language model featuring approximately one trillion parameters trained on 36 trillion tokens using a mixture-of-experts decoder architecture. It supports up to 262,000 input tokens and 65,536 output tokens. The model is available in base, instruction-tuned, and Thinking (reasoning) variants. Qwen3-Max-Thinking achieves perfect 100-point scores on AIME 2025 and HMMT math benchmarks, while the instruction variant scored 69.6% on SWE-Bench Verified. The model can seamlessly switch between thinking mode for complex reasoning and non-thinking mode for efficient dialogue. The broader Qwen3 family includes open-weight variants like Qwen3-30B-A3B (30.5B total, 3.3B active across 128 experts) and multimodal extensions including Qwen3-VL-235B and Qwen3-Omni-30B.

### Languages
- Any

### Notes
- Trillion-parameter MoE architecture trained on 36 trillion tokens
- Perfect scores on AIME 2025 and HMMT mathematical reasoning benchmarks
- 262K input / 65K output token context window
- Adaptive reasoning: switches between thinking and non-thinking modes
- Qwen3-Max is closed-weights; smaller Qwen3 variants are open-weight
- Multimodal extensions available (Qwen3-VL, Qwen3-Omni)
- Ranked third globally on Text Arena leaderboard, surpassing GPT-5-Chat

### Last Update
- 2026-01-30

## Licensing

### Opensource
- No

### License
- Proprietary (Qwen3-Max); Apache 2.0 (smaller open-weight variants)

### Free Trial
- Yes
