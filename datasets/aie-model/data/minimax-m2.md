# MiniMax M2 - https://www.minimax.io/news/minimax-m2

MiniMax M2 is a compact, fast, and cost-effective MoE (Mixture of Experts) model with 230 billion total parameters and 10 billion active parameters, built for elite performance in coding and agentic tasks.

**Dataset ID:** aie-model

## General Info

### Classification
- AIE/Model

### Version
- M2 (October 2025)

### Repo
- https://github.com/MiniMax-AI/MiniMax-M2

### Rating
- SWE-bench Verified: 69.4% (close to GPT-5's 74.9%)
- #1 among open-source models globally in composite intelligence score

### Short Description
Compact 230B-parameter MoE model (10B active) optimized for coding and agentic workflows, offering elite performance at 8% the cost of Claude Sonnet with 2x speed.

### Description
MiniMax M2 is an open-source model launched in late October 2025, designed specifically for coding and agentic workflows. As a Mixture of Experts (MoE) architecture, it features 230 billion total parameters with only 10 billion active parameters, achieving remarkable efficiency without sacrificing performance.

The model excels in end-to-end development workflows and integrates seamlessly with popular coding tools like Claude Code, Cursor, Cline, Kilo Code, and Droid. It performs exceptionally well on multi-file edits, coding-run-fix loops, and test-validated repairs. Its strong performance on Terminal-Bench and SWE-Bench-style tasks demonstrates practical effectiveness in terminals, IDEs, and CI environments across multiple programming languages.

At only 8% of the price of Claude Sonnet and twice the speed, MiniMax M2 offers compelling value for developers. The model is available for free for a limited time and ranks #1 among open-source models in composite intelligence scores across mathematics, science, instruction following, coding, and agentic tool use.

### Languages
- Any

### Notes
- Architecture: MoE (230B total params, 10B active)
- Pricing: 8% of Claude Sonnet cost, 2x faster, free for limited time
- Benchmark Performance: SWE-bench Verified 69.4% (near GPT-5's 74.9%)
- Tool Integration: Claude Code, Cursor, Cline, Kilo Code, Droid
- Coding Strengths: Multi-file edits, coding-run-fix loops, test-validated repairs
- Performance Tests: Strong on Terminal-Bench and Multi-SWE-Bench tasks
- Global Ranking: #1 open-source model in composite intelligence score
- Release: Late October 2025
- Availability: Open-source on GitHub, available via API platforms

### Last Update
- 2025-11-18

## Licensing

### Opensource
- Yes

### License
- -
