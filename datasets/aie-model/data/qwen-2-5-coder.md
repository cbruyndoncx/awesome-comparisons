# Qwen 2.5 Coder - https://github.com/QwenLM/Qwen-Code

Latest coding-focused model from Alibaba's Qwen family, featuring major upgrades in code understanding and generation capabilities. Released in 2025 as a significant advancement in open-source coding models.

## General Info

### Classification
- AIE/Model

### Version
- Qwen3-Max (September 5, 2025); latest snapshot qwen3-max-2026-01-23 (January 2026)

### Repo
- https://github.com/QwenLM/Qwen-Code

### Rating
- HumanEval: 88.4% (Qwen 2.5 Coder 7B)
- Spider: 82.0% (Qwen 2.5 Coder 7B)
- LiveCodeBench: 38.7 (Qwen2.5-Max)

### Short Description
- Alibaba Cloud's family of specialized coding models (1.5B-32B parameters) with state-of-the-art performance on code generation and debugging benchmarks.

### Description
Qwen 2.5 Coder is a family of coding-specialized language models developed by Alibaba Cloud, released on September 19, 2024. The series includes models ranging from 1.5B to 32B parameters, targeting performance levels closer to closed-source models.

The flagship Qwen 2.5 Coder 7B model achieves 88.4% on the HumanEval benchmark, surpassing both Codestral (81.1%) and DeepSeek Coder v2 Lite (81.1%). On the Spider benchmark for SQL generation, Qwen 2.5 Coder leads with 82.0% compared to Codestral's 76.6%.

In January 2025, Alibaba released Qwen2.5-Max, their most advanced model available through Alibaba Cloud's Model Studio and the Qwen Chat platform. This model excels in code generation and debugging with a LiveCodeBench score of 38.7, slightly behind Claude Sonnet's 38.9. Qwen2.5-Max is positioned to compete with models like DeepSeek R3 in the enterprise coding space.

### Languages
- Any

### Notes
- Model Family: 1.5B, 3B, 7B, 14B, 32B parameters
- Release: Qwen 2.5 Coder (September 2024), Qwen2.5-Max (January 2025)
- Benchmarks:
  - HumanEval: 88.4% (7B model)
  - Spider: 82.0% (7B model)
  - LiveCodeBench: 38.7 (Max model)
- Strengths: Code generation, debugging, SQL generation
- Availability: Alibaba Cloud Model Studio, Qwen Chat platform
- Competition: Positioned against DeepSeek R3, Codestral, Claude Sonnet
- Open source: Qwen 2.5 Coder series available for local deployment
- Developer: Alibaba Cloud
- 2025-2026 Update: Qwen2.5-Max launched January 2025 as MoE model pretrained on 20T+ tokens, outperforming GPT-4o and DeepSeek-V3. Qwen2.5-VL multimodal family released January 2025. Qwen3-Max launched September 5, 2025, outperforming other foundation models. Latest snapshot qwen3-max-2026-01-23 integrates thinking/non-thinking modes with web search, code interpreter, and webpage extraction tools. Supports 92 programming languages and 29 natural languages with 128K context window.

### Last Update
- 2026-01-30

## Licensing

### Opensource
- Yes

### License
- Apache-2.0

### Free Trial
- Yes
