[{"type":"header","level":1,"content":"Aider - https://aider.chat","children":[{"type":"text","content":"Aider is the first popular AI Coding CLI, with a fairly recently added webbased gui.\nAider is a python package and frequently updated with a mass following and an active Discord community."},{"type":"header","level":2,"content":"Version","children":[{"type":"text","content":"tbd (2025-10-18)"}]},{"type":"header","level":2,"content":"Rating","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"[4] Popular, actively maintained OSS CLI with a large community","plainChildren":""},{"type":"item","level":2,"content":"[4] Strong Git integration and multi-LLM support; good for whole-repo edits","plainChildren":""}]}]},{"type":"header","level":2,"content":"Repository","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"https://github.com/Aider-AI/aider","plainChildren":""}]}]},{"type":"header","level":2,"content":"Languages","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Any","plainChildren":""}]}]},{"type":"header","level":2,"content":"Extensible","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""},{"type":"item","level":2,"content":"Supports multiple LLM backends and local model adapters; configurable via model adapters and API-key settings","plainChildren":""}]}]},{"type":"header","level":2,"content":"Description","children":[{"type":"text","content":"Aider is an open-source AI pair-programming tool focused on the terminal-first developer workflow. It allows developers to chat with LLMs, make multi-file edits, run linters/tests, and commit changes automatically via Git — all from the CLI (with an optional web GUI). Aider works across languages and large codebases by building and using a code-map of the repository to provide context-aware edits. It supports multiple remote LLM providers and local models, enabling private and offline workflows."}]},{"type":"header","level":2,"content":"BYOK","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":2,"content":"LocalOffline","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""},{"type":"item","level":2,"content":"Supports local model usage via adapters (community integrations exist for local runtimes such as Ollama and other self-hosted LLMs)","plainChildren":""}]}]},{"type":"header","level":2,"content":"FreeTrial","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""},{"type":"item","level":2,"content":"The tool itself is free/open-source (users pay the LLM provider they connect to)","plainChildren":""}]}]},{"type":"header","level":2,"content":"GitSupport","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":2,"content":"Terminal","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":2,"content":"Opensource","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":2,"content":"License","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"MIT","plainChildren":""}]}]},{"type":"header","level":2,"content":"MCP-Client","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"No","plainChildren":""}]}]},{"type":"header","level":2,"content":"Notes","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"BYOK model: users supply API keys for whichever LLM provider they choose (OpenAI, Anthropic, DeepSeek, etc.), allowing control over costs and provider choice.","plainChildren":""},{"type":"item","level":2,"content":"Local/model support: Aider can connect to local LLMs (self-hosted or via local runtimes), enabling fully offline workflows and private model usage.","plainChildren":""},{"type":"item","level":2,"content":"Cost optimization: Supports prompt-caching patterns and lets you choose cheaper models or local models to reduce usage costs.","plainChildren":""},{"type":"item","level":2,"content":"Workflow strengths: deep Git integration (auto-commit with sensible commit messages, undo commit, diff), in-chat file management (/add, /drop), lint/test runs, and automatic retries/fixes when tests fail.","plainChildren":""},{"type":"item","level":2,"content":"UX features: terminal-first chat, optional web GUI, voice input, ability to ingest web pages/images for context, and pointing to CONVENTIONS.md to enforce project-specific rules.","plainChildren":""},{"type":"item","level":2,"content":"Installation: pip-based installer (e.g., python -m pip install aider-install; then run aider-install), then run aider in a repo with your chosen model and API key.","plainChildren":""},{"type":"item","level":2,"content":"Good fit for: teams wanting repo-aware AI edits, those requiring private/local model runs, developers who prefer CLI workflows and Git-backed safety for AI edits.","plainChildren":""},{"type":"item","level":2,"content":"Limitations/considerations: Aider is a thin orchestration layer — actual model behavior, costs, and availability depend on chosen LLM provider or local runtime; evaluate model performance and token costs for your use case before large-scale adoption.","plainChildren":""}]}]}],"sourcePath":"aider.md"},{"type":"header","level":1,"content":"Claude Code - https://claude.ai/code","children":[{"type":"text","content":"Claude Code is Anthropic's command-line, agentic developer assistant that integrates Claude models into terminal workflows to help write, refactor, and manage code across repositories."},{"type":"header","level":2,"content":"Version","children":[{"type":"text","content":"v1.0 (2025-10-18)"}]},{"type":"header","level":2,"content":"Rating","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"[4] Strong reasoning and long-context handling","plainChildren":""},{"type":"item","level":2,"content":"[3] Can be costly at the highest-capability model tiers","plainChildren":""},{"type":"item","level":2,"content":"[4] Excellent for multi-file refactors and end-to-end developer workflows","plainChildren":""}]}]},{"type":"header","level":2,"content":"Repository","children":[{"type":"text","content":"-"}]},{"type":"header","level":2,"content":"Languages","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Any","plainChildren":""}]}]},{"type":"header","level":2,"content":"Extensible","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":2,"content":"Description","children":[{"type":"text","content":"Claude Code is a low-level, terminal-first developer tool from Anthropic that exposes Claude family models (Opus, Sonnet, Haiku tiers) to developer workflows. It is intentionally unopinionated and scriptable: it runs in the terminal, integrates with Git and other CLI tools, and can be wired into CI/CD, deployment systems, and custom automation. Claude Code asks permission before making file edits or running commands, and it can be configured to use Anthropic's public API, cloud-hosted model endpoints (e.g., Bedrock, Vertex AI), or organization-managed keys.\n\nThe tool is focused on large-context reasoning, multi-file code transformations, automated testing and verification, and agentic workflows where the assistant can run sequences of commands, apply patches, and validate results. It is suitable for both single developers and teams; for the latter it is offered as part of Anthropic's Team/Enterprise plans with central administration and enhanced security controls."}]},{"type":"header","level":2,"content":"BYOK","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":2,"content":"LocalOffline","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"No","plainChildren":""},{"type":"item","level":2,"content":"Any additional details like Ollama: Claude Code relies on remote model endpoints by default. Anthropic and enterprise customers can route requests through cloud provider-hosted model deployments, but a fully offline/local model runtime is not provided as a standard option.","plainChildren":""}]}]},{"type":"header","level":2,"content":"FreeTrial","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":2,"content":"GitSupport","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":2,"content":"Terminal","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":2,"content":"Opensource","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"No","plainChildren":""}]}]},{"type":"header","level":2,"content":"License","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":2,"content":"MCP-Client","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":2,"content":"Notes","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Models: Offers multiple model tiers (commonly referenced as Opus (highest capability), Sonnet (workhorse), and Haiku (cost-efficient)). Opus is aimed at deep reasoning and large refactors; Sonnet balances cost and capability; Haiku is optimized for high-volume, lower-complexity tasks.","plainChildren":""},{"type":"item","level":2,"content":"Pricing: Available as seat-based subscriptions (Pro, Max tiers) and pay-as-you-go API token pricing. High-capability models (Opus) carry premium token costs; Sonnet often provides a better cost/performance tradeoff for everyday coding.","plainChildren":""},{"type":"item","level":2,"content":"Context window: Claude-family models marketed with very large context windows (useful for large repositories and multi-file edits).","plainChildren":""},{"type":"item","level":2,"content":"Platform support: macOS, Linux, Windows (CLI-first). Windows usage commonly requires Git for Windows for full CLI feature parity.","plainChildren":""},{"type":"item","level":2,"content":"Use cases: automated refactors, multi-file PR generation, test generation and repair, code review assistance, automated CI hooks, developer productivity automation.","plainChildren":""},{"type":"item","level":2,"content":"Safety & controls: interactive permission prompts, enterprise controls for data handling, and options to route through organization-managed endpoints.","plainChildren":""},{"type":"item","level":2,"content":"Ecosystem: community tooling and integrations exist (context engineers, wrappers, \"awesome\" lists) though the official product is closed-source.","plainChildren":""},{"type":"item","level":2,"content":"Further reading: consult the official Claude documentation at https://claude.ai/ and Anthropic's product pages for up-to-date pricing, model names, and deployment options.","plainChildren":""}]}]}],"sourcePath":"claude-code.md"},{"type":"header","level":1,"content":"Codex CLI - https://github.com/openai/codex/","children":[{"type":"text","content":"Codex CLI is an open-source command-line interface for interacting with OpenAI's Codex model and other compatible LLM providers."},{"type":"header","level":2,"content":"Version","children":[{"type":"text","content":"(2025-10-19)"}]},{"type":"header","level":2,"content":"Rating","children":[]},{"type":"header","level":2,"content":"Repository","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"https://github.com/openai/codex","plainChildren":""}]}]},{"type":"header","level":2,"content":"Languages","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Any","plainChildren":""}]}]},{"type":"header","level":2,"content":"Extensible","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes (via provider gateways and config)","plainChildren":""}]}]},{"type":"header","level":2,"content":"Description","children":[{"type":"text","content":"Codex CLI provides a lightweight, terminal-first agent that lets developers read, modify, and execute code using large language models. It runs tasks in sandboxed environments, can run linters/tests, propose edits, and integrate changes back into a local repository or create pull requests. The CLI supports multiple operation modes (suggest, auto-edit, full-auto) which control how much automation is allowed without user approval."}]},{"type":"header","level":2,"content":"BYOK","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""},{"type":"item","level":2,"content":"Uses API keys for model providers; supports configuring different gateways","plainChildren":""}]}]},{"type":"header","level":2,"content":"LocalOffline","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"No","plainChildren":""},{"type":"item","level":2,"content":"Primarily uses remote model providers, though configurations and gateways (e.g., LiteLLM/Ollama proxies) can enable local model endpoints in some setups","plainChildren":""}]}]},{"type":"header","level":2,"content":"FreeTrial","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""},{"type":"item","level":2,"content":"Availability depends on chosen model provider (OpenAI, Ollama, etc.)","plainChildren":""}]}]},{"type":"header","level":2,"content":"GitSupport","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""},{"type":"item","level":2,"content":"Can produce commits and be used to prepare PRs (depends on workflow and config)","plainChildren":""}]}]},{"type":"header","level":2,"content":"Terminal","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":2,"content":"Opensource","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":2,"content":"License","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"MIT","plainChildren":""}]}]},{"type":"header","level":2,"content":"MCP-Client","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":2,"content":"Notes","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Supports multiple model providers (OpenAI, OpenRouter, Gemini, Ollama, Mistral, DeepSeek, xAI, Groq and other OpenAI-compatible endpoints) via configuration and gateways.","plainChildren":""},{"type":"item","level":2,"content":"Configuration stored in ~/.codex/config.toml; users can create reusable prompts and agent settings per-repo.","plainChildren":""},{"type":"item","level":2,"content":"Operational modes:","plainChildren":""},{"type":"item","level":2,"content":"Suggest (default): proposes edits and commands; requires user approval before applying changes.","plainChildren":""},{"type":"item","level":2,"content":"Auto Edit: autonomously reads/writes files, but asks before executing shell commands.","plainChildren":""},{"type":"item","level":2,"content":"Full Auto: performs reads, writes, and executes commands in a sandboxed, network-disabled environment without additional prompts.","plainChildren":""},{"type":"item","level":2,"content":"AGENTS.md: repository-level guidance files can be added to help the agent understand project structure, test commands, and conventions.","plainChildren":""},{"type":"item","level":2,"content":"Useful for exploratory code tasks, automated refactors, running test-fix cycles, and generating PR-ready diffs; best results when the repository includes clear tests and documentation.","plainChildren":""}]},{"type":"text","content":"Sources: GitHub repository (https://github.com/openai/codex) and project documentation/examples aggregated from public write-ups about Codex CLI and its configuration."}]}],"sourcePath":"codex-cli.md"},{"type":"header","level":1,"content":"OpenCode - https://opencode.ai","children":[{"type":"text","content":"A terminal-native, open-source AI coding assistant that brings LLM-powered code understanding, generation and editing directly into the developer's terminal. OpenCode focuses on privacy (local-first model support), extensibility (custom actions/skills), and deep integration with developer workflows (git, LSP, shell)."},{"type":"header","level":2,"content":"Version","children":[{"type":"text","content":"v (2025-10-19)"}]},{"type":"header","level":2,"content":"Classification","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Code/Editor","plainChildren":""}]}]},{"type":"header","level":2,"content":"Rating","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"[5] Terminal-native, privacy-first design","plainChildren":""},{"type":"item","level":2,"content":"[5] Broad LLM provider support (Models.dev)","plainChildren":""},{"type":"item","level":2,"content":"[4] Rich toolset for file, shell, and repo operations","plainChildren":""}]}]},{"type":"header","level":2,"content":"Repository","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"https://github.com/sst/opencode","plainChildren":""}]}]},{"type":"header","level":2,"content":"Languages","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Any","plainChildren":""}]}]},{"type":"header","level":2,"content":"Extensible","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":2,"content":"Description","children":[{"type":"text","content":"OpenCode is an open-source AI assistant designed for use from the terminal. It provides a conversational interface and a suite of programmatic tools (file operations, grep/glob, patch/apply, diagnostics, shell execution, fetch, Sourcegraph search, etc.) that allow the assistant to read, explain, modify and create code in the context of a repository. OpenCode supports both cloud and local LLMs (through Models.dev and adapters such as Ollama), can initialize project context, produce implementation plans, and then switch to a build mode to apply edits. Its architecture supports extensible actions/skills and hierarchical agents for complex tasks."}]},{"type":"header","level":2,"content":"BYOK","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":2,"content":"LocalOffline","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""},{"type":"item","level":2,"content":"Supports running with local model backends (e.g. Ollama) and Models.dev adapters.","plainChildren":""}]}]},{"type":"header","level":2,"content":"FreeTrial","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":2,"content":"GitSupport","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":2,"content":"Terminal","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":2,"content":"Opensource","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":2,"content":"License","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"MIT","plainChildren":""}]}]},{"type":"header","level":2,"content":"MCP-Client","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":2,"content":"Notes","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Strong privacy posture: can run entirely with local models so source code does not need to be uploaded to third-party APIs.","plainChildren":""},{"type":"item","level":2,"content":"Rich tool set that mirrors common CLI developer actions (ls, grep, view, write, edit, patch, bash) so it can operate robustly on repositories.","plainChildren":""},{"type":"item","level":2,"content":"Useful workflow modes: Plan mode (no edits, design/strategy) and Build mode (apply changes). Includes undo/redo for edits.","plainChildren":""},{"type":"item","level":2,"content":"Integrates with LSP/diagnostics for precise error detection and fixes.","plainChildren":""},{"type":"item","level":2,"content":"Extensible via custom actions/skills and supports spawning sub-agents for task decomposition.","plainChildren":""},{"type":"item","level":2,"content":"Good fit for polyglot environments and CI/CD automation where a terminal-first interface and scriptability are advantages.","plainChildren":""},{"type":"item","level":2,"content":"Main website: https://opencode.ai — repo and docs live at https://github.com/sst/opencode","plainChildren":""}]}]},{"type":"header","level":2,"content":"ContextManagement","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""},{"type":"item","level":2,"content":"OpenCode builds and maintains repository-aware context by scanning and indexing the project (repository analysis via `/init`) and persisting session state and conversation history in SQLite. It exposes project-scoped sessions, keeps file associations, and uses LSP and file-system reads to surface relevant snippets when answering questions or generating changes. See: https://github.com/sst/opencode and https://opencode.ai/docs","plainChildren":""}]}]},{"type":"header","level":2,"content":"DirectFileReferences","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""},{"type":"item","level":2,"content":"Files can be directly referenced and manipulated via the CLI/TUI: file explorer panels, explicit read/view/edit actions, patch/apply operations, and path-based prompts. The assistant can open specific files, show diffs, write changes, and apply/revert patches programmatically.","plainChildren":""}]}]},{"type":"header","level":2,"content":"Hooks","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"No","plainChildren":""},{"type":"item","level":2,"content":"There are no widely-documented lifecycle \"hook\" events for agent-generated actions similar to webhooks or lifecycle callbacks. Extensibility is provided via custom actions/skills and command extensions rather than a formal lifecycle hook system (see actions/skills and custom commands in the docs/repo).","plainChildren":""}]}]},{"type":"header","level":2,"content":"SlashCommands","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""},{"type":"item","level":2,"content":"OpenCode provides slash-style commands for common workflows (examples include `/init` to analyze a repo, `/undo` and `/redo` to revert or restore changes). Users can create custom commands/prompts to speed repetitive interactions.","plainChildren":""}]}]},{"type":"header","level":2,"content":"Subagents","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""},{"type":"item","level":2,"content":"The architecture supports hierarchical agents and the spawning of sub-agents (subtasks/skills) for decomposing complex work into smaller steps; custom actions/skills can act as specialized subagents for domain tasks.","plainChildren":""}]}]},{"type":"header","level":2,"content":"CustomModes","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""},{"type":"item","level":2,"content":"OpenCode provides at least two explicit modes: Plan Mode (read-only design/planning) and Build Mode (apply edits). Modes can be toggled in the TUI (Tab key), enabling different agent behaviours (e.g., generate a plan vs. make changes).","plainChildren":""}]}]},{"type":"header","level":2,"content":"Plugins","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""},{"type":"item","level":2,"content":"Extensible via actions/skills, model adapters, and custom commands. Developers can add new \"skills\" or adapters (e.g., Ollama, Models.dev connectors) and script bespoke tool integrations. There is no single centralized marketplace documented; extensibility is file- and config-driven in the repo.","plainChildren":""}]}]},{"type":"header","level":2,"content":"Checkpoints","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""},{"type":"item","level":2,"content":"OpenCode supports undo/redo for edits, persistent session history (SQLite), and shows diffs/patches to review before applying. It also integrates with Git workflows so developers can rely on VCS history as an additional checkpoint mechanism.","plainChildren":""}]}]},{"type":"header","level":2,"content":"SpecDrivenDevelopment","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Other","plainChildren":""},{"type":"item","level":2,"content":"OpenCode is not bundled with or tied to specific spec-driven frameworks (BMAD, SpecKit, OpenSpec, Tessl, AgentOS, ClaudeFlow, SPARC, SuperClaude). It can, however, support spec-driven workflows through Plan Mode and scripted skills—developers would integrate their preferred SDD tooling themselves rather than rely on a built-in spec framework.","plainChildren":""}]}]}],"sourcePath":"opencode.md"},{"type":"header","level":1,"content":"Warp - https://www.warp.dev","children":[{"type":"text","content":"Modern AI-powered terminal and agentic development environment focused on unifying natural-language agents, terminal commands, and collaborative \"Drive\" features for teams."},{"type":"header","level":2,"content":"Version","children":[{"type":"text","content":"v2.0 (2025-09-xx)"}]},{"type":"header","level":2,"content":"Rating","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"[5] Best-in-class terminal + AI integration (Agent Mode, Drive, Warp Code)","plainChildren":""},{"type":"item","level":2,"content":"[4] Commercial/proprietary product; some enterprise privacy controls but limited public BYOK/local-offline detail","plainChildren":""}]}]},{"type":"header","level":2,"content":"Repository","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"https://www.warp.dev","plainChildren":""}]}]},{"type":"header","level":2,"content":"Languages","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Any","plainChildren":""}]}]},{"type":"header","level":2,"content":"Extensible","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""},{"type":"item","level":2,"content":"Support for Workflows, Notebooks, Drive-based sharing and agent configuration","plainChildren":""}]}]},{"type":"header","level":2,"content":"Description","children":[{"type":"text","content":"Warp is a modern terminal that has evolved into an \"Agentic Development Environment\" (ADE). It tightly integrates large language models and agent workflows with the command line, providing natural-language command generation, an interactive chat/agent interface (Agent Mode), collaborative Drive features (Workflows, Notebooks, shared Environment Variables), and code-focused tooling (Warp Code with diff-tracking). Warp's interface lets developers mix prompts and shell commands in a single input, run multi-step agent plans, review and accept diffs produced by agents, and share/run parameterized Workflows across teams."}]},{"type":"header","level":2,"content":"BYOK","children":[]},{"type":"header","level":2,"content":"LocalOffline","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"No","plainChildren":""},{"type":"item","level":2,"content":"Note: Warp runs a local classifier to detect natural-language input, and some metadata/local processing happens on-device, but AI requests are typically proxied to selected LLM providers (OpenAI, Anthropic/Claude variants) unless explicitly configured via enterprise arrangements. There is no broadly-documented fully-offline LLM mode for the AI assistant as of the latest public releases.","plainChildren":""}]}]},{"type":"header","level":2,"content":"FreeTrial","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":2,"content":"GitSupport","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""},{"type":"item","level":2,"content":"Git workflows are supported via the terminal and Warp's code/diff UX; the tool is used to author and review code diffs produced by agents.","plainChildren":""}]}]},{"type":"header","level":2,"content":"Terminal","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":2,"content":"Opensource","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"No","plainChildren":""}]}]},{"type":"header","level":2,"content":"License","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":2,"content":"MCP-Client","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Yes","plainChildren":""},{"type":"item","level":2,"content":"Warp supports selecting/modeling different hosted LLM providers and includes enterprise features like zero-data-retention guarantees and proxying; it exposes autonomy controls for when agents may call MCP servers without human approval.","plainChildren":""}]}]},{"type":"header","level":2,"content":"Notes","children":[{"type":"list","level":2,"children":[{"type":"item","level":2,"content":"Key features: Agent Mode (natural language -> commands), Warp Code (diff-tracking and stepwise code edits by agents), Drive (shared Workflows and Notebooks), Planning Mode and multi-agent orchestration.","plainChildren":""},{"type":"item","level":2,"content":"Privacy controls: granular autonomy settings (allowlists/denylists, pause/approve diffs, control file access), network logging, and zero-data-retention guarantees for enterprise customers.","plainChildren":""},{"type":"item","level":2,"content":"Strengths: Unified UX for prompt + shell input, native diff review for agent-made changes, strong collaboration primitives for teams.","plainChildren":""},{"type":"item","level":2,"content":"Limitations / unknowns: public documentation is limited on BYOK (bring-your-own-key) and fully offline local LLM operation; product is proprietary which may limit on-premise customization for some organizations.","plainChildren":""},{"type":"item","level":2,"content":"Recommended when: you want a first-class terminal with integrated AI agents and team sharing (Drive), and you prefer an opinionated, commercial product with enterprise privacy controls rather than an open-source self-hosted solution.","plainChildren":""}]}]}],"sourcePath":"warp.md"}]