[{"type":"header","level":1,"content":"Claude Haiku 4.5 - https://www.anthropic.com/claude/haiku","children":[{"type":"text","content":"Claude Haiku 4.5 delivers similar levels of coding performance to Claude Sonnet 4 but at one-third the cost and more than twice the speed. Released October 15, 2025.\n\n**Dataset ID:** aie-model"},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"4.5 (October 15, 2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"SWE-bench Verified: 73.3%","plainChildren":""},{"type":"item","level":3,"content":"Augment agentic coding evaluation: 90% of Sonnet 4.5's performance","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"text","content":"Anthropic's fast, cost-effective coding model that achieves Sonnet 4-level performance at one-third the cost and 2x+ the speed."}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Claude Haiku 4.5 was released on October 15, 2025, as Anthropic's latest compact yet powerful coding model. It delivers similar levels of coding performance to Claude Sonnet 4 while being one-third the cost and more than twice the speed.\n\nWith a score of 73.3% on SWE-bench Verified, it ranks as one of the world's best coding models. In Augment's agentic coding evaluation, Haiku 4.5 achieves 90% of Sonnet 4.5's performance while matching much larger models in capability.\n\nThe model is designed to work in tandem with Claude Sonnet 4.5, which remains the frontier model and best overall coding model. A recommended pattern is for Sonnet 4.5 to break down complex problems into multi-step plans, then orchestrate a team of multiple Haiku 4.5 instances to complete subtasks in parallel.\n\nUsers of Claude Code find that Haiku 4.5 makes the coding experience—from multiple-agent projects to rapid prototyping—markedly more responsive. The model is available on the API, Amazon Bedrock, Google Cloud's Vertex AI, and is rolling out in GitHub Copilot for Pro, Pro+, Business, and Enterprise tiers."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Release: October 15, 2025","plainChildren":""},{"type":"item","level":3,"content":"Performance: 73.3% SWE-bench Verified, 90% of Sonnet 4.5 in agentic coding","plainChildren":""},{"type":"item","level":3,"content":"Cost: One-third the cost of Sonnet 4","plainChildren":""},{"type":"item","level":3,"content":"Speed: More than 2x faster than Sonnet 4","plainChildren":""},{"type":"item","level":3,"content":"Strategic Use: Designed for parallel subtask execution orchestrated by Sonnet 4.5","plainChildren":""},{"type":"item","level":3,"content":"Availability: API, Amazon Bedrock, Vertex AI, GitHub Copilot","plainChildren":""},{"type":"item","level":3,"content":"Claude Code: Enhanced responsiveness for multi-agent projects and prototyping","plainChildren":""},{"type":"item","level":3,"content":"Developer: Anthropic","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: Part of Claude 4.5 family with Sonnet 4.5 (Sept 2025) and Opus 4.5 (Nov 2025). First Haiku-tier model with extended thinking, computer use, and context awareness. Supports 200K context window with up to 64K output tokens. Anthropic's safest model by automated alignment assessment. January 2026: Anthropic released updated 80-page constitution with reason-based alignment approach. Future iterations targeting native audio/video support and 1M+ context windows.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""},{"type":"item","level":3,"content":"Available in GitHub Copilot public preview","plainChildren":""}]}]}]}],"sourcePath":"claude-haiku-4-5.md"},{"type":"header","level":1,"content":"Claude Opus 4.5 - https://www.anthropic.com/claude/opus","children":[{"type":"text","content":"Anthropic's flagship model released November 24, 2025, first to break 80% on SWE-bench Verified (80.9%), featuring 200K context, 64K output, effort parameter for token efficiency, and 67% price reduction over previous Opus models."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"4.5 (2025-11-24)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"SWE-bench Verified: 80.9% (first to break 80%)","plainChildren":""},{"type":"item","level":3,"content":"Terminal Bench: 15% improvement over Sonnet 4.5","plainChildren":""},{"type":"item","level":3,"content":"50-75% reduction in tool calling errors and build/lint errors","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Anthropic's most intelligent model achieving 80.9% on SWE-bench Verified with advanced effort parameter, enhanced computer use capabilities, and self-improving agent features at 67% lower pricing than previous Opus.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Claude Opus 4.5 is Anthropic's flagship model released on November 24, 2025, as part of the Claude 4.5 family spanning September through November 2025. The model became the first AI to break the 80% threshold on SWE-bench Verified, achieving 80.9%—a 65% improvement over Claude 3.5 Sonnet and surpassing GPT-5.2 (approximately 70%) and Gemini 3 Pro (approximately 65%). With a 200,000 token context window and 64,000 token output capacity, Opus 4.5 automatically preserves all previous thinking blocks throughout conversations, maintaining reasoning continuity across extended multi-turn interactions. A unique effort parameter allows developers to control token usage—at medium effort, Opus 4.5 matches Sonnet 4.5's best performance while using 76% fewer output tokens; at high effort, it exceeds Sonnet 4.5 by 4.3 percentage points while using 48% fewer tokens. The model introduces a new zoom action for enhanced computer use, enabling detailed inspection of specific screen regions at full resolution for examining fine-grained UI elements. Opus 4.5 demonstrates state-of-the-art performance on complex enterprise tasks with 15% improvement over Sonnet 4.5 on Terminal Bench and achieves higher pass rates while using up to 65% fewer tokens in autonomous coding sessions. The model represents a breakthrough in self-improving AI agents—achieving peak performance in 4 iterations while other models couldn't match that quality after 10 iterations. Pricing is set at $5 per million input tokens and $25 per million output tokens, representing a 67% reduction compared to previous Opus models while delivering superior performance and token efficiency."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"First to break 80%: 80.9% on SWE-bench Verified, first AI model to exceed 80% threshold.","plainChildren":""},{"type":"item","level":3,"content":"Effort parameter: Unique control over token usage for thoroughness vs efficiency tradeoff.","plainChildren":""},{"type":"item","level":3,"content":"Token efficiency: At medium effort matches Sonnet 4.5 using 76% fewer tokens; at high effort exceeds Sonnet 4.5 using 48% fewer tokens.","plainChildren":""},{"type":"item","level":3,"content":"200K context: 200,000 token context window with 64,000 token output capacity.","plainChildren":""},{"type":"item","level":3,"content":"Thinking continuity: Automatically preserves all previous thinking blocks throughout conversations.","plainChildren":""},{"type":"item","level":3,"content":"Enhanced computer use: New zoom action for detailed screen region inspection at full resolution.","plainChildren":""},{"type":"item","level":3,"content":"65% improvement: Over Claude 3.5 Sonnet, surpassing GPT-5.2 (~70%) and Gemini 3 Pro (~65%).","plainChildren":""},{"type":"item","level":3,"content":"50-75% error reduction: In tool calling errors and build/lint errors with more reliable execution.","plainChildren":""},{"type":"item","level":3,"content":"Terminal Bench leader: 15% improvement over Sonnet 4.5 on terminal capabilities.","plainChildren":""},{"type":"item","level":3,"content":"Self-improving agents: Achieves peak performance in 4 iterations vs 10+ for competitors.","plainChildren":""},{"type":"item","level":3,"content":"Multi-agent excellence: Context management boosted performance by 15 percentage points on deep research.","plainChildren":""},{"type":"item","level":3,"content":"Long-horizon tasks: Maintains coherence across workflows spanning hours or days.","plainChildren":""},{"type":"item","level":3,"content":"Enterprise capabilities: Complex projects, financial analysis, cybersecurity, documentation, planning.","plainChildren":""},{"type":"item","level":3,"content":"67% price reduction: $5/$25 per million input/output tokens vs previous Opus pricing.","plainChildren":""},{"type":"item","level":3,"content":"Competitive efficiency: Uses roughly half as many tokens as Gemini 3 Pro and GPT-5.2 for comparable results.","plainChildren":""},{"type":"item","level":3,"content":"Superior debugging: Excels at code generation, debugging, code review, migration, refactoring, documentation.","plainChildren":""},{"type":"item","level":3,"content":"Enterprise operations: Manages spreadsheets, slides, documents while automating manual workflows.","plainChildren":""},{"type":"item","level":3,"content":"Financial analysis: Works across regulatory filings, market reports, internal data for predictive modeling.","plainChildren":""},{"type":"item","level":3,"content":"Cybersecurity: Correlates logs, issue databases, security intelligence for event detection and incident response.","plainChildren":""},{"type":"item","level":3,"content":"Dynamic performance: Described as \"dynamic rather than overthinking\" with remarkable speed improvements.","plainChildren":""},{"type":"item","level":3,"content":"Released November 24, 2025 as Anthropic's most intelligent model.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-02-06","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"claude-opus-4-5.md"},{"type":"header","level":1,"content":"Claude Opus 4.6 - https://www.anthropic.com/claude/opus","children":[{"type":"text","content":"Anthropic's most advanced AI model released February 5, 2026, featuring 1M token context window, agent teams coordination, 128k output tokens, and exceptional agentic coding capabilities with 144 Elo point lead over competitors."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"4.6 (2026-02-05)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"MRCR v2 long-context: 76%","plainChildren":""},{"type":"item","level":3,"content":"Outperforms GPT-5.2 by 144 Elo points","plainChildren":""},{"type":"item","level":3,"content":"Outperforms Opus 4.5 by 190 Elo points","plainChildren":""},{"type":"item","level":3,"content":"Best performance on BrowseComp benchmark","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Anthropic's most intelligent model with 1M token context window, agent teams feature, and best-in-class agentic coding capabilities for complex multi-step tasks and large codebase operations.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Claude Opus 4.6 is Anthropic's most advanced AI model, released on February 5, 2026, designed for complex coding, agentic workflows, and high-stakes enterprise applications. The model features a 1M token context window in beta (approximately 750,000 words), supporting up to 128k output tokens—double the previous limit. A groundbreaking agent teams feature allows multiple agents to work in parallel and coordinate autonomously on separate tasks, significantly accelerating complex workflows. Opus 4.6 excels at agentic coding with improved planning capabilities, breaking complex tasks into independent subtasks while running tools and subagents in parallel. Long-context performance improved dramatically with 76% on MRCR v2 benchmark versus 18.5% for its predecessor. The model demonstrates enhanced debugging and code review skills, catching its own mistakes and working reliably in larger codebases. New features include adaptive thinking that adjusts based on task complexity, max effort parameter for highest capability, Compaction API for effectively infinite conversations, and data residency controls. Available across claude.ai, Claude API, major cloud platforms, and GitHub Copilot. Pricing remains at $5/$25 per million input/output tokens with premium pricing ($10/$37.50) for prompts exceeding 200k tokens."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"1M token context: First Opus-class model with 1M token context window (beta), equivalent to ~750,000 words.","plainChildren":""},{"type":"item","level":3,"content":"Agent teams: Research preview feature enabling multiple agents to coordinate in parallel and work autonomously.","plainChildren":""},{"type":"item","level":3,"content":"128k output: Supports up to 128k output tokens, doubled from previous 64k limit.","plainChildren":""},{"type":"item","level":3,"content":"Superior performance: Outperforms GPT-5.2 by 144 Elo points and Opus 4.5 by 190 Elo points.","plainChildren":""},{"type":"item","level":3,"content":"Long-context mastery: 76% on MRCR v2 benchmark vs 18.5% for Opus 4.5, handling larger bodies of information consistently.","plainChildren":""},{"type":"item","level":3,"content":"Advanced planning: Breaks complex tasks into independent subtasks, runs tools in parallel, identifies blockers precisely.","plainChildren":""},{"type":"item","level":3,"content":"Enhanced debugging: Improved code review skills to catch own mistakes, works reliably in larger codebases.","plainChildren":""},{"type":"item","level":3,"content":"Adaptive thinking: Intelligently adjusts extended thinking based on contextual clues about task complexity.","plainChildren":""},{"type":"item","level":3,"content":"Max effort parameter: New effort level for absolute highest capability, balancing intelligence, speed, and cost.","plainChildren":""},{"type":"item","level":3,"content":"Compaction API: Beta feature for server-side context summarization enabling effectively infinite conversations.","plainChildren":""},{"type":"item","level":3,"content":"Data residency: Inference location control via inference_geo parameter (global or US-only at 1.1x pricing).","plainChildren":""},{"type":"item","level":3,"content":"PowerPoint integration: Research preview integrating Claude directly into PowerPoint as accessible side panel.","plainChildren":""},{"type":"item","level":3,"content":"Excel upgrades: Substantial improvements to Excel integration capabilities.","plainChildren":""},{"type":"item","level":3,"content":"API model ID: claude-opus-4-6","plainChildren":""},{"type":"item","level":3,"content":"Base pricing: $5 per million input tokens, $25 per million output tokens.","plainChildren":""},{"type":"item","level":3,"content":"Premium pricing: $10/$37.50 per million input/output tokens for prompts exceeding 200k tokens.","plainChildren":""},{"type":"item","level":3,"content":"US-only inference: Available at 1.1x token pricing for workloads requiring domestic processing.","plainChildren":""},{"type":"item","level":3,"content":"Wide availability: Immediately available on claude.ai, Claude API, all major cloud platforms, and GitHub Copilot.","plainChildren":""},{"type":"item","level":3,"content":"Breaking changes: Prefill removal—prefilling assistant messages not supported, returns 400 error.","plainChildren":""},{"type":"item","level":3,"content":"Broader audience: Originally positioned for developers, now serves product managers, financial analysts, and knowledge workers.","plainChildren":""},{"type":"item","level":3,"content":"Released February 5, 2026 as Anthropic's most advanced model to date.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-02-06","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"claude-opus-4-6.md"},{"type":"header","level":1,"content":"Claude Sonnet 4.5 - https://www.anthropic.com/claude/sonnet","children":[{"type":"text","content":"Anthropic's best coding model achieving state-of-the-art on SWE-bench Verified. Released September 29, 2025, capable of 30 hours of autonomous coding with 0% error rate on code editing benchmarks."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"4.5 (2025-09-29)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"SWE-bench Verified: 77.2%","plainChildren":""},{"type":"item","level":3,"content":"OSWorld computer-use: 61.4% (up from 42.2%)","plainChildren":""},{"type":"item","level":3,"content":"Harmlessness: 99.29% harmless response rate","plainChildren":""},{"type":"item","level":3,"content":"Agentic Security: 98.7% safety score (2/150 failures)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"State-of-the-art coding model that can autonomously code for 30 hours, build applications, manage databases, purchase domains, and perform security audits with exceptional editing accuracy.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Claude Sonnet 4.5 is Anthropic's best coding model, released on September 29, 2025, achieving state-of-the-art performance on the SWE-bench Verified evaluation for real-world software coding abilities. The model can run autonomously for up to 30 hours—a significant leap from Claude Opus 4's seven hours—during which it can build complete applications, stand up database services, purchase domain names, and perform SOC 2 security audits. Most remarkably, Claude Sonnet 4.5's code editing capabilities improved from Sonnet 4's 9% error rate to 0% on Anthropic's internal benchmarks. Despite being smaller than Claude Opus 4.1, it outperforms it in \"almost every single way\" according to Anthropic's chief product officer. Available at the same pricing as Sonnet 4 ($3/$15 per million tokens) and integrated into major coding tools including GitHub Copilot, Cursor, and Windsurf."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Best coding model: State-of-the-art on SWE-bench Verified, described as \"probably the best coding model in the world.\"","plainChildren":""},{"type":"item","level":3,"content":"30-hour autonomy: Can code autonomously for 30 hours vs 7 hours for Claude Opus 4.","plainChildren":""},{"type":"item","level":3,"content":"Perfect editing: 0% error rate on internal code editing benchmark, down from 9% on Sonnet 4.","plainChildren":""},{"type":"item","level":3,"content":"Full-stack capabilities: Builds apps, manages databases, purchases domains, performs security audits.","plainChildren":""},{"type":"item","level":3,"content":"Smaller but smarter: Outperforms larger Claude Opus 4.1 in \"almost every single way.\"","plainChildren":""},{"type":"item","level":3,"content":"SOC 2 security: Can autonomously perform security audits to ensure product compliance.","plainChildren":""},{"type":"item","level":3,"content":"Same pricing: $3 per million input tokens, $15 per million output tokens (same as Sonnet 4).","plainChildren":""},{"type":"item","level":3,"content":"Wide integration: Available in GitHub Copilot, Cursor, Windsurf, and other major tools.","plainChildren":""},{"type":"item","level":3,"content":"Anthropic: Leading AI safety-focused company founded by former OpenAI researchers.","plainChildren":""},{"type":"item","level":3,"content":"Released September 29, 2025 as a major advancement in AI coding.","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: Part of Claude 4.5 family with Haiku 4.5 (October 2025) and Opus 4.5 (November 24, 2025). SWE-bench Verified score reached 77.2%. Hybrid reasoning with extended thinking for complex problems. Computer use capabilities with OSWorld improvement from 42.2% to 61.4%. 200K context window with 64K output (1M preview). Knowledge cutoff: July 2025. Claude Code updates include checkpoints, VS Code extension, Agent SDK. Political bias reduced to 3.3% (1.3% in thinking mode). January 2026: Anthropic released updated 80-page constitution. 5-hour rolling session limits.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"claude-sonnet-4-5.md"},{"type":"header","level":1,"content":"CodeGeeX - https://github.com/THUDM/CodeGeeX","children":[{"type":"text","content":"Open-source multilingual code generation model from Tsinghua University (THUDM)."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"CodeGeeX4-ALL-9B (2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/THUDM/CodeGeeX","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"83.4% of users reported increased coding efficiency","plainChildren":""},{"type":"item","level":3,"content":"HumanEval-X: Multilingual code generation benchmark leader at release","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Multilingual large-scale code generation model (13B) from Tsinghua University (THUDM) for code generation, translation, completion, summarization, and IDE integration.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"CodeGeeX is a large-scale multilingual code generation model and toolkit developed by THUDM. It was trained on a massive corpus covering source code and natural language across many programming languages to support code generation, translation (cross‑language), completion, summarization, and explanation. The project provides model checkpoints (for research use), inference scripts, and IDE integrations (for example, a VS Code extension)."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"The original version, CodeGeeX v1 (13B parameters), was released in September 2022.","plainChildren":""},{"type":"item","level":3,"content":"Trained at large scale (reported training on 850B+ tokens and large TPU/Ascend clusters in original publications) and evaluated with a multilingual HumanEval-X benchmark.","plainChildren":""},{"type":"item","level":3,"content":"Provides cross-language code translation and multilingual code generation capabilities; reported strong performance compared to contemporaneous open models.","plainChildren":""},{"type":"item","level":3,"content":"IDE integrations (VS Code, JetBrains) exist to make the model usable as a coding assistant; downstream usage may be subject to the model weights' licensing terms.","plainChildren":""},{"type":"item","level":3,"content":"For commercial deployment, review the repository's instructions and registration process for obtaining the model weights.","plainChildren":""},{"type":"item","level":3,"content":"See also: CodeGeeX2 (follow-up) and related THUDM releases which may have differing licenses or access requirements.","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: CodeGeeX4-ALL-9B released, built on GLM-4-9B architecture with 9B parameters (down from 13B). Supports code interpreter, web search integration, function calling, repository-level code Q&A, and long context up to 1,048,576 tokens. Available via Cloud Studio and OpenAI-compatible servers. INT8-quantized variants available for resource-constrained deployments. First fully open large-scale multilingual code generation model with public end-to-end pre-training recipes.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Code: Apache-2.0","plainChildren":""},{"type":"item","level":3,"content":"Model weights: released for academic research; commercial use requires application/approval (see repository for details)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""},{"type":"item","level":3,"content":"Model code is open-source; weights are available for academic research. Commercial use of weights typically requires registration/approval per THUDM's model license terms.","plainChildren":""}]}]}]}],"sourcePath":"codegeex.md"},{"type":"header","level":1,"content":"Codeium Enterprise - https://codeium.com/","children":[{"type":"text","content":"Secure, enterprise-grade AI coding assistant with on-prem and air-gapped deployment options"},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Agent","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Latest (January 2026)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/exafunction/codeium.el","plainChildren":""},{"type":"item","level":3,"content":"https://github.com/exafunction/codeium-react-code-editor","plainChildren":""},{"type":"item","level":3,"content":"Note: The core Codeium Enterprise server and model infrastructure are proprietary; no public GitHub repo for the full enterprise/server distribution is published by the vendor.","plainChildren":""}]},{"type":"text","content":"<!-- Associated Github repositories (public components) -->"}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"[4] Strong enterprise deployment options (air-gapped, self-hosted VPC)","plainChildren":""},{"type":"item","level":3,"content":"[4] Broad language & IDE support (70+ languages, major IDEs)","plainChildren":""},{"type":"item","level":3,"content":"[3] Proprietary product — some key compliance details (e.g. BYOK) require vendor confirmation","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"text","content":"Commercial enterprise edition of Codeium providing SaaS, self-hosted VPC, and fully air-gapped on-prem deployments with enterprise security controls, audit logging, admin analytics, and private-code personalization."}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Codeium Enterprise is the commercial, enterprise-oriented edition of Codeium that focuses on security, privacy, and deployment flexibility for organizations. It supports SaaS, self-hosted VPC, and fully air-gapped on-premises deployments, enabling customers to keep code and model inference inside their network perimeter while delivering AI-assisted code completion, multi-file editing, and contextual suggestions informed by private codebases."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Deployments: Offers SaaS, self-hosted VPC, and fully air-gapped on-premises deployment models to meet strict data-sovereignty and compliance needs. Typical enterprise installations use containerized deployments (Helm/Kubernetes or Docker Compose) and can be run on infrastructure with GPU acceleration where required.","plainChildren":""},{"type":"item","level":3,"content":"Security: Enterprise features include end-to-end encryption, indexing access controls, audit logging, and administrative analytics. Air-gapped deployments ensure no code or telemetry leaves the customer environment.","plainChildren":""},{"type":"item","level":3,"content":"Platform & infra: Enterprise installs may require NVIDIA drivers and the NVIDIA Container Toolkit for GPU-accelerated deployments and access to enterprise container images (licensed by Codeium).","plainChildren":""},{"type":"item","level":3,"content":"Features: Personalization/finetuning on private codebases, subteam analytics, audit trails, priority enterprise support (dedicated Slack channels / support portal), and one-click installers for simplified deployment.","plainChildren":""},{"type":"item","level":3,"content":"Impact: Vendor materials and case studies report developer productivity gains (reduced PR cycle time, faster debugging and testing) when teams adopt the enterprise product.","plainChildren":""},{"type":"item","level":3,"content":"Gaps / vendor follow-up: Public-facing documentation does not clearly document Bring-Your-Own-Key (BYOK) key management options or the exact key management integration workflow. Organizations with strict KMS/BYOK requirements should request detailed security architecture and KMS integration docs from Codeium sales/enterprise support.","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: Expanded model access to GPT-5.2, Claude Opus 4.5, and Cognition SWE-1.5 agent model alongside proprietary LLMs. Cascade agentic feature added with deep codebase understanding and auto-fix on linter failures. WWT enterprise testing showed 30-50% efficiency gains for repetitive tasks. Industry shifting toward private self-hosted and fine-tuned LLMs for enterprise. Pricing: Teams $30/user/mo, Enterprise $45/user/mo.","plainChildren":""}]},{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://codeium.com/","plainChildren":""},{"type":"item","level":3,"content":"https://codeium.com/enterprise (vendor enterprise overview)","plainChildren":""}]},{"type":"text","content":"References & further reading"}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"codeium_enterprise.md"},{"type":"header","level":1,"content":"CodeLlama - https://ai.meta.com/blog/codegen-meta-code-llama/","children":[{"type":"text","content":"Code Llama is Meta's open-source family of large language models optimized for code generation, completion, and reasoning about code."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"text","content":"v1 (2023-08)"}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/meta-llama/codellama","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"[4] Strong open-source code generation model family with multiple sizes","plainChildren":""},{"type":"item","level":3,"content":"[3] Community ecosystem and tooling matured but still behind some proprietary offerings","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"text","content":"Code Llama is Meta's open-source family of LLMs optimized for code generation, completion, and debugging. It provides multiple model sizes (7B, 13B, 34B, 70B) and specialized variants (Instruct, Python) for different coding tasks and deployment constraints."}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Code Llama is Meta's open-source family of large language models optimized for code generation, completion, and reasoning about code. It ships in multiple sizes (7B, 13B, 34B and larger variants) and in specialized flavors such as Code Llama-Instruct (instruction-tuned) and Code Llama-Python (further fine-tuned on Python). The models use a decoder-only transformer architecture with optimizations tuned for code tasks and support fill-in-the-middle style completions and larger context windows than many older public models."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Variants: Code Llama-Instruct (better at following natural-language prompts) and Code Llama-Python (additional Python fine-tuning).","plainChildren":""},{"type":"item","level":3,"content":"Sizes: commonly available in 7B, 13B, 34B; larger checkpoints and tuned variants exist depending on releases.","plainChildren":""},{"type":"item","level":3,"content":"Context window: the official models are released with substantially larger context windows (commonly 16k tokens for code-focused variants); deployment runtimes and custom forks may offer extended context support.","plainChildren":""},{"type":"item","level":3,"content":"Deployment: Widely available through Hugging Face, community containers, and local runtimes (Ollama, private inference servers).","plainChildren":""},{"type":"item","level":3,"content":"Strengths: open-source, good quality for code tasks, multiple sizes for trade-offs between latency and capability.","plainChildren":""},{"type":"item","level":3,"content":"Limitations: still requires careful prompt engineering for complex multi-file project reasoning; ecosystem tooling (IDE/product integrations) is smaller than some commercial competitors but growing quickly.","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: No new Code Llama-specific releases. Meta shifted focus to Llama 4 (April 2025) with Scout (17B active/109B total, 10M context) and Maverick (17B active/400B total, 1M context) as natively multimodal models. Broader Llama family reached 1.2B downloads by April 2025 (1M/day). Code Llama 70B remains at 67.8% HumanEval with 100K context. Code Llama effectively superseded by Llama 4 multimodal models for new deployments.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Meta / Code Llama license (permits research and commercial use — see repository for exact terms)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"N/A","plainChildren":""},{"type":"item","level":3,"content":"Open-source model: weights freely available for download for research and commercial use under Meta's license terms, not a time-limited trial.","plainChildren":""}]}]}]}],"sourcePath":"codellama.md"},{"type":"header","level":1,"content":"Codestral - https://mistral.ai/products/codestral","children":[{"type":"text","content":"Mistral AI's specialized code-focused model family including Codestral for code completion and Devstral for agentic coding. Devstral (24B) outperforms models up to 671B parameters on SWE-Bench Verified with 128K-256K context support."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Codestral (July 2025) / Devstral 2 (2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://huggingface.co/mistralai","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"SWE-bench Verified: Outperforms DeepSeek V3 (671B) and Qwen3 (232B)","plainChildren":""},{"type":"item","level":3,"content":"Aider Polyglot: N/A","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Mistral's code-focused model family: Codestral for real-time code completion and Devstral for autonomous multi-step agentic coding workflows, with competitive SWE-bench performance at 24B parameters.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Codestral and Devstral are Mistral AI's specialized coding models released in 2025. Codestral is the premier code completion and generation model optimized for fill-in-the-middle completion and autocomplete workflows. Devstral, built on Mistral Small 3 with 24B parameters and 40 layers using grouped query attention, is designed for agentic coding -- autonomous multi-step software engineering workflows that interact with development environments, edit multiple files, and reason across entire codebases. Devstral supports 128K-256K token context and outperforms substantially larger models including DeepSeek V3 (671B) and Qwen3 (232B) on SWE-Bench Verified. Developed in partnership with All Hands AI (OpenHands), these models are part of Mistral Code, a comprehensive platform integrating Codestral, Codestral Embed, Devstral, and Mistral Medium with IDE support for VS Code and JetBrains."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Devstral: 24B parameters, 128K-256K context, built on Mistral Small 3","plainChildren":""},{"type":"item","level":3,"content":"Outperforms models 10-28x larger on SWE-Bench Verified","plainChildren":""},{"type":"item","level":3,"content":"Codestral: optimized for real-time autocomplete and fill-in-the-middle","plainChildren":""},{"type":"item","level":3,"content":"Devstral runs on a single RTX 4090 or Mac with 32GB RAM","plainChildren":""},{"type":"item","level":3,"content":"Co-developed with All Hands AI (OpenHands framework)","plainChildren":""},{"type":"item","level":3,"content":"Mistral Code platform integrates with VS Code and JetBrains IDEs","plainChildren":""},{"type":"item","level":3,"content":"Devstral API: $0.1/M input, $0.3/M output tokens","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Apache 2.0 (Devstral); Proprietary (Codestral)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"codestral.md"},{"type":"header","level":1,"content":"Composer 1 - https://cursor.com","children":[{"type":"text","content":"Cursor's frontier coding model released October 2025 as part of Cursor 2.0, achieving 4x faster generation than similarly intelligent models while maintaining frontier-level coding quality with reinforcement learning trained on real-world software engineering."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"1.0 (October 2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"4x faster than similarly intelligent models","plainChildren":""},{"type":"item","level":3,"content":"Comparable or superior quality to Claude Sonnet 4.5","plainChildren":""},{"type":"item","level":3,"content":"Less than half the tokens (200K vs 427K vs Claude Sonnet 4.5)","plainChildren":""},{"type":"item","level":3,"content":"Most coding turns complete in under 30 seconds","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Cursor's exclusive frontier coding model trained with reinforcement learning on real-world software engineering, optimized for 4x faster generation with production-grade tools integration and low-latency agentic workflows.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Composer 1 is a frontier coding model released in October 2025 as part of Cursor 2.0, specifically designed for agentic software engineering with exceptional speed optimization. The model's defining characteristic is achieving generation speeds 4x faster than similarly intelligent models while maintaining frontier-level coding results, with most coding turns completing in under 30 seconds. In direct comparison against Claude Sonnet 4.5, Composer 1 produces comparable or slightly superior coding quality while using less than half the tokens (200K vs 427K) and significantly less time. The model was trained using reinforcement learning on real-world software engineering challenges in large codebases, with access to production-grade tools including codebase-wide semantic search, code editing capabilities, string grepping functionality, and terminal command execution. To scale this training approach, Cursor implemented hundreds of thousands of concurrent sandboxed coding environments in the cloud, adapting infrastructure originally built for Background Agents and rewriting their virtual machine scheduler to handle high-volume, bursty training runs. Composer 1 is available exclusively within Cursor 2.0 and Cursor's CLI with no direct API access to external users, priced at the same level as GPT-5. The model demonstrates particular strength in multi-step coding tasks, large codebase navigation, and low-latency agent-based coding workflows. Cursor 2.0 introduces an agent-focused interface with three-panel layout displaying agent status, AI reasoning process, and generated code, enabling developers to manage multiple agents simultaneously and delegate coding tasks rather than writing code manually."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"4x faster: Generation speeds 4x faster than similarly intelligent models.","plainChildren":""},{"type":"item","level":3,"content":"Under 30 seconds: Most coding turns complete in under 30 seconds.","plainChildren":""},{"type":"item","level":3,"content":"Token efficient: Uses less than half the tokens vs Claude Sonnet 4.5 (200K vs 427K).","plainChildren":""},{"type":"item","level":3,"content":"Comparable quality: Matches or exceeds Claude Sonnet 4.5 coding quality at higher speed.","plainChildren":""},{"type":"item","level":3,"content":"Reinforcement learning: Trained on real-world software engineering challenges in large codebases.","plainChildren":""},{"type":"item","level":3,"content":"Production-grade tools: Trained with codebase-wide semantic search, code editing, grepping, terminal execution.","plainChildren":""},{"type":"item","level":3,"content":"Massive scale training: Hundreds of thousands of concurrent sandboxed coding environments in cloud.","plainChildren":""},{"type":"item","level":3,"content":"Custom infrastructure: Rewrote VM scheduler to handle high-volume, bursty training runs.","plainChildren":""},{"type":"item","level":3,"content":"Multi-step tasks: Early testers report confidence delegating complex problems to the model.","plainChildren":""},{"type":"item","level":3,"content":"Large codebase navigation: Specifically trained with semantic search for extensive codebase work.","plainChildren":""},{"type":"item","level":3,"content":"Low-latency agents: Optimized for agent-based coding with iterative solution building.","plainChildren":""},{"type":"item","level":3,"content":"Exclusive availability: Only available within Cursor 2.0 and Cursor CLI, no external API access.","plainChildren":""},{"type":"item","level":3,"content":"GPT-5 pricing: Priced at same level as GPT-5, competitively positioned.","plainChildren":""},{"type":"item","level":3,"content":"Agent-focused interface: Three-panel layout showing agent status, reasoning, and generated code.","plainChildren":""},{"type":"item","level":3,"content":"Multi-agent management: Developers can manage multiple agents simultaneously.","plainChildren":""},{"type":"item","level":3,"content":"Cursor 2.0 integration: Part of refreshed IDE interface focused on task delegation vs manual coding.","plainChildren":""},{"type":"item","level":3,"content":"Web browser integration: End-to-end code testing capabilities within Cursor 2.0.","plainChildren":""},{"type":"item","level":3,"content":"Multi-model support: Can run multiple models on same problem for comparison.","plainChildren":""},{"type":"item","level":3,"content":"Practical testing: Completed AI agent from scratch in ~3 minutes in real-world tests.","plainChildren":""},{"type":"item","level":3,"content":"Minor limitations: Occasional issues with specific integrations (e.g., YouTube transcript functions).","plainChildren":""},{"type":"item","level":3,"content":"Released October 2025 as part of Cursor 2.0 launch.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-02-06","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""},{"type":"item","level":3,"content":"Available through Cursor 2.0","plainChildren":""}]}]}]}],"sourcePath":"composer-1.md"},{"type":"header","level":1,"content":"DeepSeek R1 - https://www.deepseek.com/","children":[{"type":"text","content":"DeepSeek's open-source reasoning model matching OpenAI o1 performance. Released January 2025, achieving 90.8% on MMLU, 71.0% on AIME, and 71.5% on GPQA Diamond through novel reinforcement learning training without supervised fine-tuning."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"DeepSeek-R1 (January 2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/deepseek-ai/DeepSeek-R1","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"MMLU: 90.8%","plainChildren":""},{"type":"item","level":3,"content":"AIME: 71.0%","plainChildren":""},{"type":"item","level":3,"content":"GPQA Diamond: 71.5%","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Open-source reasoning model using MoE architecture and reinforcement learning, matching OpenAI o1 on reasoning tasks with self-verification and chain-of-thought capabilities.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"DeepSeek-R1 is an open-source reasoning model released January 2025 that achieves performance comparable to OpenAI's o1 on reasoning benchmarks. Built on the DeepSeek-V3 base model with a Mixture of Experts architecture, it uses a distinctive two-stage training approach combining supervised fine-tuning on Chain-of-Thought examples followed by iterative reinforcement learning phases. The RL framework encourages autonomous emergence of chain-of-thought reasoning, self-verification, and error correction. DeepSeek-R1-Zero, a precursor model, demonstrated that reasoning capabilities can develop purely through reinforcement learning without any supervised data. The model scores 90.8% on MMLU, 84.0% on MMLU-Pro, 71.5% on GPQA Diamond, and improved from 15.6% to 71.0% on AIME. Distilled variants built on Qwen and Llama base models are available for resource-constrained deployments."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"MoE architecture built on DeepSeek-V3 base model","plainChildren":""},{"type":"item","level":3,"content":"Two-stage training: supervised fine-tuning + reinforcement learning","plainChildren":""},{"type":"item","level":3,"content":"Self-evolving reasoning: RL produces chain-of-thought and self-verification autonomously","plainChildren":""},{"type":"item","level":3,"content":"Distilled variants available (e.g., R1-Distill-Qwen-7B scores 55.5% on AIME 2024)","plainChildren":""},{"type":"item","level":3,"content":"Strongest in coding, mathematics, and discrete reasoning tasks","plainChildren":""},{"type":"item","level":3,"content":"Full 22-page research paper published detailing training methodology","plainChildren":""},{"type":"item","level":3,"content":"Signaled China's growing influence in open-source LLM ecosystem","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"MIT","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"deepseek-r1.md"},{"type":"header","level":1,"content":"DeepSeek V3 - https://github.com/deepseek-ai/DeepSeek-V3","children":[{"type":"text","content":"Advanced 671B parameter Mixture-of-Experts model excelling in mathematics and coding. V3.1 and V3.2 updates in 2025 added enhanced reasoning and sparse attention for long contexts."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"V3.2 / V3.2-Speciale (December 1, 2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/deepseek-ai/DeepSeek-V3","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIME: 96% accuracy (V3.2-Speciale)","plainChildren":""},{"type":"item","level":3,"content":"IMO 2025: Gold medal","plainChildren":""},{"type":"item","level":3,"content":"ICPC World Finals 2025: Gold medal","plainChildren":""},{"type":"item","level":3,"content":"IOI 2025: Gold medal","plainChildren":""},{"type":"item","level":3,"content":"GPT-5 level performance (V3.2)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"671B parameter MoE model (37B activated) with exceptional math and coding performance, featuring V3.1 hybrid reasoning and V3.2 sparse attention for 128K contexts.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"DeepSeek-V3 is a 671-billion parameter Mixture-of-Experts model that achieves state-of-the-art performance in mathematics and coding tasks. The model family includes specialized variants: V3 (December 2024 baseline), V3.1 (August 2025) which combines the strengths of V3 and R1 into a hybrid model with enhanced reasoning, and V3.2-Exp (September 2025) featuring DeepSeek Sparse Attention (DSA) for improved long-context handling up to 128K tokens. Available through GitHub Models and various deployment platforms, DeepSeek V3 is particularly suited for solving advanced math problems and generating complex code, making it a top choice for technical and scientific applications."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"671B total parameters: Massive MoE architecture with 37B activated parameters.","plainChildren":""},{"type":"item","level":3,"content":"Exceptional math and coding: Best performance on most technical benchmarks.","plainChildren":""},{"type":"item","level":3,"content":"V3.1 hybrid model (August 2025): Combines V3 and R1 strengths with enhanced thinking mode for coding and math.","plainChildren":""},{"type":"item","level":3,"content":"V3.2-Exp (September 2025): Experimental version with DeepSeek Sparse Attention for long contexts.","plainChildren":""},{"type":"item","level":3,"content":"128K context length: Supports extended context windows for complex tasks.","plainChildren":""},{"type":"item","level":3,"content":"GitHub Models integration: Now generally available in GitHub Models platform.","plainChildren":""},{"type":"item","level":3,"content":"DeepSeek-Coder series: Separate specialized line (1B to 33B) trained on 87% code, 13% natural language.","plainChildren":""},{"type":"item","level":3,"content":"2T token training: DeepSeek-Coder models pre-trained on 2 trillion tokens.","plainChildren":""},{"type":"item","level":3,"content":"Active development: Continuous improvements with V3.1 and V3.2 releases in 2025.","plainChildren":""},{"type":"item","level":3,"content":"Open source: Available on GitHub with full model weights and documentation.","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: V3-0324 (March 2025) with enhanced reasoning (+19.8 AIME, +10 LiveCodeBench). V3.1-Terminus (September 2025) introduced hybrid reasoning architecture (thinking/non-thinking modes). V3.2-Exp added DeepSeek Sparse Attention (DSA) with 50%+ API price reduction. V3.2 (December 1, 2025) reached GPT-5 level with Thinking in Tool-Use capability. V3.2-Speciale rivals Gemini-3.0-Pro, won gold at IMO/ICPC/IOI 2025. Available across web, mobile, and API platforms.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"MIT","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"deepseek-v3.md"},{"type":"header","level":1,"content":"FauxPilot - https://github.com/fauxpilot/fauxpilot","children":[{"type":"text","content":"[Open-source, locally-hosted code-completion server that provides a privacy-focused alternative to cloud-based assistants like GitHub Copilot.]"},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Infrastructure","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No formal releases (last commit: April 9, 2024)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/fauxpilot/fauxpilot","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No formal benchmarks; community-driven project for local code completion","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Open-source, locally-hosted code-completion server providing a privacy-focused alternative to cloud-based assistants like GitHub Copilot.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"FauxPilot is an open-source code-completion server designed to run on-premises or on private infrastructure so that source code and telemetry do not need to be sent to a third-party cloud service. It provides an OpenAI-compatible API surface and integrations that let editors and tools use it in place of cloud assistants. The project is focused on privacy, local deployment, and model flexibility: it supports running models (notably Salesforce CodeGen variants) inside NVIDIA's Triton Inference Server with the FasterTransformer backend and can split large models across multiple GPUs."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Model support: commonly used with Salesforce CodeGen models converted for FasterTransformer / Triton; models are typically downloaded from Hugging Face and converted during setup.","plainChildren":""},{"type":"item","level":3,"content":"Hardware: requires an NVIDIA GPU (compute capability >= 6.0) and sufficient VRAM for the chosen model. VRAM can be aggregated across multiple GPUs for larger models.","plainChildren":""},{"type":"item","level":3,"content":"Installation: requires Docker, docker-compose (>= 1.28), nvidia-docker (nvidia-container-toolkit), curl and zstd for model download/extraction. A setup script helps choose and prepare a model.","plainChildren":""},{"type":"item","level":3,"content":"Integrations: offers OpenAI API compatibility, REST endpoints, and Copilot-plugin style integrations so it can be used with existing editor tooling.","plainChildren":""},{"type":"item","level":3,"content":"Privacy: primary selling point is that all inference can be run locally so developer code does not leave the network and no external telemetry is required.","plainChildren":""},{"type":"item","level":3,"content":"Support: community-driven project; documentation is community-maintained (wiki, discussion forum). There is no formal commercial support or warranty.","plainChildren":""},{"type":"item","level":3,"content":"Common pitfalls: accurate VRAM estimation is critical; ensure nvidia-docker and drivers are correctly installed and that the chosen model fits available GPU memory (or configure model sharding across GPUs).","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: No new releases or significant updates. Project appears largely inactive since April 2024. Still listed among GitHub Copilot alternatives in 2025 for privacy-focused local deployments. The broader AI code completion landscape has evolved significantly with GitHub Copilot becoming multi-model agentic assistant, VS Code Copilot adding parallel subagent execution (January 2026). FauxPilot remains viable for organizations requiring fully on-premise code completion with no external dependencies.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"MIT","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"N/A","plainChildren":""},{"type":"item","level":3,"content":"Open-source software: freely available under MIT license, not a time-limited trial.","plainChildren":""}]}]}]}],"sourcePath":"fauxpilot.md"},{"type":"header","level":1,"content":"Gemini 2.5 - https://ai.google.dev/gemini-api/docs/models","children":[{"type":"text","content":"Google's most advanced model family with thinking capabilities, featuring exceptional coding performance. Released March 2025 with 2.5 Pro achieving 63.8% on SWE-bench Verified and leading LiveCodeBench."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2.5 (2025-03)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"SWE-bench Verified: 63.8% (2.5 Pro)","plainChildren":""},{"type":"item","level":3,"content":"WebDev Arena: World-leading performance","plainChildren":""},{"type":"item","level":3,"content":"LMArena: Leaderboard leader","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Google's advanced thinking model family with 2.5 Pro for complex coding tasks and 2.5 Flash for efficient workhorse operations, featuring 1M token context and native tool use.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Gemini 2.5 is Google DeepMind's most advanced model family announced in March 2025, featuring thinking models capable of reasoning through their thoughts before responding. The flagship 2.5 Pro excels at creating visually compelling web apps, agentic code applications, and code transformation/editing, achieving 63.8% on SWE-bench Verified and leading LiveCodeBench for competition-level coding. Gemini 2.5 Flash is the most efficient workhorse model, improved across key benchmarks for reasoning, multimodality, code, and long context while using 20-30% fewer tokens. Both models feature superior speed, native tool use, and 1M token context windows. Building on Gemini 2.0 Flash (February 2025 general availability), the 2.5 family represents Google's commitment to developer-focused coding capabilities."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Exceptional coding: 2.5 Pro achieves 63.8% on SWE-bench Verified, leads LiveCodeBench for competition coding.","plainChildren":""},{"type":"item","level":3,"content":"Thinking models: Capable of reasoning through thoughts before responding for enhanced accuracy.","plainChildren":""},{"type":"item","level":3,"content":"2.5 Pro: Most advanced for complex tasks, web app creation, and agentic code applications.","plainChildren":""},{"type":"item","level":3,"content":"2.5 Flash: Most efficient workhorse, 20-30% fewer tokens while improving benchmarks.","plainChildren":""},{"type":"item","level":3,"content":"1M token context: Extended context window for handling large codebases and documents.","plainChildren":""},{"type":"item","level":3,"content":"Native tool use: Built-in capabilities for using tools and APIs directly.","plainChildren":""},{"type":"item","level":3,"content":"Multimodal: Strong visual perception and image analysis alongside code capabilities.","plainChildren":""},{"type":"item","level":3,"content":"Code transformation: Excels at editing and transforming existing codebases.","plainChildren":""},{"type":"item","level":3,"content":"Real-time generation: Direct generation of HTML, CSS, JS within conversations.","plainChildren":""},{"type":"item","level":3,"content":"Google DeepMind: Developed by Google's premier AI research division.","plainChildren":""},{"type":"item","level":3,"content":"Released March 2025 with continuous improvements through 2025.","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: 2.5 Pro and Flash reached general availability June 26, 2025. Flash became default model in Gemini app. Flash-Lite cost-optimized variant introduced June 2025. Deep Think experimental enhanced reasoning mode added. Native audio output support added for natural conversations. Computer Use Preview launched October 2025. Flash image generation/editing GA October 2, 2025. MCP tool support added to API/SDK. Gemini 1.5 models retired September 29, 2025. Gemini 2.0 models retiring March 3, 2026. Succeeded by Gemini 3 (November 2025).","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"gemini-2-5.md"},{"type":"header","level":1,"content":"Gemini 3 - https://blog.google/technology/google-labs/","children":[{"type":"text","content":"Gemini 3 is Google's latest and most advanced foundation model, released November 18, 2025, featuring state-of-the-art reasoning capabilities, record benchmark scores, and described as Google's \"best vibe coding model ever.\"\n\n**Dataset ID:** aie-model"},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"3.0 (November 18, 2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"SWE-bench Verified: 76.2%","plainChildren":""},{"type":"item","level":3,"content":"Terminal-Bench 2.0: 54.2%","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"text","content":"Google's most advanced foundation model with state-of-the-art coding and reasoning capabilities, record benchmark performance, and multimodal vibe coding features."}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Gemini 3 is Google's latest foundation model, released on November 18, 2025, representing a significant advancement in AI capabilities. The model features enhanced reasoning, agentic workflows, and exceptional coding performance, surpassing previous versions across all major benchmarks.\n\nGemini 3 Pro achieves 76.2% on SWE-bench Verified, a benchmark measuring coding agents' capabilities, greatly outperforming Gemini 2.5 Pro. On Terminal-Bench 2.0, which tests a model's tool use ability to operate a computer via terminal, Gemini 3 scores 54.2%.\n\nThe model introduces \"multimodal vibe coding,\" allowing users to visually instruct the model to make changes in code. This capability is available through multiple interfaces including AI Studio, Vertex AI, the Gemini Command Line Interface tool, and third-party developer tools such as Cursor, GitHub, JetBrains, Replit, and Manus.\n\nAlongside the base model, Google released Google Antigravity, a Gemini-powered coding interface allowing multi-pane agentic coding similar to agentic IDEs like Warp or Cursor 2.0. The public preview is available at no charge for macOS, Windows, and Linux.\n\nGemini 3 is now immediately available through the Gemini app and AI search interface, with access worldwide where Gemini models are available."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Release: November 18, 2025","plainChildren":""},{"type":"item","level":3,"content":"Benchmarks:","plainChildren":""},{"type":"item","level":3,"content":"SWE-bench Verified: 76.2%","plainChildren":""},{"type":"item","level":3,"content":"Terminal-Bench 2.0: 54.2%","plainChildren":""},{"type":"item","level":3,"content":"Key Feature: Multimodal vibe coding (visual code instruction)","plainChildren":""},{"type":"item","level":3,"content":"Google Antigravity: Multi-pane agentic coding interface (free public preview)","plainChildren":""},{"type":"item","level":3,"content":"Platforms: AI Studio, Vertex AI, Gemini CLI, third-party tools","plainChildren":""},{"type":"item","level":3,"content":"Integrations: Cursor, GitHub, JetBrains, Replit, Manus","plainChildren":""},{"type":"item","level":3,"content":"Availability: Worldwide where Gemini models are accessible","plainChildren":""},{"type":"item","level":3,"content":"Developer: Google DeepMind","plainChildren":""},{"type":"item","level":3,"content":"Special Capabilities: Advanced reasoning, agentic workflows, generative UI responses","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: Gemini 3 Flash released December 17, 2025 as new default model with PhD-level reasoning. Gemini 3 Deep Think available December 4, 2025 for AI Ultra subscribers. Gemini 3 Ultra premium tier for enterprise/mission-critical workloads. Gemini 3 launched in Google Search AI Mode day one (120+ countries). Integrated into Google Maps with hands-free conversational driving and landmark-based navigation. Rolling out in Android Auto (250M+ cars). Nano Banana Pro image model built on Gemini 3. Configurable Deep Think mode explores multiple hypotheses simultaneously. 1M token context with caching. Native multimodal handling of text/images/audio/video/code in single model.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""},{"type":"item","level":3,"content":"Available through Gemini app and AI Studio","plainChildren":""},{"type":"item","level":3,"content":"Google Antigravity available at no charge in public preview","plainChildren":""}]}]}]}],"sourcePath":"gemini-3.md"},{"type":"header","level":1,"content":"GLM-4.5 - https://huggingface.co/zai-org/GLM-4.5","children":[{"type":"text","content":"Open-source model family from Zhipu AI (Z.ai) unifying reasoning, coding, agentic abilities, and vision. Released August 2025, achieving 64.2% on SWE-bench Verified, surpassing GPT-4.1 and Claude 4 Opus."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"4.5 (2025-08)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://huggingface.co/zai-org/GLM-4.5","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"SWE-bench Verified: 64.2%","plainChildren":""},{"type":"item","level":3,"content":"TerminalBench: 37.5%","plainChildren":""},{"type":"item","level":3,"content":"Tool-Calling: 90.6% success rate","plainChildren":""},{"type":"item","level":3,"content":"3rd on combined 12-benchmark evaluation (trailing only top OpenAI and Anthropic models)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Open-source MoE model family with 355B total parameters (32B active) featuring exceptional coding performance, achieving 64.2% on SWE-bench and 80.8% win rate against Qwen3 Coder in real-world challenges.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"GLM-4.5 is an open-source large language model series from Zhipu AI (Z.ai) designed to unify advanced reasoning, coding, agentic abilities, and vision in a single powerful framework. Released in August 2025, the flagship GLM-4.5 model features 355 billion total parameters with 32 billion active parameters, while GLM-4.5-Air offers a more compact design with 106 billion total parameters and 12 billion active parameters. The series demonstrates exceptional coding performance with 64.2% on SWE-bench Verified, surpassing GPT-4.1 (48.6%) and achieving an 80.8% win rate against Qwen3 Coder in real-world coding challenges. The model runs efficiently on just eight Nvidia H20 GPUs—half the hardware of comparable models. Available variants include GLM-4-Flash, GLM-4-FlashX, GLM-4-Plus, GLM-4-Long, GLM-4-Air, and GLM-4-AirX, all MIT-licensed for commercial use."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Exceptional coding performance: 64.2% on SWE-bench Verified, 37.5% on TerminalBench, beats GPT-4.1 and Claude 4 Opus.","plainChildren":""},{"type":"item","level":3,"content":"Real-world coding wins: 80.8% win rate against Qwen3 Coder in practical coding challenges.","plainChildren":""},{"type":"item","level":3,"content":"MoE architecture: GLM-4.5 has 355B total parameters (32B active), GLM-4.5-Air has 106B total (12B active).","plainChildren":""},{"type":"item","level":3,"content":"Hardware efficient: Runs on just 8x Nvidia H20 GPUs, half the requirement of comparable models.","plainChildren":""},{"type":"item","level":3,"content":"Real-time code generation: Direct generation of HTML, CSS, JS, and SVG within conversations.","plainChildren":""},{"type":"item","level":3,"content":"Agent capabilities: GLM-4-32B variant enhanced for tool usage, web search, and code generation.","plainChildren":""},{"type":"item","level":3,"content":"Multiple variants: Flash, FlashX, Plus, Long, Air, and AirX versions for different use cases.","plainChildren":""},{"type":"item","level":3,"content":"Free tier available: GLM-4-Flash and GLM-4.5-Flash optimized for free coding and reasoning.","plainChildren":""},{"type":"item","level":3,"content":"MIT licensed: Open source with full commercial use rights, self-hosting, and custom training.","plainChildren":""},{"type":"item","level":3,"content":"Z.ai/Zhipu AI: Developed by Chinese AI startup, CEO Zhang Peng.","plainChildren":""},{"type":"item","level":3,"content":"Released August 2025 with strong benchmark results.","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: GLM-4.6 released ~2 months later with #1 coding ranking alongside Anthropic/OpenAI. GLM-4.7 released December 2025 with coding comparable to Claude Sonnet 4.5, topped open-source and domestic rankings in Artificial Analysis. Context expanded to 200K tokens. Interleaved thinking and preserved thinking features added. GLM-5 announced. Zhipu MaaS ARR grew 25x to 500M+ in 10 months. Training used 22T-token corpus with 7T tokens for code/reasoning. Dual-mode operation: thinking mode for complex tasks, non-thinking for speed.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"MIT","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"glm-4-5.md"},{"type":"header","level":1,"content":"GLM-4.6 - https://huggingface.co/zai-org/GLM-4.6","children":[{"type":"text","content":"GLM-4.6 is Zhipu AI's advanced coding model featuring a 355B-parameter Mixture of Experts (MoE) architecture with enhanced real-world coding, long-context processing, reasoning, and agentic AI capabilities.\n\n**Dataset ID:** aie-model"},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"4.6 (September 30, 2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://huggingface.co/zai-org/GLM-4.6","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"LiveCodeBench v6: 82.8% (up from 63.3% in GLM-4.5)","plainChildren":""},{"type":"item","level":3,"content":"CC-Bench: 48.6% win rate vs Claude Sonnet 4 (near-parity)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"text","content":"Open-weight 355B-parameter MoE model from Zhipu AI with enhanced coding capabilities, 200K context window, and competitive performance against Claude Sonnet 4."}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"GLM-4.6 was released by Zhipu AI on September 30, 2025, as the latest iteration in the GLM series. The model features a 355B-parameter Mixture of Experts (MoE) architecture designed with a focus on advanced agentic capabilities, reasoning, and coding performance.\n\nThe model shows significant improvements over GLM-4.5 across eight public benchmarks, with 15% token efficiency improvements and enhanced real-world coding performance. The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks and larger codebases.\n\nOn LiveCodeBench v6 (which involves writing, executing, and debugging code across languages), GLM-4.6 achieved 82.8%, a substantial jump from GLM-4.5's 63.3%. In real-world coding tests (CC-Bench), it achieves near-parity with Claude Sonnet 4 (48.6% win rate). However, Zhipu AI acknowledges that it still lags behind Claude Sonnet 4.5 in coding ability.\n\nGLM-4.6 is available via Z.ai API and OpenRouter, integrates with popular coding agents (Claude Code, Cline, Roo Code, Kilo Code), and supports local serving via vLLM and SGLang. Zhipu AI offers GLM Coding Plan, a subscription package specifically designed for AI-powered coding, available for $3 per month. As of late 2025, it is the leading open-source solution for coding tasks."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Architecture: 355B-parameter MoE with BF16/F32 tensors","plainChildren":""},{"type":"item","level":3,"content":"Context Window: 200K tokens (expanded from 128K in GLM-4.5)","plainChildren":""},{"type":"item","level":3,"content":"License: MIT (open-weight model)","plainChildren":""},{"type":"item","level":3,"content":"Performance Improvements: 15% token efficiency, 82.8% LiveCodeBench v6","plainChildren":""},{"type":"item","level":3,"content":"Benchmarks: Near-parity with Claude Sonnet 4, behind Sonnet 4.5","plainChildren":""},{"type":"item","level":3,"content":"Pricing: GLM Coding Plan at $3/month","plainChildren":""},{"type":"item","level":3,"content":"Availability: Z.ai API, OpenRouter, Hugging Face, ModelScope","plainChildren":""},{"type":"item","level":3,"content":"Inference: vLLM and SGLang support for local serving","plainChildren":""},{"type":"item","level":3,"content":"Integration: Claude Code, Cline, Roo Code, Kilo Code","plainChildren":""},{"type":"item","level":3,"content":"Developer: Zhipu AI","plainChildren":""},{"type":"item","level":3,"content":"Release: September 30, 2025","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: GLM-4.6V multimodal variant released for image/video/document understanding and GUI agent operations. GLM-4.7 launched December 2025, topping open-source and domestic model rankings. UI2Code^N RL-enhanced UI coding model unveiled November 2025. GLM-5 announced as upcoming flagship. Zhipu AI MaaS ARR grew 25x in 10 months to over 500M.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""},{"type":"item","level":3,"content":"Open-weight model","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"MIT","plainChildren":""}]}]}]}],"sourcePath":"glm-4-6.md"},{"type":"header","level":1,"content":"GLM-4.7 - https://bigmodel.cn/","children":[{"type":"text","content":"Zhipu AI's flagship large language model with 355B total / 32B active MoE architecture. Released December 2025, achieving 73.8% on SWE-bench and 66.7% on SWE-bench Multilingual with 200K token context and agentic coding capabilities."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"GLM-4.7 (December 2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/THUDM/GLM-4","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"SWE-bench: 73.8%","plainChildren":""},{"type":"item","level":3,"content":"SWE-bench Multilingual: 66.7%","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Zhipu AI's 355B/32B active MoE model with top-tier coding benchmarks, 200K context, Chain-of-Thought reasoning, and native English-Chinese bilingual support.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"GLM-4.7 is Zhipu AI's latest-generation flagship large language model, released December 2025, utilizing a Mixture of Experts architecture with 355B total parameters and 32B activated parameters per token. The model supports a 200K-205K token context window with up to 128K output tokens, enabling processing of entire codebases and generating comprehensive multi-file software modules in a single response. GLM-4.7 excels at agentic coding, decomposing high-level requirements into executable steps and handling complete task lifecycles. It achieves 73.8% on SWE-bench (5.8% improvement over GLM-4.6) and 66.7% on SWE-bench Multilingual (nearly 13% gain). The model is natively bilingual in English and Chinese with Chain of Thought reasoning for transparent problem-solving. A Flash variant is available optimized for low-latency inference."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"MoE architecture: 355B total / 32B active parameters","plainChildren":""},{"type":"item","level":3,"content":"200K token context window with 128K output capability","plainChildren":""},{"type":"item","level":3,"content":"Agentic coding: decomposes requirements into executable multi-step workflows","plainChildren":""},{"type":"item","level":3,"content":"Natively bilingual English-Chinese with strong cultural nuance handling","plainChildren":""},{"type":"item","level":3,"content":"GLM-4.7-Flash variant for low-latency inference","plainChildren":""},{"type":"item","level":3,"content":"API pricing: $0.42/M input tokens, $2.2/M output tokens","plainChildren":""},{"type":"item","level":3,"content":"Top open-source ranking on SWE-bench-Verified","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Apache 2.0","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"glm-4-7.md"},{"type":"header","level":1,"content":"GPT-5.2 Codex - https://openai.com/codex","children":[{"type":"text","content":"OpenAI's most advanced agentic coding model released December 18, 2025, designed for professional software engineering and defensive cybersecurity with enhanced long-context understanding, vision capabilities, and state-of-the-art performance on SWE-Bench Pro and Terminal-Bench 2.0."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"5.2-Codex (2025-12-18)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"SWE-Bench Pro: State-of-the-art performance","plainChildren":""},{"type":"item","level":3,"content":"Terminal-Bench 2.0: State-of-the-art performance","plainChildren":""},{"type":"item","level":3,"content":"Cybersecurity: Dramatic capability improvements across versions","plainChildren":""},{"type":"item","level":3,"content":"Long-context: Near 100% accuracy on deep document analysis","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"OpenAI's flagship agentic coding model combining GPT-5.2 and GPT-5.1-Codex-Max capabilities with enhanced long-horizon work, native context compaction, improved tool calling, and specialized defensive cybersecurity features.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"GPT-5.2 Codex is OpenAI's most advanced agentic coding model released on December 18, 2025, designed specifically for professional software engineering and defensive cybersecurity work. The model combines improved long-context understanding, reliable tool execution, and specialized security capabilities into a single powerful system. GPT-5.2 Codex excels at extended coding sessions in large repositories, maintaining full context integrity across complex tasks and reliably completing long-running work such as large refactors, code migrations, and feature builds—continuing to iterate without losing track even when plans change or attempts fail. The model achieves superior long-context understanding through native context compaction, making it more token-efficient in its reasoning while handling substantially more information, with the underlying GPT-5.2 demonstrating near 100% accuracy on deep document analysis tasks spanning hundreds of thousands of tokens. Enhanced vision performance enables accurate interpretation of screenshots, technical diagrams, charts, and UI surfaces, translating design mockups into functional prototypes. GPT-5.2 Codex delivers significantly improved performance in native Windows environments and provides improved reliability in tool calling with stronger performance on complex coding workflows. A distinguishing characteristic is enhanced cybersecurity capabilities including specialized training for defensive cybersecurity work, product-level safeguards with sandboxing and configurable network access, and invite-only trusted access for vetted professionals focused on defensive cybersecurity research. The model achieves state-of-the-art results on SWE-Bench Pro and Terminal-Bench 2.0, demonstrating dramatic performance improvements across successive versions on cybersecurity evaluations."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Long-horizon excellence: Excels at extended coding sessions in large repositories with full context integrity.","plainChildren":""},{"type":"item","level":3,"content":"Native context compaction: Superior long-context understanding with token-efficient reasoning.","plainChildren":""},{"type":"item","level":3,"content":"Near 100% accuracy: On deep document analysis tasks spanning hundreds of thousands of tokens.","plainChildren":""},{"type":"item","level":3,"content":"State-of-the-art benchmarks: Leading performance on SWE-Bench Pro and Terminal-Bench 2.0.","plainChildren":""},{"type":"item","level":3,"content":"Reliable tool calling: Improved reliability and stronger performance on complex coding workflows.","plainChildren":""},{"type":"item","level":3,"content":"Enhanced vision: Accurately interprets screenshots, diagrams, charts, UI surfaces, design mockups.","plainChildren":""},{"type":"item","level":3,"content":"Windows optimization: Significantly improved performance in native Windows environments.","plainChildren":""},{"type":"item","level":3,"content":"Defensive cybersecurity: Specialized training for professional security work and research.","plainChildren":""},{"type":"item","level":3,"content":"Security safeguards: Product-level sandboxing and configurable network access for safe agentic tasks.","plainChildren":""},{"type":"item","level":3,"content":"Invite-only access: Trusted access for vetted professionals focused on defensive cybersecurity.","plainChildren":""},{"type":"item","level":3,"content":"Complex refactoring: Manages code transformations across extended sessions without losing context.","plainChildren":""},{"type":"item","level":3,"content":"Code migrations: Handles intricate dependency updates and structural changes reliably.","plainChildren":""},{"type":"item","level":3,"content":"Feature development: Builds new functionality while maintaining context across large codebases.","plainChildren":""},{"type":"item","level":3,"content":"Design-to-code: Converts design mockups and UI specifications into functional implementations.","plainChildren":""},{"type":"item","level":3,"content":"Long-running sessions: Executes complex command sequences with reliable automation.","plainChildren":""},{"type":"item","level":3,"content":"Iterative resilience: Continues working effectively even when plans change or attempts fail.","plainChildren":""},{"type":"item","level":3,"content":"Dramatic security trajectory: Sharp performance jumps from GPT-5-Codex to 5.1-Codex-Max to 5.2-Codex.","plainChildren":""},{"type":"item","level":3,"content":"API model ID: gpt-5-2-codex","plainChildren":""},{"type":"item","level":3,"content":"Released December 18, 2025: Available across paid ChatGPT plans (Plus, Pro, Go, Business, Enterprise).","plainChildren":""},{"type":"item","level":3,"content":"API rollout: Broader API access rolled out in weeks following ChatGPT launch.","plainChildren":""},{"type":"item","level":3,"content":"Integration: Available through ChatGPT Enterprise, Codex CLI, and IDE integrations.","plainChildren":""},{"type":"item","level":3,"content":"Evolution: Successor to GPT-5.2 Codex, preceded GPT-5.3 Codex (February 2026).","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-02-06","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"gpt-5-2-codex.md"},{"type":"header","level":1,"content":"GPT-5.3 Codex - https://openai.com/codex","children":[{"type":"text","content":"OpenAI's most capable agentic coding model released February 5, 2026, featuring interactive supervision, 25% faster performance, and state-of-the-art results on SWE-Bench Pro, Terminal-Bench 2.0, OSWorld, and GDPval benchmarks."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"5.3-Codex (2026-02-05)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"SWE-Bench Pro: New industry high","plainChildren":""},{"type":"item","level":3,"content":"Terminal-Bench 2.0: New industry high","plainChildren":""},{"type":"item","level":3,"content":"OSWorld: Strong performance","plainChildren":""},{"type":"item","level":3,"content":"GDPval: Strong performance","plainChildren":""},{"type":"item","level":3,"content":"Cybersecurity: High capability classification","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"OpenAI's most advanced agentic coding model combining frontier coding performance with enhanced reasoning, achieving state-of-the-art benchmarks while operating 25% faster with fewer tokens than any prior model.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"GPT-5.3 Codex is OpenAI's most capable agentic coding model released on February 5, 2026, purpose-built for agent-style development workflows where the model can use tools, operate a computer, and complete longer tasks end-to-end. The model combines frontier coding performance with advanced reasoning and professional knowledge capabilities, enabling it to handle long-running tasks involving research, tool use, and complex execution. A defining feature is interactive supervision—users can steer and direct the model while it's working without losing context, collaborating like with a colleague. GPT-5.3 Codex provides frequent updates and progress reports, making it easier to manage and direct multiple agents working in parallel. The model operates 25% faster than its predecessor through infrastructure and inference stack improvements, and achieves state-of-the-art performance on multiple benchmarks including new industry highs on SWE-Bench Pro and Terminal-Bench 2.0. Remarkably, early versions of GPT-5.3 Codex helped create itself—the Codex team used the model to debug training, manage deployment, and diagnose evaluations. The model progressed from writing and reviewing code to executing nearly anything developers and professionals can accomplish on a computer, including frontend work and real-world computer-use tasks. Co-designed for, trained with, and served on NVIDIA GB200 NVL72 systems, representing OpenAI's first model classified as \"High capability\" for cybersecurity tasks under their Preparedness Framework."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Most capable agentic model: Purpose-built for agent-style development with tool use and computer operation.","plainChildren":""},{"type":"item","level":3,"content":"Interactive supervision: Real-time collaboration—ask questions, discuss approaches, steer mid-task without losing context.","plainChildren":""},{"type":"item","level":3,"content":"25% faster: Significant performance improvement through infrastructure and inference optimization.","plainChildren":""},{"type":"item","level":3,"content":"SWE-Bench Pro leader: New industry high on rigorous evaluation spanning four languages (more contamination-resistant than Verified).","plainChildren":""},{"type":"item","level":3,"content":"Terminal-Bench 2.0 leader: Far exceeds previous state-of-the-art on terminal skills for coding agents.","plainChildren":""},{"type":"item","level":3,"content":"Token efficiency: Achieves state-of-the-art results with fewer tokens than any prior model.","plainChildren":""},{"type":"item","level":3,"content":"Self-improvement: Early versions helped create itself—debugging training, managing deployment, diagnosing evaluations.","plainChildren":""},{"type":"item","level":3,"content":"Frequent updates: Provides progress reports and key decision updates throughout execution.","plainChildren":""},{"type":"item","level":3,"content":"High cybersecurity capability: First OpenAI model classified as \"High capability\" for cybersecurity under Preparedness Framework.","plainChildren":""},{"type":"item","level":3,"content":"NVIDIA GB200 optimized: Co-designed for and served on NVIDIA GB200 NVL72 systems.","plainChildren":""},{"type":"item","level":3,"content":"Broad scope: Evolved from code writing/review to executing nearly anything developers can do on computers.","plainChildren":""},{"type":"item","level":3,"content":"Real-world tasks: Strong performance on OSWorld and GDPval benchmarks measuring practical capabilities.","plainChildren":""},{"type":"item","level":3,"content":"API model ID: gpt-5.3-codex","plainChildren":""},{"type":"item","level":3,"content":"API status: Coming soon—not yet generally available, being prepared for safe rollout.","plainChildren":""},{"type":"item","level":3,"content":"Platform access: Available via ChatGPT app, Codex CLI, IDE extensions, and web interface on paid plans.","plainChildren":""},{"type":"item","level":3,"content":"Configuration: Specify in config.toml with model = \"gpt-5.3-codex\" or use /model command in CLI.","plainChildren":""},{"type":"item","level":3,"content":"Follow-up behavior: Enable steering in Settings > General > Follow-up behavior.","plainChildren":""},{"type":"item","level":3,"content":"Production-ready output: Automatically generates more polished, feature-complete results (e.g., auto-transitioning carousels, optimized pricing displays).","plainChildren":""},{"type":"item","level":3,"content":"Security ecosystem: Expansion of Aardvark security research agent and partnerships for open-source codebase scanning.","plainChildren":""},{"type":"item","level":3,"content":"Real-world impact: Used internally to optimize infrastructure, identify bugs, debug cache issues, dynamically scale GPU clusters.","plainChildren":""},{"type":"item","level":3,"content":"Released February 5, 2026 as OpenAI's most advanced agentic coding model.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-02-06","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"gpt-5-3-codex.md"},{"type":"header","level":1,"content":"GPT-5 - https://openai.com/index/introducing-gpt-5/","children":[{"type":"text","content":"OpenAI's most advanced model described as \"PhD-level expert in your pocket.\" Released August 7, 2025, leading SWE-bench Verified at 74.9% and achieving 88% on Aider Polyglot for real-world coding."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"GPT-5.2 (January 2026)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"SWE-bench Verified: 74.9%","plainChildren":""},{"type":"item","level":3,"content":"Aider Polyglot: 88%","plainChildren":""},{"type":"item","level":3,"content":"GPQA: 88.4% (GPT-5 Pro, without tools)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"OpenAI's flagship model achieving state-of-the-art 74.9% on SWE-bench Verified and 88% on Aider Polyglot, featuring significant improvements in math, coding, visual perception, and health analysis.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"GPT-5 is OpenAI's most advanced language model released on August 7, 2025, described as a \"PhD-level expert in your pocket\" capable of tackling everything from code and math to health advice and image analysis. The model leads the industry on SWE-bench Verified with 74.9% accuracy, ahead of o3's 69.1%, and achieves an impressive 88% on Aider Polyglot for real-world coding tasks. GPT-5 represents a significant leap forward in capabilities, particularly in mathematics, coding, visual perception, and health domains. Alongside GPT-5, OpenAI released the o-series models (o3 and o4-mini) trained to think longer before responding, and GPT-4.1 which excels at following user instructions and formatting requirements. The model family continues OpenAI's tradition of pushing the boundaries of AI capabilities while maintaining practical applicability."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Industry-leading coding: 74.9% on SWE-bench Verified (highest benchmark score), 88% on Aider Polyglot.","plainChildren":""},{"type":"item","level":3,"content":"PhD-level expertise: Described as \"PhD-level expert\" across multiple domains.","plainChildren":""},{"type":"item","level":3,"content":"Multimodal excellence: Superior visual perception, image analysis, and multi-modal reasoning.","plainChildren":""},{"type":"item","level":3,"content":"Math and health: Significant improvements in mathematical reasoning and health advice capabilities.","plainChildren":""},{"type":"item","level":3,"content":"o-series reasoning: Complemented by o3 and o4-mini models that think longer for better reasoning.","plainChildren":""},{"type":"item","level":3,"content":"GPT-4.1 precision: Parallel release excels at format compliance (JSON, XML) and instruction following.","plainChildren":""},{"type":"item","level":3,"content":"Real-world performance: 88% on Aider Polyglot demonstrates practical coding effectiveness.","plainChildren":""},{"type":"item","level":3,"content":"Multi-step tasks: Enhanced capabilities for complex, multi-step problem solving.","plainChildren":""},{"type":"item","level":3,"content":"OpenAI flagship: Latest advancement from the creators of ChatGPT and GPT-4.","plainChildren":""},{"type":"item","level":3,"content":"Released August 7, 2025 with immediate availability.","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: GPT-5.2 released January 2026 with enhanced reasoning, coding, and professional capabilities. GPT-5-Codex and GPT-5.2-Codex (November 2025) for professional software engineering and defensive cybersecurity. GPT-5 Instant (October 2025) improved mental health support. GPT-5 Pro replaces o3-pro with 88.4% GPQA. Custom GPTs auto-updated to GPT-5.2 on January 12, 2025. Knowledge cutoff: August 2025. Safe completions training with transparent refusals. Older GPT-5 models being phased out in favor of GPT-5.2.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"gpt-5.md"},{"type":"header","level":1,"content":"Grok 2.5 - https://x.ai/","children":[{"type":"text","content":"xAI's 314 billion parameter open-source model released under Apache 2.0 license. Features MoE architecture with 64 transformer layers, activating 2 of 8 experts per token, marking a strategic shift toward open-source AI development."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Grok 2.5 (Late 2024 / Early 2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://huggingface.co/xai-org","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"N/A (pre-dates standardized SWE-bench comparisons for this release)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"xAI's 314B open-source MoE model with 64 transformer layers and Apache 2.0 licensing, enabling commercial use and customization of enterprise-grade language capabilities.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Grok 2.5 is xAI's 314 billion parameter language model released under the Apache 2.0 license, representing a strategic move toward open-source AI. The model uses a Mixture of Experts architecture with 64 transformer layers, 48 attention heads for queries, 8 for keys/values, and a 131,072-token vocabulary. It intelligently selects 2 out of 8 experts per token for efficient inference. The Apache 2.0 license grants full commercial freedom including downloading, modifying, and distributing the model, though it prohibits using Grok 2.5 as training data for competing AI models. Running the full model requires at least 8 GPUs with 40GB+ VRAM, with the complete model file reaching approximately 500GB. Grok 2.5 was xAI's most advanced model before Grok 3 (February 2025) and Grok 4 (July 2025)."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"314B parameters with MoE: activates 2 of 8 experts per token","plainChildren":""},{"type":"item","level":3,"content":"64 transformer layers, 48 query attention heads, 8 key/value heads","plainChildren":""},{"type":"item","level":3,"content":"Apache 2.0 license with restriction against training competing models","plainChildren":""},{"type":"item","level":3,"content":"Requires 8+ GPUs with 40GB+ VRAM; ~500GB model file","plainChildren":""},{"type":"item","level":3,"content":"Succeeded by Grok 3 (Feb 2025) and Grok 4 (July 2025)","plainChildren":""},{"type":"item","level":3,"content":"Open-source adopters report 51% ROI vs 41% for proprietary solutions","plainChildren":""},{"type":"item","level":3,"content":"Full source code available on Hugging Face","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Apache 2.0 (with restriction on training competing models)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"grok-2-5.md"},{"type":"header","level":1,"content":"Grok 4 - https://x.ai/grok","children":[{"type":"text","content":"xAI's flagship AI model described as \"the most intelligent model in the world.\" Released July 9, 2025, featuring native tool use, real-time search, and specialized coding capabilities through Grok Code Fast 1."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"4.0 (2025-07-09)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"EQ-Bench: Leadership position (Grok 4.1)","plainChildren":""},{"type":"item","level":3,"content":"Hallucination reduction: ~65% fewer hallucinations in Grok 4.1 vs Grok 4","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"xAI's most advanced model trained on 200,000 GPU Colossus cluster with reinforcement learning at pretraining scale, featuring native tool use and specialized coding variant Grok Code Fast 1.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Grok 4 is xAI's flagship AI model released on July 9, 2025, described as \"the most intelligent model in the world.\" Trained using Colossus, xAI's 200,000 GPU cluster, Grok 4 runs reinforcement learning at pretraining scale to refine its reasoning abilities. The model features native tool use and real-time search integration, making it particularly effective for dynamic information retrieval and complex task execution. On August 28, 2025, xAI released Grok Code Fast 1, a specialized coding variant that excels at agentic coding, initially offered free on GitHub Copilot, Cursor, Cline, Roo Code, Kilo Code, opencode, and Windsurf. Grok 3 (the predecessor) achieved 79.4-80.4% on LiveCodeBench, demonstrating strong coding capabilities inherited by Grok 4."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Most intelligent model claim: xAI describes Grok 4 as \"the most intelligent model in the world\" at launch.","plainChildren":""},{"type":"item","level":3,"content":"Colossus training: Trained on 200,000 GPU cluster using reinforcement learning at pretraining scale.","plainChildren":""},{"type":"item","level":3,"content":"Native tool use: Built-in capabilities for using tools and APIs directly.","plainChildren":""},{"type":"item","level":3,"content":"Real-time search: Integrated real-time web search for up-to-date information.","plainChildren":""},{"type":"item","level":3,"content":"Grok Code Fast 1: Specialized coding model (August 2025) optimized for agentic coding workflows.","plainChildren":""},{"type":"item","level":3,"content":"Free initial access: Grok Code Fast 1 offered free on launch partners including GitHub Copilot and Cursor.","plainChildren":""},{"type":"item","level":3,"content":"Strong coding lineage: Grok 3 achieved 79.4-80.4% on LiveCodeBench, foundation for Grok 4.","plainChildren":""},{"type":"item","level":3,"content":"10x training increase: Grok 3 used 10x more compute than Grok 2, continuing scaling trajectory.","plainChildren":""},{"type":"item","level":3,"content":"xAI: Founded and led by Elon Musk.","plainChildren":""},{"type":"item","level":3,"content":"Released July 9, 2025 with Grok 4 Heavy variant.","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: Grok 4.1 released November 17, 2025 with EQ-Bench leadership and 65% fewer hallucinations. Grok 2.5 open-sourced under Apache-2.0 (August 2025). September 2025 added low-latency voice interactions and multimodal video generation via Grok Imagine. SuperGrok at $30/mo (128K memory), SuperGrok Heavy at $300/mo. Grok 5 announced for January 2026 with potential AGI capabilities. xAI API provides full access to Grok 4 Pro and Code variants.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"grok-4.md"},{"type":"header","level":1,"content":"Grok Code Fast 1 - https://x.ai/news/grok-code-fast-1","children":[{"type":"text","content":"Grok Code Fast 1 is xAI's speedy and economical reasoning model that excels at agentic coding, built from scratch with a brand-new architecture and offering exceptional speed at approximately 160 tokens per second.\n\n**Dataset ID:** aie-model"},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"1.0 (August 2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"SWE-Bench-Verified: 70.8%","plainChildren":""},{"type":"item","level":3,"content":"Speed: ~160 tokens/second (vs GPT-5 at 50.1, Gemini 2.5 Pro at 92.4, Claude 4 Sonnet at 78.7)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"text","content":"xAI's ultra-fast agentic coding model with 314B parameters (MoE), achieving 70.8% on SWE-Bench while delivering ~160 tokens/second throughput."}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Grok Code Fast 1 was released by xAI in late August 2025 as a speedy and economical reasoning model optimized for agentic coding. The model was built from scratch with a brand-new architecture and initially released quietly under the codename \"sonic\" before its official announcement.\n\nBuilt using a mixture-of-experts architecture with an estimated 314 billion parameters, the model was trained on a pre-training corpus rich with programming-related content and curated datasets reflecting real-world pull requests and coding tasks. It features a 256k token context window, enabling it to process larger codebases in context.\n\nThe model is exceptionally versatile across the software development stack and particularly adept at TypeScript, Python, Java, Rust, C++, and Go. It can complete tasks ranging from building zero-to-one projects to performing surgical bug fixes. On the full subset of SWE-Bench-Verified, grok-code-fast-1 scored 70.8% using xAI's internal evaluation harness.\n\nAccording to xAI's benchmarks, the model executes at approximately 160 tokens per second, significantly outpacing competitors: GPT-5 at 50.1 tokens/second, Gemini 2.5 Pro at 92.4, and Claude 4 Sonnet at 78.7. The model is available via the xAI API with competitive pricing and was offered for free through launch partners including GitHub Copilot, Cursor, Cline, Roo Code, Kilo Code, opencode, and Windsurf for a limited time."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"TypeScript","plainChildren":""},{"type":"item","level":3,"content":"Python","plainChildren":""},{"type":"item","level":3,"content":"Java","plainChildren":""},{"type":"item","level":3,"content":"Rust","plainChildren":""},{"type":"item","level":3,"content":"C++","plainChildren":""},{"type":"item","level":3,"content":"Go","plainChildren":""},{"type":"item","level":3,"content":"Supports other languages but particularly adept at these","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Architecture: Mixture-of-experts with ~314B parameters","plainChildren":""},{"type":"item","level":3,"content":"Context Window: 256k tokens","plainChildren":""},{"type":"item","level":3,"content":"Speed: ~160 tokens/second (industry-leading)","plainChildren":""},{"type":"item","level":3,"content":"Benchmark: 70.8% SWE-Bench-Verified","plainChildren":""},{"type":"item","level":3,"content":"Pricing: $0.20/1M input tokens, $1.50/1M output tokens, $0.02/1M cached tokens","plainChildren":""},{"type":"item","level":3,"content":"Launch Partners: GitHub Copilot, Cursor, Cline, Roo Code, Kilo Code, opencode, Windsurf","plainChildren":""},{"type":"item","level":3,"content":"Free Tier: Available free for limited time through select partners","plainChildren":""},{"type":"item","level":3,"content":"Codename: Initially released as \"sonic\"","plainChildren":""},{"type":"item","level":3,"content":"Training: Rich programming content, real-world PR datasets","plainChildren":""},{"type":"item","level":3,"content":"Future: Multimodal variant in training with parallel tool calling and extended context","plainChildren":""},{"type":"item","level":3,"content":"Developer: xAI","plainChildren":""},{"type":"item","level":3,"content":"Release: August 2025","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: General availability October 1, 2025. January 21, 2026 update added summarized thinking traces in Oracle Cloud Infrastructure Generative AI platform. Free launch period ended, transitioned to paid model. New multimodal variant in training with parallel tool usage and extended context. xAI commits to rapid update cadence (days, not weeks).","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""},{"type":"item","level":3,"content":"Free access through launch partner tools for limited time","plainChildren":""}]}]}]}],"sourcePath":"grok-code-fast-1.md"},{"type":"header","level":1,"content":"Kimi K2 Thinking - https://huggingface.co/moonshotai/Kimi-K2-Thinking","children":[{"type":"text","content":"Enhanced reasoning model from Moonshot AI that tops benchmarks for reasoning, coding, and agentic tasks. Released November 2025, outperforming GPT-5, Claude Sonnet 4.5 Thinking, and Grok-4."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"K2.5 (January 27, 2026)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/MoonshotAI/Kimi-K2","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"MATH-500: Outperformed frontier US models (OpenAI, Anthropic)","plainChildren":""},{"type":"item","level":3,"content":"HLE-Full: Highest score among 2,500 questions across math/physics domains","plainChildren":""},{"type":"item","level":3,"content":"NIST CAISI: Most capable PRC-based AI model at time of release","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Advanced reasoning-enhanced version of Kimi K2 that claims top position in reasoning, coding, and agentic-tool benchmarks, surpassing GPT-5 and Claude Sonnet 4.5 Thinking mode.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Kimi K2 Thinking is Moonshot AI's enhanced reasoning model released in November 2025, building upon the foundational Kimi K2 architecture with specialized capabilities for complex reasoning tasks. The model has claimed the top position across reasoning, coding, and agentic-tool benchmarks, outperforming proprietary leaders including OpenAI's GPT-5, Anthropic's Claude Sonnet 4.5 (Thinking mode), and xAI's Grok-4. It combines the trillion-parameter MoE architecture with advanced reasoning techniques to deliver exceptional performance on multi-step problems, code generation, and tool-using scenarios. Available as open weights on Hugging Face and through Moonshot AI's API, K2 Thinking represents a significant milestone in open-source AI capabilities."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Leading benchmarks: Tops GPT-5, Claude Sonnet 4.5 Thinking, and Grok-4 on standard evaluations.","plainChildren":""},{"type":"item","level":3,"content":"Reasoning-enhanced: Specialized architecture for complex multi-step reasoning and problem-solving.","plainChildren":""},{"type":"item","level":3,"content":"Exceptional coding: State-of-the-art performance on coding benchmarks with reasoning capabilities.","plainChildren":""},{"type":"item","level":3,"content":"Agentic excellence: Superior performance on tool-using and autonomous agent tasks.","plainChildren":""},{"type":"item","level":3,"content":"Open weights: Available on Hugging Face for community access and research.","plainChildren":""},{"type":"item","level":3,"content":"Built on K2 foundation: Leverages trillion-parameter MoE architecture of base Kimi K2.","plainChildren":""},{"type":"item","level":3,"content":"November 2025 release: Latest advancement from Alibaba-backed Moonshot AI.","plainChildren":""},{"type":"item","level":3,"content":"Cost-competitive: Maintains competitive API pricing while delivering leading performance.","plainChildren":""},{"type":"item","level":3,"content":"Community impact: Demonstrates that open-weight models can match or exceed proprietary leaders.","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: K2.5 released January 27, 2026 as open-source with enhanced multimodal capabilities (15T mixed visual/text tokens, 400M vision encoder), Agent Swarm (up to 100 concurrent agents), and parallelized attention. Heavy Mode runs 8 independent reasoning paths simultaneously. INT4 quantization with QAT enables 2x inference speed. Moonshot raised at $4.8B valuation. 2026 roadmap targets 50% inference cost reduction.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""},{"type":"item","level":3,"content":"Open weights available on Hugging Face","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Apache-2.0","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"kimi-k2-thinking.md"},{"type":"header","level":1,"content":"Kimi K2 - https://github.com/MoonshotAI/Kimi-K2","children":[{"type":"text","content":"State-of-the-art Mixture-of-Experts (MoE) language model from Moonshot AI with 1 trillion total parameters and exceptional coding performance. Released July 2025, outperforming Claude Opus 4 and GPT-4.1 on coding benchmarks."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"K2.5 (January 27, 2026)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/MoonshotAI/Kimi-K2","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"SWE-bench Verified: 65.8% (K2), higher on SWE-bench Multilingual (K2.5)","plainChildren":""},{"type":"item","level":3,"content":"SWE-bench Full: 71.6%","plainChildren":""},{"type":"item","level":3,"content":"LiveCodeBench: 53.7%","plainChildren":""},{"type":"item","level":3,"content":"HLE-Full: Surpasses GPT-5.2 and Claude Opus 4.5 (with tools)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Trillion-parameter open-weight MoE model from Alibaba-backed Moonshot AI, achieving state-of-the-art performance in coding, math, and agentic reasoning with 32B activated parameters.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model developed by Moonshot AI, a Beijing startup backed by Alibaba. Released in July 2025, it features 32 billion activated parameters and 1 trillion total parameters, making it one of the largest open-weight models available. The model achieves exceptional performance in frontier knowledge, mathematics, and coding, surpassing Claude Opus 4 on multiple benchmarks and demonstrating better overall performance than OpenAI's GPT-4.1. With 65.8% pass@1 on SWE-bench Verified and 71.6% on full SWE-bench, Kimi K2 excels at agentic reasoning and real-world coding tasks. The model is cost-effective at $0.15 per million input tokens and $2.50 per million output tokens—significantly cheaper than competitors."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Trillion-parameter MoE: 1 trillion total parameters with 32B activated, one of the largest open-weight models.","plainChildren":""},{"type":"item","level":3,"content":"Exceptional coding performance: 71.6% on SWE-bench, 65.8% on SWE-bench Verified, 53.7% on LiveCodeBench.","plainChildren":""},{"type":"item","level":3,"content":"Surpasses proprietary models: Outperforms Claude Opus 4 and GPT-4.1 on coding benchmarks.","plainChildren":""},{"type":"item","level":3,"content":"Long context support: Optimized for long-context understanding and processing.","plainChildren":""},{"type":"item","level":3,"content":"Agentic reasoning: Strong performance on tool use and multi-step reasoning tasks.","plainChildren":""},{"type":"item","level":3,"content":"Cost-effective: $0.15/M input tokens, $2.50/M output tokens (100x cheaper than Claude Opus 4 for input).","plainChildren":""},{"type":"item","level":3,"content":"Open weights: Available on GitHub and Hugging Face for community use.","plainChildren":""},{"type":"item","level":3,"content":"Moonshot AI: Developed by Alibaba-backed Beijing startup.","plainChildren":""},{"type":"item","level":3,"content":"Released July 2025 as a major advancement in open-weight models.","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: K2 Thinking and K2 Thinking Turbo released November 6, 2025 for complex reasoning and agentic tasks. Major price cuts: 75% off cached input, 50% off uncached input. K2.5 released January 27, 2026 as native multimodal agentic model trained on 15T mixed visual/text tokens. K2.5 features 256K context, MoonViT 400M vision encoder, 384 experts (8 selected per token), Agent Swarm for multi-agent parallel execution. Benchmarks show K2.5 surpassing GPT-5.2 on SWE-bench Multilingual and VideoMMMU. Moonshot raised at $4.8B valuation.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""},{"type":"item","level":3,"content":"Open weights available on GitHub and Hugging Face","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Apache-2.0","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"kimi-k2.md"},{"type":"header","level":1,"content":"Kimi Linear - https://github.com/MoonshotAI/Kimi-Linear","children":[{"type":"text","content":"Moonshot AI's efficient hybrid linear attention model released October 2025 with 48B total/3B active parameters, achieving 6x faster decoding at 1M token contexts and 75% KV cache reduction through innovative Kimi Delta Attention mechanism."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"48B A3B Instruct (October 31 - November 1, 2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/MoonshotAI/Kimi-Linear","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"RULER benchmark: 84.3 vs 81.3 for MLA baseline","plainChildren":""},{"type":"item","level":3,"content":"6.3x speedup at 1M tokens (1.84ms vs 11.48ms per output token)","plainChildren":""},{"type":"item","level":3,"content":"75% KV cache reduction","plainChildren":""},{"type":"item","level":3,"content":"Superior performance across short-context, long-context, and RL scaling regimes","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Efficient 48B/3B MoE model with hybrid linear attention achieving 6x faster decoding at 1M token contexts and 75% memory reduction through Kimi Delta Attention, outperforming full attention architectures.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Kimi Linear is a hybrid linear attention model released by Moonshot AI in October 2025 with 48 billion total parameters and 3 billion active parameters that achieves superior performance and efficiency compared to full attention architectures through its innovative Kimi Delta Attention mechanism. The model uses a layerwise hybrid structure that strategically combines Kimi Delta Attention (KDA) with Multi-Head Latent Attention (MLA) layers in a 3:1 ratio, balancing computational efficiency with global information processing capabilities. KDA extends the Gated DeltaNet framework with a finer-grained, channel-wise gating mechanism that allows independent control over memory decay rates across individual feature dimensions, using specialized Diagonal-Plus-Low-Rank (DPLR) transition matrices optimized through a bespoke chunkwise algorithm that delivers approximately 100% improvement in operator efficiency and nearly 2x kernel speedup for sequence lengths up to 64K. This eliminates three matrix multiplications per chunk while remaining consistent with the classical delta rule. Kimi Linear reduces KV cache requirements by up to 75% during inference, enabling processing of batches up to 4 times larger on the same GPU hardware or deployment on cheaper infrastructure. The model achieves up to 6x faster decoding throughput at 1 million token contexts compared to full MLA baselines—specifically 1.84 milliseconds per output token versus 11.48 milliseconds for the MLA baseline at precisely 1 million tokens, transforming ultra-long context processing from offline-only speeds to potentially interactive performance levels. Supporting a 1,048,576 token context window and released under MIT license, Kimi Linear demonstrates superior performance across short-context, long-context, and reinforcement learning scaling regimes, with particularly strong results on RULER benchmark (84.3 vs 81.3 for MLA baseline) and Repo QA tasks."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Hybrid architecture: 3:1 ratio of Kimi Delta Attention to Multi-Head Latent Attention layers.","plainChildren":""},{"type":"item","level":3,"content":"Kimi Delta Attention: Finer-grained channel-wise gating for independent memory decay control.","plainChildren":""},{"type":"item","level":3,"content":"100% operator efficiency: Specialized DPLR transition matrices with bespoke chunkwise algorithm.","plainChildren":""},{"type":"item","level":3,"content":"2x kernel speedup: Nearly 2x faster for sequence lengths up to 64K tokens.","plainChildren":""},{"type":"item","level":3,"content":"75% KV cache reduction: Enables 4x larger batches on same hardware or cheaper deployment.","plainChildren":""},{"type":"item","level":3,"content":"6x faster decoding: At 1M token contexts vs full MLA baseline (1.84ms vs 11.48ms per token).","plainChildren":""},{"type":"item","level":3,"content":"1M token context: Supports 1,048,576 token context window.","plainChildren":""},{"type":"item","level":3,"content":"Interactive ultra-long context: Transforms offline-only speeds to interactive performance.","plainChildren":""},{"type":"item","level":3,"content":"Superior benchmarks: Outperforms full MLA across all evaluation scenarios with identical training.","plainChildren":""},{"type":"item","level":3,"content":"RULER excellence: 84.3 vs 81.3 for MLA baseline on long-range retrieval tasks.","plainChildren":""},{"type":"item","level":3,"content":"Drop-in replacement: Functions as direct replacement for traditional full attention architectures.","plainChildren":""},{"type":"item","level":3,"content":"No positional encoding: MLA layers use no positional encoding; KDA handles implicit positioning.","plainChildren":""},{"type":"item","level":3,"content":"Eliminates matrix operations: Removes three matrix multiplications per chunk.","plainChildren":""},{"type":"item","level":3,"content":"48B total / 3B active: Mixture of Experts architecture with efficient parameter usage.","plainChildren":""},{"type":"item","level":3,"content":"MIT licensed: Open-source with KDA kernel and vLLM implementations available.","plainChildren":""},{"type":"item","level":3,"content":"Pre-trained checkpoints: Both pre-trained and instruction-tuned models released.","plainChildren":""},{"type":"item","level":3,"content":"Code repository QA: Strong performance on finding information in large code repositories.","plainChildren":""},{"type":"item","level":3,"content":"RL scaling: Particularly effective for reinforcement learning with longer sequences.","plainChildren":""},{"type":"item","level":3,"content":"Cost-sensitive deployment: Optimized for hardware efficiency and cost reduction.","plainChildren":""},{"type":"item","level":3,"content":"Released October 31 - November 1, 2025 as efficiency-focused alternative to K2.","plainChildren":""},{"type":"item","level":3,"content":"Moonshot AI: From Alibaba-backed Beijing startup known for long-context innovations.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-02-06","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"MIT","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"kimi-linear.md"},{"type":"header","level":1,"content":"Kite — https://kite.com","children":[{"type":"text","content":"A desktop AI code-completion assistant focused initially on Python, later multi-language. Ran local models for low-latency, privacy-first completions and editor integrations. Company ceased operations in late 2022 and released parts of its codebase as open source."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Agent","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"text","content":"Archived (2022-12-31)"}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/kiteco/kiteco","plainChildren":""},{"type":"item","level":3,"content":"https://github.com/kiteco","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"[4] Strong historical accuracy for Python completions (context-aware compared to older alphabetical completions)","plainChildren":""},{"type":"item","level":3,"content":"[3] Multi-language coverage decent but uneven (best for Python)","plainChildren":""},{"type":"item","level":3,"content":"[4] Privacy: good (local processing design)","plainChildren":""},{"type":"item","level":3,"content":"[2] Business viability: failed to monetize sufficiently","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Former AI-powered code-completion assistant that ran local ML models for low-latency, privacy-first context-aware completions and inline documentation across popular editors; company ceased operations in late 2022 and released parts of its codebase as open source.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Kite was an early AI-assisted coding tool (founded 2014) that provided context-aware code completions, documentation lookups, and inline examples inside editors. Its core differentiator was processing code and ML inference locally on the developer's machine to reduce latency and address privacy concerns. Kite trained models on large bodies of open-source code and tuned them for code prediction tasks rather than using plain NLP models. Despite strong technical work and a sizeable user base, Kite shut down operations in late 2022 and open-sourced a portion of its codebase."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Python","plainChildren":""},{"type":"item","level":3,"content":"JavaScript / TypeScript","plainChildren":""},{"type":"item","level":3,"content":"Java","plainChildren":""},{"type":"item","level":3,"content":"Go","plainChildren":""},{"type":"item","level":3,"content":"Other","plainChildren":""},{"type":"item","level":3,"content":"Many others via editor plugins","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Founded 2014; widely adopted by Python developers for smarter completions and docs.","plainChildren":""},{"type":"item","level":3,"content":"Raised funding and grew to a large community, but announced shutdown in late 2022 due to a combination of technical limits (models not yet delivering transformative improvements) and monetization challenges.","plainChildren":""},{"type":"item","level":3,"content":"Legacy: influenced expectations for context-aware completions and privacy-conscious local inference; lessons from Kite informed subsequent entrants and enterprise offerings in the AI coding space.","plainChildren":""},{"type":"item","level":3,"content":"As of 2025 the company is inactive; repositories remain as historical artifacts and starting points for community forks and research.","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: No new development. Company remains defunct since late 2022. Repositories serve as historical artifacts only. The AI coding assistant space has been dominated by GitHub Copilot, Cursor, Cline, and other modern tools. Kite's legacy influence is visible in privacy-first local inference approaches adopted by newer tools.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""},{"type":"item","level":3,"content":"Company published several repositories under the kiteco GitHub organization after winding down; some components and research artifacts are available for reuse.","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Mixed / see repository","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"N/A","plainChildren":""},{"type":"item","level":3,"content":"Open-source (post-shutdown): repositories freely available, not a time-limited trial.","plainChildren":""}]}]}]}],"sourcePath":"kite.md"},{"type":"header","level":1,"content":"Llama 4 - https://llama.meta.com/","children":[{"type":"text","content":"Meta's open-weight natively multimodal AI models using Mixture-of-Experts architecture. Released April 2025, Llama 4 Scout (109B/17B active) and Maverick (400B/17B active) deliver frontier-level performance with industry-leading 10M token context length."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Llama 4 Maverick 402B / Scout 109B (April 2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/meta-llama/llama-models","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"SWE-bench Verified: Competitive with GPT-4o","plainChildren":""},{"type":"item","level":3,"content":"Aider Polyglot: N/A","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Meta's open-weight multimodal MoE models with 17B active parameters per token, supporting up to 10M token context and rivaling GPT-4 and Gemini 2.0 on coding, reasoning, and image tasks.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Llama 4 is Meta's fourth-generation open-weight large language model family, featuring two main variants: Scout (109B total parameters, 16 experts) and Maverick (400B total parameters, 128 experts), both activating only 17B parameters per token via Mixture-of-Experts. Scout supports an industry-leading 10 million token context window and can run on a single NVIDIA H100 GPU with Int4 quantization at roughly $0.09 per million tokens. Maverick is the flagship variant designed for demanding reasoning and specialized knowledge tasks. Both models are natively multimodal, processing text and image inputs through early fusion architecture, and were trained on 40 trillion tokens across 200 languages. They use interleaved attention layers with iRoPE for enhanced generalization across extended sequences."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"MoE architecture: 17B active parameters per token for both Scout and Maverick","plainChildren":""},{"type":"item","level":3,"content":"Scout fits on a single H100 GPU with Int4 quantization at ~$0.09/M tokens","plainChildren":""},{"type":"item","level":3,"content":"10M token context window (Scout), 1M tokens (Maverick, extendable to 10M)","plainChildren":""},{"type":"item","level":3,"content":"Natively multimodal with early fusion for text and images","plainChildren":""},{"type":"item","level":3,"content":"Trained on 200 languages with fine-tuning for 12 specific languages","plainChildren":""},{"type":"item","level":3,"content":"Maverick matches or exceeds GPT-4 and Gemini 2.0 on many benchmarks","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Llama 4 Community License","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"llama-4.md"},{"type":"header","level":1,"content":"MiniMax M2 - https://www.minimax.io/news/minimax-m2","children":[{"type":"text","content":"MiniMax M2 is a compact, fast, and cost-effective MoE (Mixture of Experts) model with 230 billion total parameters and 10 billion active parameters, built for elite performance in coding and agentic tasks.\n\n**Dataset ID:** aie-model"},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"M2.1 (December 23, 2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/MiniMax-AI/MiniMax-M2","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"SWE-bench Verified: 69.4% (close to GPT-5's 74.9%)","plainChildren":""},{"type":"item","level":3,"content":"#1 among open-source models globally in composite intelligence score","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"text","content":"Compact 230B-parameter MoE model (10B active) optimized for coding and agentic workflows, offering elite performance at 8% the cost of Claude Sonnet with 2x speed."}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"MiniMax M2 is an open-source model launched in late October 2025, designed specifically for coding and agentic workflows. As a Mixture of Experts (MoE) architecture, it features 230 billion total parameters with only 10 billion active parameters, achieving remarkable efficiency without sacrificing performance.\n\nThe model excels in end-to-end development workflows and integrates seamlessly with popular coding tools like Claude Code, Cursor, Cline, Kilo Code, and Droid. It performs exceptionally well on multi-file edits, coding-run-fix loops, and test-validated repairs. Its strong performance on Terminal-Bench and SWE-Bench-style tasks demonstrates practical effectiveness in terminals, IDEs, and CI environments across multiple programming languages.\n\nAt only 8% of the price of Claude Sonnet and twice the speed, MiniMax M2 offers compelling value for developers. The model is available for free for a limited time and ranks #1 among open-source models in composite intelligence scores across mathematics, science, instruction following, coding, and agentic tool use."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Architecture: MoE (230B total params, 10B active)","plainChildren":""},{"type":"item","level":3,"content":"Pricing: 8% of Claude Sonnet cost, 2x faster, free for limited time","plainChildren":""},{"type":"item","level":3,"content":"Benchmark Performance: SWE-bench Verified 69.4% (near GPT-5's 74.9%)","plainChildren":""},{"type":"item","level":3,"content":"Tool Integration: Claude Code, Cursor, Cline, Kilo Code, Droid","plainChildren":""},{"type":"item","level":3,"content":"Coding Strengths: Multi-file edits, coding-run-fix loops, test-validated repairs","plainChildren":""},{"type":"item","level":3,"content":"Performance Tests: Strong on Terminal-Bench and Multi-SWE-Bench tasks","plainChildren":""},{"type":"item","level":3,"content":"Global Ranking: #1 open-source model in composite intelligence score","plainChildren":""},{"type":"item","level":3,"content":"Release: Late October 2025","plainChildren":""},{"type":"item","level":3,"content":"Availability: Open-source on GitHub, available via API platforms","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: M2.1 released December 23, 2025 with enhanced Rust/Java/Go/C++/Kotlin/ObjC/TS/JS support, improved UI/UX design capabilities (VIBE-Web 91.5), better instruction following, and dialogue/writing enhancements. Revenue grew 7x to $70M ARR. 2026 roadmap includes agent reliability, voice integration, and structured workflows.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Open Source (custom MiniMax license)","plainChildren":""}]}]}]}],"sourcePath":"minimax-m2.md"},{"type":"header","level":1,"content":"NVIDIA Nemotron Nano 2 VL - https://developer.nvidia.com/blog/develop-specialized-ai-agents-with-new-nvidia-nemotron-vision-rag-and-guardrail-models/","children":[{"type":"text","content":"NVIDIA Nemotron Nano 2 VL is a 12B multimodal reasoning model that enables AI assistants to extract, interpret, and act on information across text, images, tables, and videos with improved accuracy and efficiency.\n\n**Dataset ID:** aie-model"},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"V2 (October/November 2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-BF16","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"OCRBench v2: Leading results","plainChildren":""},{"type":"item","level":3,"content":"Average across benchmarks: ~74 (MMMU, MathVista, AI2D, OCRBench, OCR-Reasoning, ChartQA, DocVQA, Video-MME)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"text","content":"12B-parameter multimodal vision-language model with hybrid Mamba-Transformer architecture, optimized for document understanding, video comprehension, and efficient reasoning tasks."}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"NVIDIA Nemotron Nano 2 VL is a 12-billion parameter multimodal reasoning model released in late October/early November 2025. The model features a hybrid Mamba-Transformer architecture delivering on-par accuracy with high token throughput and low latency for efficient large-scale reasoning across visual and text tasks.\n\nBuilt on top of the Nemotron Nano V2 12B reasoning LLM and RADIOv2.5 vision encoder, the model introduces the Efficient Video Sampling (EVS) method that identifies and prunes temporally static patches in video sequences. EVS reduces token redundancy while preserving essential semantics, enabling the model to process longer clips and deliver results more swiftly—achieving up to 2.5x higher throughput without sacrificing accuracy.\n\nThe model was trained on the Nemotron VLM Dataset V2 with over 11 million high-quality samples, optimized for optical-character recognition, chart reasoning, and multimodal comprehension. With a context length of 49,152 tokens, it enhances capability for multi-image and video understanding.\n\nNemotron Nano 2 VL achieves leading results on OCRBench v2 and scores approximately 74 average across major benchmarks, surpassing prior open VL baselines. The model is available under a permissive NVIDIA open license with deployment supported across NeMo, NIM, and major inference runtimes including Hugging Face, Fireworks AI, Nebius AI Studio, and vLLM."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""},{"type":"item","level":3,"content":"Primarily focused on visual understanding tasks rather than code generation","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Architecture: Hybrid Mamba-Transformer, 12B parameters","plainChildren":""},{"type":"item","level":3,"content":"Context Length: 49,152 tokens","plainChildren":""},{"type":"item","level":3,"content":"Efficient Video Sampling (EVS): Up to 2.5x throughput improvement","plainChildren":""},{"type":"item","level":3,"content":"Training Data: Nemotron VLM Dataset V2 (11M+ high-quality samples)","plainChildren":""},{"type":"item","level":3,"content":"Benchmarks: ~74 average (MMMU, MathVista, AI2D, OCRBench, OCR-Reasoning, ChartQA, DocVQA, Video-MME)","plainChildren":""},{"type":"item","level":3,"content":"Vision Encoder: RADIOv2.5","plainChildren":""},{"type":"item","level":3,"content":"Base LLM: Nemotron Nano V2 12B","plainChildren":""},{"type":"item","level":3,"content":"Specializations: OCR, chart reasoning, document understanding, video comprehension","plainChildren":""},{"type":"item","level":3,"content":"License: Permissive NVIDIA open license","plainChildren":""},{"type":"item","level":3,"content":"Deployment: NeMo, NIM, vLLM, Hugging Face, Fireworks AI, Nebius AI Studio","plainChildren":""},{"type":"item","level":3,"content":"Release: October 28, 2025","plainChildren":""},{"type":"item","level":3,"content":"Developer: NVIDIA","plainChildren":""},{"type":"item","level":3,"content":"Pricing: $0.20/M input tokens, $0.60/M output tokens (January 2026)","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: Context window extended to 128K tokens (from 16K). Available in BF16, FP8, and FP4 precision formats on HuggingFace. Supports reasoning-on/off modes for flexible inference. 35% higher throughput than Llama-3.1-Nemotron-Nano-VL-8B. Up to 6x higher inference throughput on reasoning tasks on single NVIDIA A10G GPU. Multi-language support including English, Chinese, Arabic, Turkish, Russian. Free tier available on OpenRouter.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""},{"type":"item","level":3,"content":"Open-weights with permissive NVIDIA license","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"NVIDIA Open Model License (permissive)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""},{"type":"item","level":3,"content":"Available for free under NVIDIA's open license","plainChildren":""}]}]}]}],"sourcePath":"nvidia-nemotron-nano-2-vl.md"},{"type":"header","level":1,"content":"Phi-4 - https://azure.microsoft.com/en-us/products/phi","children":[{"type":"text","content":"Microsoft's 14B parameter small language model delivering competitive reasoning performance against models 5-50x its size. Scores 84.8% on MMLU and 80.4% on MATH, optimized for local deployment on consumer hardware."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Phi-4 Reasoning Plus (2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://huggingface.co/microsoft/phi-4","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"MMLU: 84.8%","plainChildren":""},{"type":"item","level":3,"content":"MATH: 80.4%","plainChildren":""},{"type":"item","level":3,"content":"GPQA: 56.1%","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Microsoft's 14B small language model excelling at mathematical reasoning and on-device deployment, competing with models many times its size through data quality optimization.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Phi-4 is Microsoft's 14-billion parameter small language model designed for high-performance reasoning while maintaining efficiency for local deployment on consumer hardware. The family includes Phi-4 Reasoning (14B), Phi-4-mini-instruct (3.8B with grouped-query attention and 200K vocabulary), Phi-4-multimodal (5.6B with unified text/image/audio input), and Phi-4 Reasoning Plus (enhanced via additional reinforcement learning). Trained on 9.8 trillion tokens from synthetic datasets, filtered web content, and academic materials using 1,920 H100-80G GPUs over 21 days. Phi-4 scores 84.8% on MMLU (surpassing GPT-4o-mini at 81.8%), 80.4% on MATH, 80.6% on MGSM, and 56.1% on GPQA. The model operates with a 16K token context length and runs efficiently on consumer laptops, edge devices, mobile processors, and Intel NPUs."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"14B parameters competing with models 5-50x larger through data quality optimization","plainChildren":""},{"type":"item","level":3,"content":"Phi-4-mini (3.8B) runs on mobile and edge devices","plainChildren":""},{"type":"item","level":3,"content":"Phi-4-multimodal (5.6B) handles text, image, and audio inputs","plainChildren":""},{"type":"item","level":3,"content":"On-device deployment: no cloud connectivity required for inference","plainChildren":""},{"type":"item","level":3,"content":"Intel NPU acceleration support; 1,955 tokens/sec on Xeon 6 processors","plainChildren":""},{"type":"item","level":3,"content":"Knowledge cutoff: June 2024 and earlier","plainChildren":""},{"type":"item","level":3,"content":"Philosophy: data quality over model scale","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"MIT","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"phi-4.md"},{"type":"header","level":1,"content":"PolyCoder - https://github.com/VHellendoorn/Code-LMs","children":[{"type":"text","content":"Open-source code generation model with strong C-language performance"},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2022-03-01","plainChildren":""},{"type":"item","level":3,"content":"Available in multiple model sizes: 160M, 405M, and 2.7B parameters","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/VHellendoorn/Code-LMs","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"[4] Excellent open-source alternative for systems-level code (C/C++).","plainChildren":""},{"type":"item","level":3,"content":"[3] Not competitive with 2024/2025 largest commercial models on all-language benchmarks.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Open-source GPT-2-style autoregressive code model (160M/405M/2.7B) with particularly strong performance on C/C++ code; includes checkpoints, training scripts, and tokenizer configs under the MIT license for reproducible research and on-prem deployments.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"PolyCoder is an open-source autoregressive code model developed by researchers at Carnegie Mellon University. Built on a GPT-2 style decoder-only transformer, PolyCoder was trained on approximately 249GB of GitHub-sourced code across a dozen languages and published in early 2022. The project ships model checkpoints, preprocessing scripts, tokenizer configs and evaluation notebooks under a permissive MIT license, enabling reproducible research, self-hosting, and fine-tuning.\n\nPolyCoder was released in multiple sizes (160M, 405M and 2.7B parameters). The 2.7B model was notable for outperforming contemporaneous models on C code generation benchmarks in the original paper, making it particularly well-suited for systems and embedded programming tasks where correctness and low-level API usage matter."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"C/C++","plainChildren":""},{"type":"item","level":3,"content":"Python","plainChildren":""},{"type":"item","level":3,"content":"JavaScript","plainChildren":""},{"type":"item","level":3,"content":"Java","plainChildren":""},{"type":"item","level":3,"content":"Go","plainChildren":""},{"type":"item","level":3,"content":"PHP","plainChildren":""},{"type":"item","level":3,"content":"Ruby","plainChildren":""},{"type":"item","level":3,"content":"C#","plainChildren":""},{"type":"item","level":3,"content":"Other","plainChildren":""},{"type":"item","level":3,"content":"12 in total","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Trained on ~249GB of code; primary claim-to-fame is strong C-code performance compared to models available in 2022.","plainChildren":""},{"type":"item","level":3,"content":"Comes in three sizes (160M, 405M, 2.7B) so teams can choose a footprint that matches hardware constraints.","plainChildren":""},{"type":"item","level":3,"content":"Checkpoints and training/evaluation scripts were published to enable reproducible research; model files were also archived on Zenodo and mirrored to Hugging Face by community contributors.","plainChildren":""},{"type":"item","level":3,"content":"Requires modern Transformers (4.23+) for out-of-the-box loading; community adapters support LoRA/QLoRA fine-tuning and GGUF quantized deployments.","plainChildren":""},{"type":"item","level":3,"content":"Good choice for on-premise, privacy-sensitive deployments (no external API calls required).","plainChildren":""},{"type":"item","level":3,"content":"Keep expectations realistic: PolyCoder is a 2022-era model and does not match the capabilities of later multi-hundred-billion-parameter code-specialized models, though it remains valuable for C/C++ and systems-level use-cases.","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: No new official releases. Model remains a 2022-era artifact, superseded by Code Llama 70B, GPT-4o, and other modern code models. Community maintains GGUF quantized builds for llama.cpp and Text-Generation-WebUI. LoRA/QLoRA fine-tuning adapters available. Still valuable for on-premise C/C++ focused deployments with strict privacy requirements. Can be integrated into CI/CD pipelines via Flask/FastAPI wrappers.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"MIT","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"N/A","plainChildren":""},{"type":"item","level":3,"content":"Open-source model: checkpoints, code, and training artifacts are freely available under MIT license (Zenodo archive and GitHub repo), not a time-limited trial.","plainChildren":""}]}]}]}],"sourcePath":"polycoder.md"},{"type":"header","level":1,"content":"Qwen 2.5 Coder - https://github.com/QwenLM/Qwen-Code","children":[{"type":"text","content":"Latest coding-focused model from Alibaba's Qwen family, featuring major upgrades in code understanding and generation capabilities. Released in 2025 as a significant advancement in open-source coding models."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Qwen3-Max (September 5, 2025); latest snapshot qwen3-max-2026-01-23 (January 2026)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/QwenLM/Qwen-Code","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"HumanEval: 88.4% (Qwen 2.5 Coder 7B)","plainChildren":""},{"type":"item","level":3,"content":"Spider: 82.0% (Qwen 2.5 Coder 7B)","plainChildren":""},{"type":"item","level":3,"content":"LiveCodeBench: 38.7 (Qwen2.5-Max)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Alibaba Cloud's family of specialized coding models (1.5B-32B parameters) with state-of-the-art performance on code generation and debugging benchmarks.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Qwen 2.5 Coder is a family of coding-specialized language models developed by Alibaba Cloud, released on September 19, 2024. The series includes models ranging from 1.5B to 32B parameters, targeting performance levels closer to closed-source models.\n\nThe flagship Qwen 2.5 Coder 7B model achieves 88.4% on the HumanEval benchmark, surpassing both Codestral (81.1%) and DeepSeek Coder v2 Lite (81.1%). On the Spider benchmark for SQL generation, Qwen 2.5 Coder leads with 82.0% compared to Codestral's 76.6%.\n\nIn January 2025, Alibaba released Qwen2.5-Max, their most advanced model available through Alibaba Cloud's Model Studio and the Qwen Chat platform. This model excels in code generation and debugging with a LiveCodeBench score of 38.7, slightly behind Claude Sonnet's 38.9. Qwen2.5-Max is positioned to compete with models like DeepSeek R3 in the enterprise coding space."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Model Family: 1.5B, 3B, 7B, 14B, 32B parameters","plainChildren":""},{"type":"item","level":3,"content":"Release: Qwen 2.5 Coder (September 2024), Qwen2.5-Max (January 2025)","plainChildren":""},{"type":"item","level":3,"content":"Benchmarks:","plainChildren":""},{"type":"item","level":3,"content":"HumanEval: 88.4% (7B model)","plainChildren":""},{"type":"item","level":3,"content":"Spider: 82.0% (7B model)","plainChildren":""},{"type":"item","level":3,"content":"LiveCodeBench: 38.7 (Max model)","plainChildren":""},{"type":"item","level":3,"content":"Strengths: Code generation, debugging, SQL generation","plainChildren":""},{"type":"item","level":3,"content":"Availability: Alibaba Cloud Model Studio, Qwen Chat platform","plainChildren":""},{"type":"item","level":3,"content":"Competition: Positioned against DeepSeek R3, Codestral, Claude Sonnet","plainChildren":""},{"type":"item","level":3,"content":"Open source: Qwen 2.5 Coder series available for local deployment","plainChildren":""},{"type":"item","level":3,"content":"Developer: Alibaba Cloud","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: Qwen2.5-Max launched January 2025 as MoE model pretrained on 20T+ tokens, outperforming GPT-4o and DeepSeek-V3. Qwen2.5-VL multimodal family released January 2025. Qwen3-Max launched September 5, 2025, outperforming other foundation models. Latest snapshot qwen3-max-2026-01-23 integrates thinking/non-thinking modes with web search, code interpreter, and webpage extraction tools. Supports 92 programming languages and 29 natural languages with 128K context window.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Apache-2.0","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"qwen-2-5-coder.md"},{"type":"header","level":1,"content":"Qwen3 - https://qwenlm.github.io/blog/qwen3/","children":[{"type":"text","content":"Alibaba's flagship trillion-parameter large language model with MoE architecture. Qwen3-Max achieves 69.6% on SWE-Bench Verified and perfect scores on AIME 2025 and HMMT math benchmarks, competing with GPT-5 and Claude Opus 4."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Qwen3-Max (2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/QwenLM/Qwen3","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"SWE-bench Verified: 69.6%","plainChildren":""},{"type":"item","level":3,"content":"AIME 2025: 100%","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Alibaba's trillion-parameter MoE model excelling at reasoning, coding, and multilingual tasks with adaptive thinking modes and 262K token context.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Qwen3-Max is Alibaba's flagship large language model featuring approximately one trillion parameters trained on 36 trillion tokens using a mixture-of-experts decoder architecture. It supports up to 262,000 input tokens and 65,536 output tokens. The model is available in base, instruction-tuned, and Thinking (reasoning) variants. Qwen3-Max-Thinking achieves perfect 100-point scores on AIME 2025 and HMMT math benchmarks, while the instruction variant scored 69.6% on SWE-Bench Verified. The model can seamlessly switch between thinking mode for complex reasoning and non-thinking mode for efficient dialogue. The broader Qwen3 family includes open-weight variants like Qwen3-30B-A3B (30.5B total, 3.3B active across 128 experts) and multimodal extensions including Qwen3-VL-235B and Qwen3-Omni-30B."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Trillion-parameter MoE architecture trained on 36 trillion tokens","plainChildren":""},{"type":"item","level":3,"content":"Perfect scores on AIME 2025 and HMMT mathematical reasoning benchmarks","plainChildren":""},{"type":"item","level":3,"content":"262K input / 65K output token context window","plainChildren":""},{"type":"item","level":3,"content":"Adaptive reasoning: switches between thinking and non-thinking modes","plainChildren":""},{"type":"item","level":3,"content":"Qwen3-Max is closed-weights; smaller Qwen3 variants are open-weight","plainChildren":""},{"type":"item","level":3,"content":"Multimodal extensions available (Qwen3-VL, Qwen3-Omni)","plainChildren":""},{"type":"item","level":3,"content":"Ranked third globally on Text Arena leaderboard, surpassing GPT-5-Chat","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary (Qwen3-Max); Apache 2.0 (smaller open-weight variants)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"qwen3.md"},{"type":"header","level":1,"content":"Seed-Coder - https://github.com/ByteDance-Seed/Seed-Coder","children":[{"type":"text","content":"Lightweight open-source code LLM family from ByteDance Seed trained on 6 trillion tokens. Achieves state-of-the-art performance at 8B scale, surpassing much larger models."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"1.0 (May 8, 2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/ByteDance-Seed/Seed-Coder","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"SOTA among open-source models at 8B parameter scale","plainChildren":""},{"type":"item","level":3,"content":"Surpasses many larger models on coding benchmarks","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Family of lightweight open-source code LLMs (base, instruct, reasoning) trained on 6T tokens from GitHub code, commit histories, and code-related web data, achieving SOTA at 8B scale.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Seed-Coder is a family of lightweight open-source code language models developed by ByteDance Seed, comprising base, instruct, and reasoning model variants. The pretraining corpus comprises approximately 6 trillion tokens sourced from GitHub code, commit histories, and code-related web data, providing comprehensive coverage of modern software development practices. Despite its relatively compact 8B parameter scale, Seed-Coder achieves state-of-the-art performance among open-source models at this size and even surpasses some much larger models in coding benchmarks. All models in the family are publicly available on Hugging Face, making them accessible for research, fine-tuning, and deployment. The model family represents ByteDance's contribution to open-source AI coding tools, providing an efficient alternative for developers who need strong coding capabilities without requiring massive computational resources."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"6 trillion tokens: Massive training corpus from GitHub code, commits, and code-related web data.","plainChildren":""},{"type":"item","level":3,"content":"Lightweight efficiency: Achieves SOTA at 8B scale, surpassing much larger models.","plainChildren":""},{"type":"item","level":3,"content":"Model family: Includes base, instruct, and reasoning variants for different use cases.","plainChildren":""},{"type":"item","level":3,"content":"Open source: All models publicly available on Hugging Face collection.","plainChildren":""},{"type":"item","level":3,"content":"State-of-the-art 8B: Best performance among open-source models at 8B parameter scale.","plainChildren":""},{"type":"item","level":3,"content":"Compact deployment: Suitable for resource-constrained environments while maintaining quality.","plainChildren":""},{"type":"item","level":3,"content":"ByteDance Seed: Developed by ByteDance's AI research division.","plainChildren":""},{"type":"item","level":3,"content":"Research-friendly: Accessible for academic research, fine-tuning, and custom deployments.","plainChildren":""},{"type":"item","level":3,"content":"Code-focused training: Specialized corpus emphasizing practical software development.","plainChildren":""},{"type":"item","level":3,"content":"Released May 8, 2025 as part of ByteDance's open-source AI strategy.","plainChildren":""},{"type":"item","level":3,"content":"Model variants: Base (32K context), Instruct (32K context), Reasoning (64K context), plus bf16 reasoning variant.","plainChildren":""},{"type":"item","level":3,"content":"Model-centric data curation: Uses LLMs to filter and curate training data, minimizing manual effort.","plainChildren":""},{"type":"item","level":3,"content":"Deployment: transformers library, vLLM offline batched inference, multi-GPU distributed serving with tensor parallelism.","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: Released May 8, 2025. ByteDance Seed team preparing next-generation launches including Doubao 2.0, Seedream 5.0, and Seeddance 2.0 potentially by February 2026. Model supports vLLM deployment and multi-GPU distributed serving for long-context inputs up to 32K tokens.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"MIT","plainChildren":""},{"type":"item","level":3,"content":"Note: Different sources report both MIT and Apache-2.0; verify from official repository","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"seed-coder.md"},{"type":"header","level":1,"content":"StarCoder - https://huggingface.co/bigcode/starcoder","children":[{"type":"text","content":"An open-source large language model for code, developed by the BigCode community and released on Hugging Face. Built for code generation, completion and code-aware assistance across many programming languages."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"StarCoder2-15B (February 2024)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/bigcode-project/starcoder","plainChildren":""},{"type":"item","level":3,"content":"Hugging Face: https://huggingface.co/bigcode/starcoder","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"[5] Strong open-source code generation baseline","plainChildren":""},{"type":"item","level":3,"content":"[4] Excellent multilingual code coverage and long context","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Open-source 15.5B-parameter decoder-only transformer for code generation and infilling, trained on permissively licensed GitHub code (The Stack). Optimized for code completion, infilling, translation between languages, and code-aware QA.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"StarCoder is a decoder-only transformer model optimized for programming tasks. The original released model has ~15.5 billion parameters and was trained by the BigCode community on The Stack — a large, permissively-licensed corpus of GitHub code — with heavy filtering and preprocessing. StarCoder offers a long context window (commonly released with an 8k token context), strong multilingual code support, and capabilities for code completion, infilling, translation between languages, and code-aware question answering.\n\nThe project emphasizes responsible open-source release practices (dataset opt-outs, PII redaction tools, attribution tracing) and is distributed under an OpenRAIL-style usage license on Hugging Face. The model can be run locally (with appropriate hardware) or served via inference runtimes (transformers, text-generation-inference, Docker containers, or community tools like Ollama)."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""},{"type":"item","level":3,"content":"80+ programming languages (Python, JavaScript, Java, C/C++, Go, Ruby, Rust, TypeScript, PHP, Shell, SQL, etc.)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Key strengths: open-source, strong code performance for many languages, long context handling (8k tokens), and community-driven tooling.","plainChildren":""},{"type":"item","level":3,"content":"Training data: The Stack (curated permissively-licensed GitHub code); BigCode published data curation and opt-out tooling.","plainChildren":""},{"type":"item","level":3,"content":"Privacy & safety: authors provided a PII redaction pipeline and attribution tracing to help source provenance and mitigate leakage risks.","plainChildren":""},{"type":"item","level":3,"content":"Variants/evolution: StarCoder followed by StarCoder2 family (further improvements and language coverage in later releases).","plainChildren":""},{"type":"item","level":3,"content":"Typical uses: editor completions, code generation from docstrings, refactoring assistance, automated code reviews, and local/offline deployments for privacy-sensitive environments.","plainChildren":""},{"type":"item","level":3,"content":"Limitations: may reproduce licensed or low-quality snippets from training data; users should validate generated code for correctness, security, and licensing implications.","plainChildren":""},{"type":"item","level":3,"content":"Integration tips: use temperature/top-p tuning for generation quality, provide clear prompts (function signatures, tests) for best results, and prefer fp16/bf16 runtime on GPU for performance.","plainChildren":""},{"type":"item","level":3,"content":"2025-2026 Update: StarCoder2 (February 2024) is latest major release with 3B/7B/15B variants trained on 4.3T tokens across 619 languages (330% more training data). Context doubled to 16K tokens with Grouped Query Attention. StarCoder2-15B-Instruct achieves 72.6% HumanEval (surpasses CodeLlama-70B-Instruct). Collaboration between Hugging Face, ServiceNow, and NVIDIA. ServiceNow reported 52% developer productivity increase. Stanford Transparency Index: 85/100 for 2024. Integrated into Theia IDE for AI auto-completion (January 2025). BigCodeBench evaluated at ICLR'25. Active development continues with starcoder.cpp and evaluation harness.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2026-01-30","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Other","plainChildren":""},{"type":"item","level":3,"content":"BigCode-OpenRAIL / OpenRAIL-M (model card requires agreement on use)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Free Trial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"N/A","plainChildren":""},{"type":"item","level":3,"content":"Open-source model: weights and checkpoints are freely available (subject to accepting the model card / license on Hugging Face), not a time-limited trial.","plainChildren":""}]}]}]}],"sourcePath":"starcoder.md"}]