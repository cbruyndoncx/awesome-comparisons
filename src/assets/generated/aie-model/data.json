[{"type":"header","level":1,"content":"Claude Haiku 4.5 - https://www.anthropic.com/claude/haiku","children":[{"type":"text","content":"Claude Haiku 4.5 delivers similar levels of coding performance to Claude Sonnet 4 but at one-third the cost and more than twice the speed. Released October 15, 2025.\n\n**Dataset ID:** aie-model"},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"4.5 (October 15, 2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"SWE-bench Verified: 73.3%","plainChildren":""},{"type":"item","level":3,"content":"Augment agentic coding evaluation: 90% of Sonnet 4.5's performance","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"text","content":"Anthropic's fast, cost-effective coding model that achieves Sonnet 4-level performance at one-third the cost and 2x+ the speed."}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Claude Haiku 4.5 was released on October 15, 2025, as Anthropic's latest compact yet powerful coding model. It delivers similar levels of coding performance to Claude Sonnet 4 while being one-third the cost and more than twice the speed.\n\nWith a score of 73.3% on SWE-bench Verified, it ranks as one of the world's best coding models. In Augment's agentic coding evaluation, Haiku 4.5 achieves 90% of Sonnet 4.5's performance while matching much larger models in capability.\n\nThe model is designed to work in tandem with Claude Sonnet 4.5, which remains the frontier model and best overall coding model. A recommended pattern is for Sonnet 4.5 to break down complex problems into multi-step plans, then orchestrate a team of multiple Haiku 4.5 instances to complete subtasks in parallel.\n\nUsers of Claude Code find that Haiku 4.5 makes the coding experience—from multiple-agent projects to rapid prototyping—markedly more responsive. The model is available on the API, Amazon Bedrock, Google Cloud's Vertex AI, and is rolling out in GitHub Copilot for Pro, Pro+, Business, and Enterprise tiers."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Release: October 15, 2025","plainChildren":""},{"type":"item","level":3,"content":"Performance: 73.3% SWE-bench Verified, 90% of Sonnet 4.5 in agentic coding","plainChildren":""},{"type":"item","level":3,"content":"Cost: One-third the cost of Sonnet 4","plainChildren":""},{"type":"item","level":3,"content":"Speed: More than 2x faster than Sonnet 4","plainChildren":""},{"type":"item","level":3,"content":"Strategic Use: Designed for parallel subtask execution orchestrated by Sonnet 4.5","plainChildren":""},{"type":"item","level":3,"content":"Availability: API, Amazon Bedrock, Vertex AI, GitHub Copilot","plainChildren":""},{"type":"item","level":3,"content":"Claude Code: Enhanced responsiveness for multi-agent projects and prototyping","plainChildren":""},{"type":"item","level":3,"content":"Developer: Anthropic","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2025-11-18","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":3,"content":"FreeTrial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""},{"type":"item","level":3,"content":"Available in GitHub Copilot public preview","plainChildren":""}]}]}]}],"sourcePath":"claude-haiku-4-5.md"},{"type":"header","level":1,"content":"Claude Sonnet 4.5 - https://www.anthropic.com/claude/sonnet","children":[{"type":"text","content":"Anthropic's best coding model achieving state-of-the-art on SWE-bench Verified. Released September 29, 2025, capable of 30 hours of autonomous coding with 0% error rate on code editing benchmarks."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"4.5 (2025-09-29)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"State-of-the-art coding model that can autonomously code for 30 hours, build applications, manage databases, purchase domains, and perform security audits with exceptional editing accuracy.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Claude Sonnet 4.5 is Anthropic's best coding model, released on September 29, 2025, achieving state-of-the-art performance on the SWE-bench Verified evaluation for real-world software coding abilities. The model can run autonomously for up to 30 hours—a significant leap from Claude Opus 4's seven hours—during which it can build complete applications, stand up database services, purchase domain names, and perform SOC 2 security audits. Most remarkably, Claude Sonnet 4.5's code editing capabilities improved from Sonnet 4's 9% error rate to 0% on Anthropic's internal benchmarks. Despite being smaller than Claude Opus 4.1, it outperforms it in \"almost every single way\" according to Anthropic's chief product officer. Available at the same pricing as Sonnet 4 ($3/$15 per million tokens) and integrated into major coding tools including GitHub Copilot, Cursor, and Windsurf."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Best coding model: State-of-the-art on SWE-bench Verified, described as \"probably the best coding model in the world.\"","plainChildren":""},{"type":"item","level":3,"content":"30-hour autonomy: Can code autonomously for 30 hours vs 7 hours for Claude Opus 4.","plainChildren":""},{"type":"item","level":3,"content":"Perfect editing: 0% error rate on internal code editing benchmark, down from 9% on Sonnet 4.","plainChildren":""},{"type":"item","level":3,"content":"Full-stack capabilities: Builds apps, manages databases, purchases domains, performs security audits.","plainChildren":""},{"type":"item","level":3,"content":"Smaller but smarter: Outperforms larger Claude Opus 4.1 in \"almost every single way.\"","plainChildren":""},{"type":"item","level":3,"content":"SOC 2 security: Can autonomously perform security audits to ensure product compliance.","plainChildren":""},{"type":"item","level":3,"content":"Same pricing: $3 per million input tokens, $15 per million output tokens (same as Sonnet 4).","plainChildren":""},{"type":"item","level":3,"content":"Wide integration: Available in GitHub Copilot, Cursor, Windsurf, and other major tools.","plainChildren":""},{"type":"item","level":3,"content":"Anthropic: Leading AI safety-focused company founded by former OpenAI researchers.","plainChildren":""},{"type":"item","level":3,"content":"Released September 29, 2025 as a major advancement in AI coding.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2025-09-29","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":3,"content":"FreeTrial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"claude-sonnet-4-5.md"},{"type":"header","level":1,"content":"CodeGeeX - https://github.com/THUDM/CodeGeeX","children":[{"type":"text","content":"Open-source multilingual code generation model from Tsinghua University (THUDM)."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"CodeGeeX v1 (13B parameters, September 2022)","plainChildren":""},{"type":"item","level":3,"content":"CodeGeeX2 (follow-up release, 2023-07-24)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/THUDM/CodeGeeX","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"N/A","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Multilingual large-scale code generation model (13B) from Tsinghua University (THUDM) for code generation, translation, completion, summarization, and IDE integration.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"CodeGeeX is a large-scale multilingual code generation model and toolkit developed by THUDM. It was trained on a massive corpus covering source code and natural language across many programming languages to support code generation, translation (cross‑language), completion, summarization, and explanation. The project provides model checkpoints (for research use), inference scripts, and IDE integrations (for example, a VS Code extension)."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Trained at large scale (reported training on 850B+ tokens and large TPU/Ascend clusters in original publications) and evaluated with a multilingual HumanEval-X benchmark.","plainChildren":""},{"type":"item","level":3,"content":"Provides cross-language code translation and multilingual code generation capabilities; reported strong performance compared to contemporaneous open models.","plainChildren":""},{"type":"item","level":3,"content":"IDE integrations (VS Code, JetBrains) exist to make the model usable as a coding assistant; downstream usage may be subject to the model weights' licensing terms.","plainChildren":""},{"type":"item","level":3,"content":"For commercial deployment, review the repository's instructions and registration process for obtaining the model weights.","plainChildren":""},{"type":"item","level":3,"content":"See also: CodeGeeX2 (follow-up) and related THUDM releases which may have differing licenses or access requirements.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2024-07-24","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Code: Apache-2.0","plainChildren":""},{"type":"item","level":3,"content":"Model weights: released for academic research; commercial use requires application/approval (see repository for details)","plainChildren":""}]}]},{"type":"header","level":3,"content":"FreeTrial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""},{"type":"item","level":3,"content":"Model code is open-source; weights are available for academic research. Commercial use of weights typically requires registration/approval per THUDM's model license terms.","plainChildren":""}]}]}]}],"sourcePath":"codegeex.md"},{"type":"header","level":1,"content":"Codeium Enterprise - https://codeium.com/","children":[{"type":"text","content":"Secure, enterprise-grade AI coding assistant with on-prem and air-gapped deployment options"},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Agent","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"text","content":"n/a"}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/exafunction/codeium.el","plainChildren":""},{"type":"item","level":3,"content":"https://github.com/exafunction/codeium-react-code-editor","plainChildren":""},{"type":"item","level":3,"content":"Note: The core Codeium Enterprise server and model infrastructure are proprietary; no public GitHub repo for the full enterprise/server distribution is published by the vendor.","plainChildren":""}]},{"type":"text","content":"<!-- Associated Github repositories (public components) -->"}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"[4] Strong enterprise deployment options (air-gapped, self-hosted VPC)","plainChildren":""},{"type":"item","level":3,"content":"[4] Broad language & IDE support (70+ languages, major IDEs)","plainChildren":""},{"type":"item","level":3,"content":"[3] Proprietary product — some key compliance details (e.g. BYOK) require vendor confirmation","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"text","content":"Commercial enterprise edition of Codeium providing SaaS, self-hosted VPC, and fully air-gapped on-prem deployments with enterprise security controls, audit logging, admin analytics, and private-code personalization."}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Codeium Enterprise is the commercial, enterprise-oriented edition of Codeium that focuses on security, privacy, and deployment flexibility for organizations. It supports SaaS, self-hosted VPC, and fully air-gapped on-premises deployments, enabling customers to keep code and model inference inside their network perimeter while delivering AI-assisted code completion, multi-file editing, and contextual suggestions informed by private codebases."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Deployments: Offers SaaS, self-hosted VPC, and fully air-gapped on-premises deployment models to meet strict data-sovereignty and compliance needs. Typical enterprise installations use containerized deployments (Helm/Kubernetes or Docker Compose) and can be run on infrastructure with GPU acceleration where required.","plainChildren":""},{"type":"item","level":3,"content":"Security: Enterprise features include end-to-end encryption, indexing access controls, audit logging, and administrative analytics. Air-gapped deployments ensure no code or telemetry leaves the customer environment.","plainChildren":""},{"type":"item","level":3,"content":"Platform & infra: Enterprise installs may require NVIDIA drivers and the NVIDIA Container Toolkit for GPU-accelerated deployments and access to enterprise container images (licensed by Codeium).","plainChildren":""},{"type":"item","level":3,"content":"Features: Personalization/finetuning on private codebases, subteam analytics, audit trails, priority enterprise support (dedicated Slack channels / support portal), and one-click installers for simplified deployment.","plainChildren":""},{"type":"item","level":3,"content":"Impact: Vendor materials and case studies report developer productivity gains (reduced PR cycle time, faster debugging and testing) when teams adopt the enterprise product.","plainChildren":""},{"type":"item","level":3,"content":"Gaps / vendor follow-up: Public-facing documentation does not clearly document Bring-Your-Own-Key (BYOK) key management options or the exact key management integration workflow. Organizations with strict KMS/BYOK requirements should request detailed security architecture and KMS integration docs from Codeium sales/enterprise support.","plainChildren":""}]},{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://codeium.com/","plainChildren":""},{"type":"item","level":3,"content":"https://codeium.com/enterprise (vendor enterprise overview)","plainChildren":""}]},{"type":"text","content":"References & further reading"}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2025-11-16","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":3,"content":"FreeTrial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"codeium_enterprise.md"},{"type":"header","level":1,"content":"CodeLlama - https://ai.meta.com/blog/codegen-meta-code-llama/","children":[{"type":"text","content":"Code Llama is Meta's open-source family of large language models optimized for code generation, completion, and reasoning about code."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"text","content":"v1 (2023-08)"}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/meta-llama/codellama","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"[4] Strong open-source code generation model family with multiple sizes","plainChildren":""},{"type":"item","level":3,"content":"[3] Community ecosystem and tooling matured but still behind some proprietary offerings","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"text","content":"Code Llama is Meta's open-source family of LLMs optimized for code generation, completion, and debugging. It provides multiple model sizes (7B, 13B, 34B, 70B) and specialized variants (Instruct, Python) for different coding tasks and deployment constraints."}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Code Llama is Meta's open-source family of large language models optimized for code generation, completion, and reasoning about code. It ships in multiple sizes (7B, 13B, 34B and larger variants) and in specialized flavors such as Code Llama-Instruct (instruction-tuned) and Code Llama-Python (further fine-tuned on Python). The models use a decoder-only transformer architecture with optimizations tuned for code tasks and support fill-in-the-middle style completions and larger context windows than many older public models."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Variants: Code Llama-Instruct (better at following natural-language prompts) and Code Llama-Python (additional Python fine-tuning).","plainChildren":""},{"type":"item","level":3,"content":"Sizes: commonly available in 7B, 13B, 34B; larger checkpoints and tuned variants exist depending on releases.","plainChildren":""},{"type":"item","level":3,"content":"Context window: the official models are released with substantially larger context windows (commonly 16k tokens for code-focused variants); deployment runtimes and custom forks may offer extended context support.","plainChildren":""},{"type":"item","level":3,"content":"Deployment: Widely available through Hugging Face, community containers, and local runtimes (Ollama, private inference servers).","plainChildren":""},{"type":"item","level":3,"content":"Strengths: open-source, good quality for code tasks, multiple sizes for trade-offs between latency and capability.","plainChildren":""},{"type":"item","level":3,"content":"Limitations: still requires careful prompt engineering for complex multi-file project reasoning; ecosystem tooling (IDE/product integrations) is smaller than some commercial competitors but growing quickly.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Initial release: August 24, 2023","plainChildren":""},{"type":"item","level":3,"content":"70B release: January 29, 2024","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Meta / Code Llama license (permits research and commercial use — see repository for exact terms)","plainChildren":""}]}]},{"type":"header","level":3,"content":"FreeTrial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"N/A","plainChildren":""},{"type":"item","level":3,"content":"Open-source model: weights freely available for download for research and commercial use under Meta's license terms, not a time-limited trial.","plainChildren":""}]}]}]}],"sourcePath":"codellama.md"},{"type":"header","level":1,"content":"DeepSeek V3 - https://github.com/deepseek-ai/DeepSeek-V3","children":[{"type":"text","content":"Advanced 671B parameter Mixture-of-Experts model excelling in mathematics and coding. V3.1 and V3.2 updates in 2025 added enhanced reasoning and sparse attention for long contexts."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"V3.2-Exp (2025-09)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/deepseek-ai/DeepSeek-V3","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"671B parameter MoE model (37B activated) with exceptional math and coding performance, featuring V3.1 hybrid reasoning and V3.2 sparse attention for 128K contexts.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"DeepSeek-V3 is a 671-billion parameter Mixture-of-Experts model that achieves state-of-the-art performance in mathematics and coding tasks. The model family includes specialized variants: V3 (December 2024 baseline), V3.1 (August 2025) which combines the strengths of V3 and R1 into a hybrid model with enhanced reasoning, and V3.2-Exp (September 2025) featuring DeepSeek Sparse Attention (DSA) for improved long-context handling up to 128K tokens. Available through GitHub Models and various deployment platforms, DeepSeek V3 is particularly suited for solving advanced math problems and generating complex code, making it a top choice for technical and scientific applications."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"671B total parameters: Massive MoE architecture with 37B activated parameters.","plainChildren":""},{"type":"item","level":3,"content":"Exceptional math and coding: Best performance on most technical benchmarks.","plainChildren":""},{"type":"item","level":3,"content":"V3.1 hybrid model (August 2025): Combines V3 and R1 strengths with enhanced thinking mode for coding and math.","plainChildren":""},{"type":"item","level":3,"content":"V3.2-Exp (September 2025): Experimental version with DeepSeek Sparse Attention for long contexts.","plainChildren":""},{"type":"item","level":3,"content":"128K context length: Supports extended context windows for complex tasks.","plainChildren":""},{"type":"item","level":3,"content":"GitHub Models integration: Now generally available in GitHub Models platform.","plainChildren":""},{"type":"item","level":3,"content":"DeepSeek-Coder series: Separate specialized line (1B to 33B) trained on 87% code, 13% natural language.","plainChildren":""},{"type":"item","level":3,"content":"2T token training: DeepSeek-Coder models pre-trained on 2 trillion tokens.","plainChildren":""},{"type":"item","level":3,"content":"Active development: Continuous improvements with V3.1 and V3.2 releases in 2025.","plainChildren":""},{"type":"item","level":3,"content":"Open source: Available on GitHub with full model weights and documentation.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2025-09","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"MIT","plainChildren":""}]}]},{"type":"header","level":3,"content":"FreeTrial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"deepseek-v3.md"},{"type":"header","level":1,"content":"FauxPilot - https://github.com/fauxpilot/fauxpilot","children":[{"type":"text","content":"[Open-source, locally-hosted code-completion server that provides a privacy-focused alternative to cloud-based assistants like GitHub Copilot.]"},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Infrastructure","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""},{"type":"item","level":3,"content":"No formal releases/tags (GitHub 'releases' list is empty; repository does not provide official release version numbers)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/fauxpilot/fauxpilot","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"N/A","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Open-source, locally-hosted code-completion server providing a privacy-focused alternative to cloud-based assistants like GitHub Copilot.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"FauxPilot is an open-source code-completion server designed to run on-premises or on private infrastructure so that source code and telemetry do not need to be sent to a third-party cloud service. It provides an OpenAI-compatible API surface and integrations that let editors and tools use it in place of cloud assistants. The project is focused on privacy, local deployment, and model flexibility: it supports running models (notably Salesforce CodeGen variants) inside NVIDIA's Triton Inference Server with the FasterTransformer backend and can split large models across multiple GPUs."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Model support: commonly used with Salesforce CodeGen models converted for FasterTransformer / Triton; models are typically downloaded from Hugging Face and converted during setup.","plainChildren":""},{"type":"item","level":3,"content":"Hardware: requires an NVIDIA GPU (compute capability >= 6.0) and sufficient VRAM for the chosen model. VRAM can be aggregated across multiple GPUs for larger models.","plainChildren":""},{"type":"item","level":3,"content":"Installation: requires Docker, docker-compose (>= 1.28), nvidia-docker (nvidia-container-toolkit), curl and zstd for model download/extraction. A setup script helps choose and prepare a model.","plainChildren":""},{"type":"item","level":3,"content":"Integrations: offers OpenAI API compatibility, REST endpoints, and Copilot-plugin style integrations so it can be used with existing editor tooling.","plainChildren":""},{"type":"item","level":3,"content":"Privacy: primary selling point is that all inference can be run locally so developer code does not leave the network and no external telemetry is required.","plainChildren":""},{"type":"item","level":3,"content":"Support: community-driven project; documentation is community-maintained (wiki, discussion forum). There is no formal commercial support or warranty.","plainChildren":""},{"type":"item","level":3,"content":"Common pitfalls: accurate VRAM estimation is critical; ensure nvidia-docker and drivers are correctly installed and that the chosen model fits available GPU memory (or configure model sharding across GPUs).","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2024-04-09","plainChildren":""},{"type":"item","level":3,"content":"Last push timestamp: 2024-04-09T08:42:23Z (from GitHub API)","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"MIT","plainChildren":""}]}]},{"type":"header","level":3,"content":"FreeTrial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"N/A","plainChildren":""},{"type":"item","level":3,"content":"Open-source software: freely available under MIT license, not a time-limited trial.","plainChildren":""}]}]}]}],"sourcePath":"fauxpilot.md"},{"type":"header","level":1,"content":"Gemini 2.5 - https://ai.google.dev/gemini-api/docs/models","children":[{"type":"text","content":"Google's most advanced model family with thinking capabilities, featuring exceptional coding performance. Released March 2025 with 2.5 Pro achieving 63.8% on SWE-bench Verified and leading LiveCodeBench."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2.5 (2025-03)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Google's advanced thinking model family with 2.5 Pro for complex coding tasks and 2.5 Flash for efficient workhorse operations, featuring 1M token context and native tool use.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Gemini 2.5 is Google DeepMind's most advanced model family announced in March 2025, featuring thinking models capable of reasoning through their thoughts before responding. The flagship 2.5 Pro excels at creating visually compelling web apps, agentic code applications, and code transformation/editing, achieving 63.8% on SWE-bench Verified and leading LiveCodeBench for competition-level coding. Gemini 2.5 Flash is the most efficient workhorse model, improved across key benchmarks for reasoning, multimodality, code, and long context while using 20-30% fewer tokens. Both models feature superior speed, native tool use, and 1M token context windows. Building on Gemini 2.0 Flash (February 2025 general availability), the 2.5 family represents Google's commitment to developer-focused coding capabilities."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Exceptional coding: 2.5 Pro achieves 63.8% on SWE-bench Verified, leads LiveCodeBench for competition coding.","plainChildren":""},{"type":"item","level":3,"content":"Thinking models: Capable of reasoning through thoughts before responding for enhanced accuracy.","plainChildren":""},{"type":"item","level":3,"content":"2.5 Pro: Most advanced for complex tasks, web app creation, and agentic code applications.","plainChildren":""},{"type":"item","level":3,"content":"2.5 Flash: Most efficient workhorse, 20-30% fewer tokens while improving benchmarks.","plainChildren":""},{"type":"item","level":3,"content":"1M token context: Extended context window for handling large codebases and documents.","plainChildren":""},{"type":"item","level":3,"content":"Native tool use: Built-in capabilities for using tools and APIs directly.","plainChildren":""},{"type":"item","level":3,"content":"Multimodal: Strong visual perception and image analysis alongside code capabilities.","plainChildren":""},{"type":"item","level":3,"content":"Code transformation: Excels at editing and transforming existing codebases.","plainChildren":""},{"type":"item","level":3,"content":"Real-time generation: Direct generation of HTML, CSS, JS within conversations.","plainChildren":""},{"type":"item","level":3,"content":"Google DeepMind: Developed by Google's premier AI research division.","plainChildren":""},{"type":"item","level":3,"content":"Released March 2025 with continuous improvements through 2025.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2025-09","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":3,"content":"FreeTrial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"gemini-2-5.md"},{"type":"header","level":1,"content":"Gemini 3 - https://blog.google/technology/google-labs/","children":[{"type":"text","content":"Gemini 3 is Google's latest and most advanced foundation model, released November 18, 2025, featuring state-of-the-art reasoning capabilities, record benchmark scores, and described as Google's \"best vibe coding model ever.\"\n\n**Dataset ID:** aie-model"},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"3.0 (November 18, 2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"SWE-bench Verified: 76.2%","plainChildren":""},{"type":"item","level":3,"content":"Terminal-Bench 2.0: 54.2%","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"text","content":"Google's most advanced foundation model with state-of-the-art coding and reasoning capabilities, record benchmark performance, and multimodal vibe coding features."}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Gemini 3 is Google's latest foundation model, released on November 18, 2025, representing a significant advancement in AI capabilities. The model features enhanced reasoning, agentic workflows, and exceptional coding performance, surpassing previous versions across all major benchmarks.\n\nGemini 3 Pro achieves 76.2% on SWE-bench Verified, a benchmark measuring coding agents' capabilities, greatly outperforming Gemini 2.5 Pro. On Terminal-Bench 2.0, which tests a model's tool use ability to operate a computer via terminal, Gemini 3 scores 54.2%.\n\nThe model introduces \"multimodal vibe coding,\" allowing users to visually instruct the model to make changes in code. This capability is available through multiple interfaces including AI Studio, Vertex AI, the Gemini Command Line Interface tool, and third-party developer tools such as Cursor, GitHub, JetBrains, Replit, and Manus.\n\nAlongside the base model, Google released Google Antigravity, a Gemini-powered coding interface allowing multi-pane agentic coding similar to agentic IDEs like Warp or Cursor 2.0. The public preview is available at no charge for macOS, Windows, and Linux.\n\nGemini 3 is now immediately available through the Gemini app and AI search interface, with access worldwide where Gemini models are available."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Release: November 18, 2025","plainChildren":""},{"type":"item","level":3,"content":"Benchmarks:","plainChildren":""},{"type":"item","level":3,"content":"SWE-bench Verified: 76.2%","plainChildren":""},{"type":"item","level":3,"content":"Terminal-Bench 2.0: 54.2%","plainChildren":""},{"type":"item","level":3,"content":"Key Feature: Multimodal vibe coding (visual code instruction)","plainChildren":""},{"type":"item","level":3,"content":"Google Antigravity: Multi-pane agentic coding interface (free public preview)","plainChildren":""},{"type":"item","level":3,"content":"Platforms: AI Studio, Vertex AI, Gemini CLI, third-party tools","plainChildren":""},{"type":"item","level":3,"content":"Integrations: Cursor, GitHub, JetBrains, Replit, Manus","plainChildren":""},{"type":"item","level":3,"content":"Availability: Worldwide where Gemini models are accessible","plainChildren":""},{"type":"item","level":3,"content":"Developer: Google DeepMind","plainChildren":""},{"type":"item","level":3,"content":"Special Capabilities: Advanced reasoning, agentic workflows, generative UI responses","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2025-11-18","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":3,"content":"FreeTrial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""},{"type":"item","level":3,"content":"Available through Gemini app and AI Studio","plainChildren":""},{"type":"item","level":3,"content":"Google Antigravity available at no charge in public preview","plainChildren":""}]}]}]}],"sourcePath":"gemini-3.md"},{"type":"header","level":1,"content":"GLM-4.5 - https://huggingface.co/zai-org/GLM-4.5","children":[{"type":"text","content":"Open-source model family from Zhipu AI (Z.ai) unifying reasoning, coding, agentic abilities, and vision. Released August 2025, achieving 64.2% on SWE-bench Verified, surpassing GPT-4.1 and Claude 4 Opus."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"4.5 (2025-08)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://huggingface.co/zai-org/GLM-4.5","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Open-source MoE model family with 355B total parameters (32B active) featuring exceptional coding performance, achieving 64.2% on SWE-bench and 80.8% win rate against Qwen3 Coder in real-world challenges.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"GLM-4.5 is an open-source large language model series from Zhipu AI (Z.ai) designed to unify advanced reasoning, coding, agentic abilities, and vision in a single powerful framework. Released in August 2025, the flagship GLM-4.5 model features 355 billion total parameters with 32 billion active parameters, while GLM-4.5-Air offers a more compact design with 106 billion total parameters and 12 billion active parameters. The series demonstrates exceptional coding performance with 64.2% on SWE-bench Verified, surpassing GPT-4.1 (48.6%) and achieving an 80.8% win rate against Qwen3 Coder in real-world coding challenges. The model runs efficiently on just eight Nvidia H20 GPUs—half the hardware of comparable models. Available variants include GLM-4-Flash, GLM-4-FlashX, GLM-4-Plus, GLM-4-Long, GLM-4-Air, and GLM-4-AirX, all MIT-licensed for commercial use."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Exceptional coding performance: 64.2% on SWE-bench Verified, 37.5% on TerminalBench, beats GPT-4.1 and Claude 4 Opus.","plainChildren":""},{"type":"item","level":3,"content":"Real-world coding wins: 80.8% win rate against Qwen3 Coder in practical coding challenges.","plainChildren":""},{"type":"item","level":3,"content":"MoE architecture: GLM-4.5 has 355B total parameters (32B active), GLM-4.5-Air has 106B total (12B active).","plainChildren":""},{"type":"item","level":3,"content":"Hardware efficient: Runs on just 8x Nvidia H20 GPUs, half the requirement of comparable models.","plainChildren":""},{"type":"item","level":3,"content":"Real-time code generation: Direct generation of HTML, CSS, JS, and SVG within conversations.","plainChildren":""},{"type":"item","level":3,"content":"Agent capabilities: GLM-4-32B variant enhanced for tool usage, web search, and code generation.","plainChildren":""},{"type":"item","level":3,"content":"Multiple variants: Flash, FlashX, Plus, Long, Air, and AirX versions for different use cases.","plainChildren":""},{"type":"item","level":3,"content":"Free tier available: GLM-4-Flash and GLM-4.5-Flash optimized for free coding and reasoning.","plainChildren":""},{"type":"item","level":3,"content":"MIT licensed: Open source with full commercial use rights, self-hosting, and custom training.","plainChildren":""},{"type":"item","level":3,"content":"Z.ai/Zhipu AI: Developed by Chinese AI startup, CEO Zhang Peng.","plainChildren":""},{"type":"item","level":3,"content":"Released August 2025 with strong benchmark results.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2025-08","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"MIT","plainChildren":""}]}]},{"type":"header","level":3,"content":"FreeTrial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"glm-4-5.md"},{"type":"header","level":1,"content":"GLM-4.6 - https://huggingface.co/zai-org/GLM-4.6","children":[{"type":"text","content":"GLM-4.6 is Zhipu AI's advanced coding model featuring a 355B-parameter Mixture of Experts (MoE) architecture with enhanced real-world coding, long-context processing, reasoning, and agentic AI capabilities.\n\n**Dataset ID:** aie-model"},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"4.6 (September 30, 2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://huggingface.co/zai-org/GLM-4.6","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"LiveCodeBench v6: 82.8% (up from 63.3% in GLM-4.5)","plainChildren":""},{"type":"item","level":3,"content":"CC-Bench: 48.6% win rate vs Claude Sonnet 4 (near-parity)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"text","content":"Open-weight 355B-parameter MoE model from Zhipu AI with enhanced coding capabilities, 200K context window, and competitive performance against Claude Sonnet 4."}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"GLM-4.6 was released by Zhipu AI on September 30, 2025, as the latest iteration in the GLM series. The model features a 355B-parameter Mixture of Experts (MoE) architecture designed with a focus on advanced agentic capabilities, reasoning, and coding performance.\n\nThe model shows significant improvements over GLM-4.5 across eight public benchmarks, with 15% token efficiency improvements and enhanced real-world coding performance. The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks and larger codebases.\n\nOn LiveCodeBench v6 (which involves writing, executing, and debugging code across languages), GLM-4.6 achieved 82.8%, a substantial jump from GLM-4.5's 63.3%. In real-world coding tests (CC-Bench), it achieves near-parity with Claude Sonnet 4 (48.6% win rate). However, Zhipu AI acknowledges that it still lags behind Claude Sonnet 4.5 in coding ability.\n\nGLM-4.6 is available via Z.ai API and OpenRouter, integrates with popular coding agents (Claude Code, Cline, Roo Code, Kilo Code), and supports local serving via vLLM and SGLang. Zhipu AI offers GLM Coding Plan, a subscription package specifically designed for AI-powered coding, available for $3 per month. As of late 2025, it is the leading open-source solution for coding tasks."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Architecture: 355B-parameter MoE with BF16/F32 tensors","plainChildren":""},{"type":"item","level":3,"content":"Context Window: 200K tokens (expanded from 128K in GLM-4.5)","plainChildren":""},{"type":"item","level":3,"content":"License: MIT (open-weight model)","plainChildren":""},{"type":"item","level":3,"content":"Performance Improvements: 15% token efficiency, 82.8% LiveCodeBench v6","plainChildren":""},{"type":"item","level":3,"content":"Benchmarks: Near-parity with Claude Sonnet 4, behind Sonnet 4.5","plainChildren":""},{"type":"item","level":3,"content":"Pricing: GLM Coding Plan at $3/month","plainChildren":""},{"type":"item","level":3,"content":"Availability: Z.ai API, OpenRouter, Hugging Face, ModelScope","plainChildren":""},{"type":"item","level":3,"content":"Inference: vLLM and SGLang support for local serving","plainChildren":""},{"type":"item","level":3,"content":"Integration: Claude Code, Cline, Roo Code, Kilo Code","plainChildren":""},{"type":"item","level":3,"content":"Developer: Zhipu AI","plainChildren":""},{"type":"item","level":3,"content":"Release: September 30, 2025","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2025-11-18","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""},{"type":"item","level":3,"content":"Open-weight model","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"MIT","plainChildren":""}]}]}]}],"sourcePath":"glm-4-6.md"},{"type":"header","level":1,"content":"GPT-5 - https://openai.com/index/introducing-gpt-5/","children":[{"type":"text","content":"OpenAI's most advanced model described as \"PhD-level expert in your pocket.\" Released August 7, 2025, leading SWE-bench Verified at 74.9% and achieving 88% on Aider Polyglot for real-world coding."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"5.0 (2025-08-07)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"OpenAI's flagship model achieving state-of-the-art 74.9% on SWE-bench Verified and 88% on Aider Polyglot, featuring significant improvements in math, coding, visual perception, and health analysis.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"GPT-5 is OpenAI's most advanced language model released on August 7, 2025, described as a \"PhD-level expert in your pocket\" capable of tackling everything from code and math to health advice and image analysis. The model leads the industry on SWE-bench Verified with 74.9% accuracy, ahead of o3's 69.1%, and achieves an impressive 88% on Aider Polyglot for real-world coding tasks. GPT-5 represents a significant leap forward in capabilities, particularly in mathematics, coding, visual perception, and health domains. Alongside GPT-5, OpenAI released the o-series models (o3 and o4-mini) trained to think longer before responding, and GPT-4.1 which excels at following user instructions and formatting requirements. The model family continues OpenAI's tradition of pushing the boundaries of AI capabilities while maintaining practical applicability."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Industry-leading coding: 74.9% on SWE-bench Verified (highest benchmark score), 88% on Aider Polyglot.","plainChildren":""},{"type":"item","level":3,"content":"PhD-level expertise: Described as \"PhD-level expert\" across multiple domains.","plainChildren":""},{"type":"item","level":3,"content":"Multimodal excellence: Superior visual perception, image analysis, and multi-modal reasoning.","plainChildren":""},{"type":"item","level":3,"content":"Math and health: Significant improvements in mathematical reasoning and health advice capabilities.","plainChildren":""},{"type":"item","level":3,"content":"o-series reasoning: Complemented by o3 and o4-mini models that think longer for better reasoning.","plainChildren":""},{"type":"item","level":3,"content":"GPT-4.1 precision: Parallel release excels at format compliance (JSON, XML) and instruction following.","plainChildren":""},{"type":"item","level":3,"content":"Real-world performance: 88% on Aider Polyglot demonstrates practical coding effectiveness.","plainChildren":""},{"type":"item","level":3,"content":"Multi-step tasks: Enhanced capabilities for complex, multi-step problem solving.","plainChildren":""},{"type":"item","level":3,"content":"OpenAI flagship: Latest advancement from the creators of ChatGPT and GPT-4.","plainChildren":""},{"type":"item","level":3,"content":"Released August 7, 2025 with immediate availability.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2025-08-07","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":3,"content":"FreeTrial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"gpt-5.md"},{"type":"header","level":1,"content":"Grok 4 - https://x.ai/grok","children":[{"type":"text","content":"xAI's flagship AI model described as \"the most intelligent model in the world.\" Released July 9, 2025, featuring native tool use, real-time search, and specialized coding capabilities through Grok Code Fast 1."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"4.0 (2025-07-09)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"xAI's most advanced model trained on 200,000 GPU Colossus cluster with reinforcement learning at pretraining scale, featuring native tool use and specialized coding variant Grok Code Fast 1.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Grok 4 is xAI's flagship AI model released on July 9, 2025, described as \"the most intelligent model in the world.\" Trained using Colossus, xAI's 200,000 GPU cluster, Grok 4 runs reinforcement learning at pretraining scale to refine its reasoning abilities. The model features native tool use and real-time search integration, making it particularly effective for dynamic information retrieval and complex task execution. On August 28, 2025, xAI released Grok Code Fast 1, a specialized coding variant that excels at agentic coding, initially offered free on GitHub Copilot, Cursor, Cline, Roo Code, Kilo Code, opencode, and Windsurf. Grok 3 (the predecessor) achieved 79.4-80.4% on LiveCodeBench, demonstrating strong coding capabilities inherited by Grok 4."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Most intelligent model claim: xAI describes Grok 4 as \"the most intelligent model in the world\" at launch.","plainChildren":""},{"type":"item","level":3,"content":"Colossus training: Trained on 200,000 GPU cluster using reinforcement learning at pretraining scale.","plainChildren":""},{"type":"item","level":3,"content":"Native tool use: Built-in capabilities for using tools and APIs directly.","plainChildren":""},{"type":"item","level":3,"content":"Real-time search: Integrated real-time web search for up-to-date information.","plainChildren":""},{"type":"item","level":3,"content":"Grok Code Fast 1: Specialized coding model (August 2025) optimized for agentic coding workflows.","plainChildren":""},{"type":"item","level":3,"content":"Free initial access: Grok Code Fast 1 offered free on launch partners including GitHub Copilot and Cursor.","plainChildren":""},{"type":"item","level":3,"content":"Strong coding lineage: Grok 3 achieved 79.4-80.4% on LiveCodeBench, foundation for Grok 4.","plainChildren":""},{"type":"item","level":3,"content":"10x training increase: Grok 3 used 10x more compute than Grok 2, continuing scaling trajectory.","plainChildren":""},{"type":"item","level":3,"content":"xAI: Founded and led by Elon Musk.","plainChildren":""},{"type":"item","level":3,"content":"Released July 9, 2025 with Grok 4 Heavy variant.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2025-08-28","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":3,"content":"FreeTrial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"grok-4.md"},{"type":"header","level":1,"content":"Grok Code Fast 1 - https://x.ai/news/grok-code-fast-1","children":[{"type":"text","content":"Grok Code Fast 1 is xAI's speedy and economical reasoning model that excels at agentic coding, built from scratch with a brand-new architecture and offering exceptional speed at approximately 160 tokens per second.\n\n**Dataset ID:** aie-model"},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"1.0 (August 2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"SWE-Bench-Verified: 70.8%","plainChildren":""},{"type":"item","level":3,"content":"Speed: ~160 tokens/second (vs GPT-5 at 50.1, Gemini 2.5 Pro at 92.4, Claude 4 Sonnet at 78.7)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"text","content":"xAI's ultra-fast agentic coding model with 314B parameters (MoE), achieving 70.8% on SWE-Bench while delivering ~160 tokens/second throughput."}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Grok Code Fast 1 was released by xAI in late August 2025 as a speedy and economical reasoning model optimized for agentic coding. The model was built from scratch with a brand-new architecture and initially released quietly under the codename \"sonic\" before its official announcement.\n\nBuilt using a mixture-of-experts architecture with an estimated 314 billion parameters, the model was trained on a pre-training corpus rich with programming-related content and curated datasets reflecting real-world pull requests and coding tasks. It features a 256k token context window, enabling it to process larger codebases in context.\n\nThe model is exceptionally versatile across the software development stack and particularly adept at TypeScript, Python, Java, Rust, C++, and Go. It can complete tasks ranging from building zero-to-one projects to performing surgical bug fixes. On the full subset of SWE-Bench-Verified, grok-code-fast-1 scored 70.8% using xAI's internal evaluation harness.\n\nAccording to xAI's benchmarks, the model executes at approximately 160 tokens per second, significantly outpacing competitors: GPT-5 at 50.1 tokens/second, Gemini 2.5 Pro at 92.4, and Claude 4 Sonnet at 78.7. The model is available via the xAI API with competitive pricing and was offered for free through launch partners including GitHub Copilot, Cursor, Cline, Roo Code, Kilo Code, opencode, and Windsurf for a limited time."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"TypeScript","plainChildren":""},{"type":"item","level":3,"content":"Python","plainChildren":""},{"type":"item","level":3,"content":"Java","plainChildren":""},{"type":"item","level":3,"content":"Rust","plainChildren":""},{"type":"item","level":3,"content":"C++","plainChildren":""},{"type":"item","level":3,"content":"Go","plainChildren":""},{"type":"item","level":3,"content":"Supports other languages but particularly adept at these","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Architecture: Mixture-of-experts with ~314B parameters","plainChildren":""},{"type":"item","level":3,"content":"Context Window: 256k tokens","plainChildren":""},{"type":"item","level":3,"content":"Speed: ~160 tokens/second (industry-leading)","plainChildren":""},{"type":"item","level":3,"content":"Benchmark: 70.8% SWE-Bench-Verified","plainChildren":""},{"type":"item","level":3,"content":"Pricing: $0.20/1M input tokens, $1.50/1M output tokens, $0.02/1M cached tokens","plainChildren":""},{"type":"item","level":3,"content":"Launch Partners: GitHub Copilot, Cursor, Cline, Roo Code, Kilo Code, opencode, Windsurf","plainChildren":""},{"type":"item","level":3,"content":"Free Tier: Available free for limited time through select partners","plainChildren":""},{"type":"item","level":3,"content":"Codename: Initially released as \"sonic\"","plainChildren":""},{"type":"item","level":3,"content":"Training: Rich programming content, real-world PR datasets","plainChildren":""},{"type":"item","level":3,"content":"Future: Multimodal variant in training with parallel tool calling and extended context","plainChildren":""},{"type":"item","level":3,"content":"Developer: xAI","plainChildren":""},{"type":"item","level":3,"content":"Release: August 2025","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2025-11-18","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"No","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary","plainChildren":""}]}]},{"type":"header","level":3,"content":"FreeTrial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""},{"type":"item","level":3,"content":"Free access through launch partner tools for limited time","plainChildren":""}]}]}]}],"sourcePath":"grok-code-fast-1.md"},{"type":"header","level":1,"content":"Kimi K2 Thinking - https://huggingface.co/moonshotai/Kimi-K2-Thinking","children":[{"type":"text","content":"Enhanced reasoning model from Moonshot AI that tops benchmarks for reasoning, coding, and agentic tasks. Released November 2025, outperforming GPT-5, Claude Sonnet 4.5 Thinking, and Grok-4."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"1.0 (2025-11)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/MoonshotAI/Kimi-K2","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Advanced reasoning-enhanced version of Kimi K2 that claims top position in reasoning, coding, and agentic-tool benchmarks, surpassing GPT-5 and Claude Sonnet 4.5 Thinking mode.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Kimi K2 Thinking is Moonshot AI's enhanced reasoning model released in November 2025, building upon the foundational Kimi K2 architecture with specialized capabilities for complex reasoning tasks. The model has claimed the top position across reasoning, coding, and agentic-tool benchmarks, outperforming proprietary leaders including OpenAI's GPT-5, Anthropic's Claude Sonnet 4.5 (Thinking mode), and xAI's Grok-4. It combines the trillion-parameter MoE architecture with advanced reasoning techniques to deliver exceptional performance on multi-step problems, code generation, and tool-using scenarios. Available as open weights on Hugging Face and through Moonshot AI's API, K2 Thinking represents a significant milestone in open-source AI capabilities."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Leading benchmarks: Tops GPT-5, Claude Sonnet 4.5 Thinking, and Grok-4 on standard evaluations.","plainChildren":""},{"type":"item","level":3,"content":"Reasoning-enhanced: Specialized architecture for complex multi-step reasoning and problem-solving.","plainChildren":""},{"type":"item","level":3,"content":"Exceptional coding: State-of-the-art performance on coding benchmarks with reasoning capabilities.","plainChildren":""},{"type":"item","level":3,"content":"Agentic excellence: Superior performance on tool-using and autonomous agent tasks.","plainChildren":""},{"type":"item","level":3,"content":"Open weights: Available on Hugging Face for community access and research.","plainChildren":""},{"type":"item","level":3,"content":"Built on K2 foundation: Leverages trillion-parameter MoE architecture of base Kimi K2.","plainChildren":""},{"type":"item","level":3,"content":"November 2025 release: Latest advancement from Alibaba-backed Moonshot AI.","plainChildren":""},{"type":"item","level":3,"content":"Cost-competitive: Maintains competitive API pricing while delivering leading performance.","plainChildren":""},{"type":"item","level":3,"content":"Community impact: Demonstrates that open-weight models can match or exceed proprietary leaders.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2025-11","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""},{"type":"item","level":3,"content":"Open weights available on Hugging Face","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Apache-2.0","plainChildren":""}]}]},{"type":"header","level":3,"content":"FreeTrial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"kimi-k2-thinking.md"},{"type":"header","level":1,"content":"Kimi K2 - https://github.com/MoonshotAI/Kimi-K2","children":[{"type":"text","content":"State-of-the-art Mixture-of-Experts (MoE) language model from Moonshot AI with 1 trillion total parameters and exceptional coding performance. Released July 2025, outperforming Claude Opus 4 and GPT-4.1 on coding benchmarks."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"1.0 (2025-07)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/MoonshotAI/Kimi-K2","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Trillion-parameter open-weight MoE model from Alibaba-backed Moonshot AI, achieving state-of-the-art performance in coding, math, and agentic reasoning with 32B activated parameters.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model developed by Moonshot AI, a Beijing startup backed by Alibaba. Released in July 2025, it features 32 billion activated parameters and 1 trillion total parameters, making it one of the largest open-weight models available. The model achieves exceptional performance in frontier knowledge, mathematics, and coding, surpassing Claude Opus 4 on multiple benchmarks and demonstrating better overall performance than OpenAI's GPT-4.1. With 65.8% pass@1 on SWE-bench Verified and 71.6% on full SWE-bench, Kimi K2 excels at agentic reasoning and real-world coding tasks. The model is cost-effective at $0.15 per million input tokens and $2.50 per million output tokens—significantly cheaper than competitors."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Trillion-parameter MoE: 1 trillion total parameters with 32B activated, one of the largest open-weight models.","plainChildren":""},{"type":"item","level":3,"content":"Exceptional coding performance: 71.6% on SWE-bench, 65.8% on SWE-bench Verified, 53.7% on LiveCodeBench.","plainChildren":""},{"type":"item","level":3,"content":"Surpasses proprietary models: Outperforms Claude Opus 4 and GPT-4.1 on coding benchmarks.","plainChildren":""},{"type":"item","level":3,"content":"Long context support: Optimized for long-context understanding and processing.","plainChildren":""},{"type":"item","level":3,"content":"Agentic reasoning: Strong performance on tool use and multi-step reasoning tasks.","plainChildren":""},{"type":"item","level":3,"content":"Cost-effective: $0.15/M input tokens, $2.50/M output tokens (100x cheaper than Claude Opus 4 for input).","plainChildren":""},{"type":"item","level":3,"content":"Open weights: Available on GitHub and Hugging Face for community use.","plainChildren":""},{"type":"item","level":3,"content":"Moonshot AI: Developed by Alibaba-backed Beijing startup.","plainChildren":""},{"type":"item","level":3,"content":"Released July 2025 as a major advancement in open-weight models.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2025-07","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""},{"type":"item","level":3,"content":"Open weights available on GitHub and Hugging Face","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Apache-2.0","plainChildren":""}]}]},{"type":"header","level":3,"content":"FreeTrial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"kimi-k2.md"},{"type":"header","level":1,"content":"Kite — https://kite.com","children":[{"type":"text","content":"A desktop AI code-completion assistant focused initially on Python, later multi-language. Ran local models for low-latency, privacy-first completions and editor integrations. Company ceased operations in late 2022 and released parts of its codebase as open source."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Agent","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"text","content":"Archived (2022-12-31)"}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/kiteco/kiteco","plainChildren":""},{"type":"item","level":3,"content":"https://github.com/kiteco","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"[4] Strong historical accuracy for Python completions (context-aware compared to older alphabetical completions)","plainChildren":""},{"type":"item","level":3,"content":"[3] Multi-language coverage decent but uneven (best for Python)","plainChildren":""},{"type":"item","level":3,"content":"[4] Privacy: good (local processing design)","plainChildren":""},{"type":"item","level":3,"content":"[2] Business viability: failed to monetize sufficiently","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Former AI-powered code-completion assistant that ran local ML models for low-latency, privacy-first context-aware completions and inline documentation across popular editors; company ceased operations in late 2022 and released parts of its codebase as open source.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Kite was an early AI-assisted coding tool (founded 2014) that provided context-aware code completions, documentation lookups, and inline examples inside editors. Its core differentiator was processing code and ML inference locally on the developer's machine to reduce latency and address privacy concerns. Kite trained models on large bodies of open-source code and tuned them for code prediction tasks rather than using plain NLP models. Despite strong technical work and a sizeable user base, Kite shut down operations in late 2022 and open-sourced a portion of its codebase."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Python","plainChildren":""},{"type":"item","level":3,"content":"JavaScript / TypeScript","plainChildren":""},{"type":"item","level":3,"content":"Java","plainChildren":""},{"type":"item","level":3,"content":"Go","plainChildren":""},{"type":"item","level":3,"content":"Other","plainChildren":""},{"type":"item","level":3,"content":"Many others via editor plugins","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Founded 2014; widely adopted by Python developers for smarter completions and docs.","plainChildren":""},{"type":"item","level":3,"content":"Raised funding and grew to a large community, but announced shutdown in late 2022 due to a combination of technical limits (models not yet delivering transformative improvements) and monetization challenges.","plainChildren":""},{"type":"item","level":3,"content":"Legacy: influenced expectations for context-aware completions and privacy-conscious local inference; lessons from Kite informed subsequent entrants and enterprise offerings in the AI coding space.","plainChildren":""},{"type":"item","level":3,"content":"As of 2025 the company is inactive; repositories remain as historical artifacts and starting points for community forks and research.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2022-12-31","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""},{"type":"item","level":3,"content":"Company published several repositories under the kiteco GitHub organization after winding down; some components and research artifacts are available for reuse.","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Mixed / see repository","plainChildren":""}]}]},{"type":"header","level":3,"content":"FreeTrial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"N/A","plainChildren":""},{"type":"item","level":3,"content":"Open-source (post-shutdown): repositories freely available, not a time-limited trial.","plainChildren":""}]}]}]}],"sourcePath":"kite.md"},{"type":"header","level":1,"content":"MiniMax M2 - https://www.minimax.io/news/minimax-m2","children":[{"type":"text","content":"MiniMax M2 is a compact, fast, and cost-effective MoE (Mixture of Experts) model with 230 billion total parameters and 10 billion active parameters, built for elite performance in coding and agentic tasks.\n\n**Dataset ID:** aie-model"},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"M2 (October 2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/MiniMax-AI/MiniMax-M2","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"SWE-bench Verified: 69.4% (close to GPT-5's 74.9%)","plainChildren":""},{"type":"item","level":3,"content":"#1 among open-source models globally in composite intelligence score","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"text","content":"Compact 230B-parameter MoE model (10B active) optimized for coding and agentic workflows, offering elite performance at 8% the cost of Claude Sonnet with 2x speed."}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"MiniMax M2 is an open-source model launched in late October 2025, designed specifically for coding and agentic workflows. As a Mixture of Experts (MoE) architecture, it features 230 billion total parameters with only 10 billion active parameters, achieving remarkable efficiency without sacrificing performance.\n\nThe model excels in end-to-end development workflows and integrates seamlessly with popular coding tools like Claude Code, Cursor, Cline, Kilo Code, and Droid. It performs exceptionally well on multi-file edits, coding-run-fix loops, and test-validated repairs. Its strong performance on Terminal-Bench and SWE-Bench-style tasks demonstrates practical effectiveness in terminals, IDEs, and CI environments across multiple programming languages.\n\nAt only 8% of the price of Claude Sonnet and twice the speed, MiniMax M2 offers compelling value for developers. The model is available for free for a limited time and ranks #1 among open-source models in composite intelligence scores across mathematics, science, instruction following, coding, and agentic tool use."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Architecture: MoE (230B total params, 10B active)","plainChildren":""},{"type":"item","level":3,"content":"Pricing: 8% of Claude Sonnet cost, 2x faster, free for limited time","plainChildren":""},{"type":"item","level":3,"content":"Benchmark Performance: SWE-bench Verified 69.4% (near GPT-5's 74.9%)","plainChildren":""},{"type":"item","level":3,"content":"Tool Integration: Claude Code, Cursor, Cline, Kilo Code, Droid","plainChildren":""},{"type":"item","level":3,"content":"Coding Strengths: Multi-file edits, coding-run-fix loops, test-validated repairs","plainChildren":""},{"type":"item","level":3,"content":"Performance Tests: Strong on Terminal-Bench and Multi-SWE-Bench tasks","plainChildren":""},{"type":"item","level":3,"content":"Global Ranking: #1 open-source model in composite intelligence score","plainChildren":""},{"type":"item","level":3,"content":"Release: Late October 2025","plainChildren":""},{"type":"item","level":3,"content":"Availability: Open-source on GitHub, available via API platforms","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2025-11-18","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]}]}],"sourcePath":"minimax-m2.md"},{"type":"header","level":1,"content":"NVIDIA Nemotron Nano 2 VL - https://developer.nvidia.com/blog/develop-specialized-ai-agents-with-new-nvidia-nemotron-vision-rag-and-guardrail-models/","children":[{"type":"text","content":"NVIDIA Nemotron Nano 2 VL is a 12B multimodal reasoning model that enables AI assistants to extract, interpret, and act on information across text, images, tables, and videos with improved accuracy and efficiency.\n\n**Dataset ID:** aie-model"},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"V2 (October/November 2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-BF16","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"OCRBench v2: Leading results","plainChildren":""},{"type":"item","level":3,"content":"Average across benchmarks: ~74 (MMMU, MathVista, AI2D, OCRBench, OCR-Reasoning, ChartQA, DocVQA, Video-MME)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"text","content":"12B-parameter multimodal vision-language model with hybrid Mamba-Transformer architecture, optimized for document understanding, video comprehension, and efficient reasoning tasks."}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"NVIDIA Nemotron Nano 2 VL is a 12-billion parameter multimodal reasoning model released in late October/early November 2025. The model features a hybrid Mamba-Transformer architecture delivering on-par accuracy with high token throughput and low latency for efficient large-scale reasoning across visual and text tasks.\n\nBuilt on top of the Nemotron Nano V2 12B reasoning LLM and RADIOv2.5 vision encoder, the model introduces the Efficient Video Sampling (EVS) method that identifies and prunes temporally static patches in video sequences. EVS reduces token redundancy while preserving essential semantics, enabling the model to process longer clips and deliver results more swiftly—achieving up to 2.5x higher throughput without sacrificing accuracy.\n\nThe model was trained on the Nemotron VLM Dataset V2 with over 11 million high-quality samples, optimized for optical-character recognition, chart reasoning, and multimodal comprehension. With a context length of 49,152 tokens, it enhances capability for multi-image and video understanding.\n\nNemotron Nano 2 VL achieves leading results on OCRBench v2 and scores approximately 74 average across major benchmarks, surpassing prior open VL baselines. The model is available under a permissive NVIDIA open license with deployment supported across NeMo, NIM, and major inference runtimes including Hugging Face, Fireworks AI, Nebius AI Studio, and vLLM."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""},{"type":"item","level":3,"content":"Primarily focused on visual understanding tasks rather than code generation","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Architecture: Hybrid Mamba-Transformer, 12B parameters","plainChildren":""},{"type":"item","level":3,"content":"Context Length: 49,152 tokens","plainChildren":""},{"type":"item","level":3,"content":"Efficient Video Sampling (EVS): Up to 2.5x throughput improvement","plainChildren":""},{"type":"item","level":3,"content":"Training Data: Nemotron VLM Dataset V2 (11M+ high-quality samples)","plainChildren":""},{"type":"item","level":3,"content":"Benchmarks: ~74 average (MMMU, MathVista, AI2D, OCRBench, OCR-Reasoning, ChartQA, DocVQA, Video-MME)","plainChildren":""},{"type":"item","level":3,"content":"Vision Encoder: RADIOv2.5","plainChildren":""},{"type":"item","level":3,"content":"Base LLM: Nemotron Nano V2 12B","plainChildren":""},{"type":"item","level":3,"content":"Specializations: OCR, chart reasoning, document understanding, video comprehension","plainChildren":""},{"type":"item","level":3,"content":"License: Permissive NVIDIA open license","plainChildren":""},{"type":"item","level":3,"content":"Deployment: NeMo, NIM, vLLM, Hugging Face, Fireworks AI, Nebius AI Studio","plainChildren":""},{"type":"item","level":3,"content":"Release: October/November 2025","plainChildren":""},{"type":"item","level":3,"content":"Developer: NVIDIA","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2025-11-18","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""},{"type":"item","level":3,"content":"Open-weights with permissive NVIDIA license","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Proprietary","plainChildren":""},{"type":"item","level":3,"content":"NVIDIA open license (permissive)","plainChildren":""}]}]},{"type":"header","level":3,"content":"FreeTrial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""},{"type":"item","level":3,"content":"Available for free under NVIDIA's open license","plainChildren":""}]}]}]}],"sourcePath":"nvidia-nemotron-nano-2-vl.md"},{"type":"header","level":1,"content":"PolyCoder - https://github.com/VHellendoorn/Code-LMs","children":[{"type":"text","content":"Open-source code generation model with strong C-language performance"},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2022-03-01","plainChildren":""},{"type":"item","level":3,"content":"Available in multiple model sizes: 160M, 405M, and 2.7B parameters","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/VHellendoorn/Code-LMs","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"[4] Excellent open-source alternative for systems-level code (C/C++).","plainChildren":""},{"type":"item","level":3,"content":"[3] Not competitive with 2024/2025 largest commercial models on all-language benchmarks.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Open-source GPT-2-style autoregressive code model (160M/405M/2.7B) with particularly strong performance on C/C++ code; includes checkpoints, training scripts, and tokenizer configs under the MIT license for reproducible research and on-prem deployments.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"PolyCoder is an open-source autoregressive code model developed by researchers at Carnegie Mellon University. Built on a GPT-2 style decoder-only transformer, PolyCoder was trained on approximately 249GB of GitHub-sourced code across a dozen languages and published in early 2022. The project ships model checkpoints, preprocessing scripts, tokenizer configs and evaluation notebooks under a permissive MIT license, enabling reproducible research, self-hosting, and fine-tuning.\n\nPolyCoder was released in multiple sizes (160M, 405M and 2.7B parameters). The 2.7B model was notable for outperforming contemporaneous models on C code generation benchmarks in the original paper, making it particularly well-suited for systems and embedded programming tasks where correctness and low-level API usage matter."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"C/C++","plainChildren":""},{"type":"item","level":3,"content":"Python","plainChildren":""},{"type":"item","level":3,"content":"JavaScript","plainChildren":""},{"type":"item","level":3,"content":"Java","plainChildren":""},{"type":"item","level":3,"content":"Go","plainChildren":""},{"type":"item","level":3,"content":"PHP","plainChildren":""},{"type":"item","level":3,"content":"Ruby","plainChildren":""},{"type":"item","level":3,"content":"C#","plainChildren":""},{"type":"item","level":3,"content":"Other","plainChildren":""},{"type":"item","level":3,"content":"12 in total","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Trained on ~249GB of code; primary claim-to-fame is strong C-code performance compared to models available in 2022.","plainChildren":""},{"type":"item","level":3,"content":"Comes in three sizes (160M, 405M, 2.7B) so teams can choose a footprint that matches hardware constraints.","plainChildren":""},{"type":"item","level":3,"content":"Checkpoints and training/evaluation scripts were published to enable reproducible research; model files were also archived on Zenodo and mirrored to Hugging Face by community contributors.","plainChildren":""},{"type":"item","level":3,"content":"Requires modern Transformers (4.23+) for out-of-the-box loading; community adapters support LoRA/QLoRA fine-tuning and GGUF quantized deployments.","plainChildren":""},{"type":"item","level":3,"content":"Good choice for on-premise, privacy-sensitive deployments (no external API calls required).","plainChildren":""},{"type":"item","level":3,"content":"Keep expectations realistic: PolyCoder is a 2022-era model and does not match the capabilities of later multi-hundred-billion-parameter code-specialized models, though it remains valuable for C/C++ and systems-level use-cases.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2022-03-01","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"MIT","plainChildren":""}]}]},{"type":"header","level":3,"content":"FreeTrial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"N/A","plainChildren":""},{"type":"item","level":3,"content":"Open-source model: checkpoints, code, and training artifacts are freely available under MIT license (Zenodo archive and GitHub repo), not a time-limited trial.","plainChildren":""}]}]}]}],"sourcePath":"polycoder.md"},{"type":"header","level":1,"content":"Qwen 2.5 Coder - https://github.com/QwenLM/Qwen-Code","children":[{"type":"text","content":"Latest coding-focused model from Alibaba's Qwen family, featuring major upgrades in code understanding and generation capabilities. Released in 2025 as a significant advancement in open-source coding models."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2.5 (September 2024), Qwen2.5-Max (January 2025)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/QwenLM/Qwen-Code","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"HumanEval: 88.4% (Qwen 2.5 Coder 7B)","plainChildren":""},{"type":"item","level":3,"content":"Spider: 82.0% (Qwen 2.5 Coder 7B)","plainChildren":""},{"type":"item","level":3,"content":"LiveCodeBench: 38.7 (Qwen2.5-Max)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Alibaba Cloud's family of specialized coding models (1.5B-32B parameters) with state-of-the-art performance on code generation and debugging benchmarks.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Qwen 2.5 Coder is a family of coding-specialized language models developed by Alibaba Cloud, released on September 19, 2024. The series includes models ranging from 1.5B to 32B parameters, targeting performance levels closer to closed-source models.\n\nThe flagship Qwen 2.5 Coder 7B model achieves 88.4% on the HumanEval benchmark, surpassing both Codestral (81.1%) and DeepSeek Coder v2 Lite (81.1%). On the Spider benchmark for SQL generation, Qwen 2.5 Coder leads with 82.0% compared to Codestral's 76.6%.\n\nIn January 2025, Alibaba released Qwen2.5-Max, their most advanced model available through Alibaba Cloud's Model Studio and the Qwen Chat platform. This model excels in code generation and debugging with a LiveCodeBench score of 38.7, slightly behind Claude Sonnet's 38.9. Qwen2.5-Max is positioned to compete with models like DeepSeek R3 in the enterprise coding space."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Model Family: 1.5B, 3B, 7B, 14B, 32B parameters","plainChildren":""},{"type":"item","level":3,"content":"Release: Qwen 2.5 Coder (September 2024), Qwen2.5-Max (January 2025)","plainChildren":""},{"type":"item","level":3,"content":"Benchmarks:","plainChildren":""},{"type":"item","level":3,"content":"HumanEval: 88.4% (7B model)","plainChildren":""},{"type":"item","level":3,"content":"Spider: 82.0% (7B model)","plainChildren":""},{"type":"item","level":3,"content":"LiveCodeBench: 38.7 (Max model)","plainChildren":""},{"type":"item","level":3,"content":"Strengths: Code generation, debugging, SQL generation","plainChildren":""},{"type":"item","level":3,"content":"Availability: Alibaba Cloud Model Studio, Qwen Chat platform","plainChildren":""},{"type":"item","level":3,"content":"Competition: Positioned against DeepSeek R3, Codestral, Claude Sonnet","plainChildren":""},{"type":"item","level":3,"content":"Open source: Qwen 2.5 Coder series available for local deployment","plainChildren":""},{"type":"item","level":3,"content":"Developer: Alibaba Cloud","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2025-11-18","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Apache-2.0","plainChildren":""}]}]},{"type":"header","level":3,"content":"FreeTrial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"qwen-2-5-coder.md"},{"type":"header","level":1,"content":"Seed-Coder - https://github.com/ByteDance-Seed/Seed-Coder","children":[{"type":"text","content":"Lightweight open-source code LLM family from ByteDance Seed trained on 6 trillion tokens. Achieves state-of-the-art performance at 8B scale, surpassing much larger models."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"1.0 (2025-06)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/ByteDance-Seed/Seed-Coder","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"-","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Family of lightweight open-source code LLMs (base, instruct, reasoning) trained on 6T tokens from GitHub code, commit histories, and code-related web data, achieving SOTA at 8B scale.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"Seed-Coder is a family of lightweight open-source code language models developed by ByteDance Seed, comprising base, instruct, and reasoning model variants. The pretraining corpus comprises approximately 6 trillion tokens sourced from GitHub code, commit histories, and code-related web data, providing comprehensive coverage of modern software development practices. Despite its relatively compact 8B parameter scale, Seed-Coder achieves state-of-the-art performance among open-source models at this size and even surpasses some much larger models in coding benchmarks. All models in the family are publicly available on Hugging Face, making them accessible for research, fine-tuning, and deployment. The model family represents ByteDance's contribution to open-source AI coding tools, providing an efficient alternative for developers who need strong coding capabilities without requiring massive computational resources."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"6 trillion tokens: Massive training corpus from GitHub code, commits, and code-related web data.","plainChildren":""},{"type":"item","level":3,"content":"Lightweight efficiency: Achieves SOTA at 8B scale, surpassing much larger models.","plainChildren":""},{"type":"item","level":3,"content":"Model family: Includes base, instruct, and reasoning variants for different use cases.","plainChildren":""},{"type":"item","level":3,"content":"Open source: All models publicly available on Hugging Face collection.","plainChildren":""},{"type":"item","level":3,"content":"State-of-the-art 8B: Best performance among open-source models at 8B parameter scale.","plainChildren":""},{"type":"item","level":3,"content":"Compact deployment: Suitable for resource-constrained environments while maintaining quality.","plainChildren":""},{"type":"item","level":3,"content":"ByteDance Seed: Developed by ByteDance's AI research division.","plainChildren":""},{"type":"item","level":3,"content":"Research-friendly: Accessible for academic research, fine-tuning, and custom deployments.","plainChildren":""},{"type":"item","level":3,"content":"Code-focused training: Specialized corpus emphasizing practical software development.","plainChildren":""},{"type":"item","level":3,"content":"Released June 2025 as part of ByteDance's open-source AI strategy.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2025-06","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"MIT","plainChildren":""},{"type":"item","level":3,"content":"Note: Different sources report both MIT and Apache-2.0; verify from official repository","plainChildren":""}]}]},{"type":"header","level":3,"content":"FreeTrial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]}]}],"sourcePath":"seed-coder.md"},{"type":"header","level":1,"content":"StarCoder - https://huggingface.co/bigcode/starcoder","children":[{"type":"text","content":"An open-source large language model for code, developed by the BigCode community and released on Hugging Face. Built for code generation, completion and code-aware assistance across many programming languages."},{"type":"header","level":2,"content":"General Info","children":[{"type":"header","level":3,"content":"Classification","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"AIE/Model","plainChildren":""}]}]},{"type":"header","level":3,"content":"Version","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"v1 (15.5B parameters, 2023)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Repo","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"https://github.com/bigcode-project/starcoder","plainChildren":""},{"type":"item","level":3,"content":"Hugging Face: https://huggingface.co/bigcode/starcoder","plainChildren":""}]}]},{"type":"header","level":3,"content":"Rating","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"[5] Strong open-source code generation baseline","plainChildren":""},{"type":"item","level":3,"content":"[4] Excellent multilingual code coverage and long context","plainChildren":""}]}]},{"type":"header","level":3,"content":"Short Description","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Open-source 15.5B-parameter decoder-only transformer for code generation and infilling, trained on permissively licensed GitHub code (The Stack). Optimized for code completion, infilling, translation between languages, and code-aware QA.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Description","children":[{"type":"text","content":"StarCoder is a decoder-only transformer model optimized for programming tasks. The original released model has ~15.5 billion parameters and was trained by the BigCode community on The Stack — a large, permissively-licensed corpus of GitHub code — with heavy filtering and preprocessing. StarCoder offers a long context window (commonly released with an 8k token context), strong multilingual code support, and capabilities for code completion, infilling, translation between languages, and code-aware question answering.\n\nThe project emphasizes responsible open-source release practices (dataset opt-outs, PII redaction tools, attribution tracing) and is distributed under an OpenRAIL-style usage license on Hugging Face. The model can be run locally (with appropriate hardware) or served via inference runtimes (transformers, text-generation-inference, Docker containers, or community tools like Ollama)."}]},{"type":"header","level":3,"content":"Languages","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Any","plainChildren":""},{"type":"item","level":3,"content":"80+ programming languages (Python, JavaScript, Java, C/C++, Go, Ruby, Rust, TypeScript, PHP, Shell, SQL, etc.)","plainChildren":""}]}]},{"type":"header","level":3,"content":"Notes","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Key strengths: open-source, strong code performance for many languages, long context handling (8k tokens), and community-driven tooling.","plainChildren":""},{"type":"item","level":3,"content":"Training data: The Stack (curated permissively-licensed GitHub code); BigCode published data curation and opt-out tooling.","plainChildren":""},{"type":"item","level":3,"content":"Privacy & safety: authors provided a PII redaction pipeline and attribution tracing to help source provenance and mitigate leakage risks.","plainChildren":""},{"type":"item","level":3,"content":"Variants/evolution: StarCoder followed by StarCoder2 family (further improvements and language coverage in later releases).","plainChildren":""},{"type":"item","level":3,"content":"Typical uses: editor completions, code generation from docstrings, refactoring assistance, automated code reviews, and local/offline deployments for privacy-sensitive environments.","plainChildren":""},{"type":"item","level":3,"content":"Limitations: may reproduce licensed or low-quality snippets from training data; users should validate generated code for correctness, security, and licensing implications.","plainChildren":""},{"type":"item","level":3,"content":"Integration tips: use temperature/top-p tuning for generation quality, provide clear prompts (function signatures, tests) for best results, and prefer fp16/bf16 runtime on GPU for performance.","plainChildren":""}]}]},{"type":"header","level":3,"content":"Last Update","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"2023-05-04 (original StarCoder release on Hugging Face / GitHub)","plainChildren":""},{"type":"item","level":3,"content":"2024-02-28 (StarCoder2 announced / successor family)","plainChildren":""}]}]}]},{"type":"header","level":2,"content":"Licensing","children":[{"type":"header","level":3,"content":"Opensource","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Yes","plainChildren":""}]}]},{"type":"header","level":3,"content":"License","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"Other","plainChildren":""},{"type":"item","level":3,"content":"BigCode-OpenRAIL / OpenRAIL-M (model card requires agreement on use)","plainChildren":""}]}]},{"type":"header","level":3,"content":"FreeTrial","children":[{"type":"list","level":3,"children":[{"type":"item","level":3,"content":"N/A","plainChildren":""},{"type":"item","level":3,"content":"Open-source model: weights and checkpoints are freely available (subject to accepting the model card / license on Hugging Face), not a time-limited trial.","plainChildren":""}]}]}]}],"sourcePath":"starcoder.md"}]